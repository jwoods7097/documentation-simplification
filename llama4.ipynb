{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e66bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea4fe39",
   "metadata": {},
   "source": [
    "# Loading Dataset\n",
    "\n",
    "The dataset can be found at https://huggingface.co/datasets/code-search-net/code_search_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c363fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Fsoft-AIC/the-vault-function\", split_set=[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c67f1a8",
   "metadata": {},
   "source": [
    "Here are the columns in the dataset. We will be using `original_docstring` for documentation and `original_string` for code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fdfdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9679300e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hexsha', 'repo', 'path', 'license', 'language', 'identifier',\n",
       "       'return_type', 'original_string', 'original_docstring', 'docstring',\n",
       "       'docstring_tokens', 'code', 'code_tokens', 'short_docstring',\n",
       "       'short_docstring_tokens', 'comment', 'parameters', 'docstring_params'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8d9ab",
   "metadata": {},
   "source": [
    "Getting a random sample from the test split to test our baseline model. We will increase the size once we finalize the experimental plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ae0f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = test_df.groupby('language', group_keys=False).sample(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7026741f",
   "metadata": {},
   "source": [
    "# Testing the Alternative model\n",
    "\n",
    "Here we define the system prompt for the Llama 4 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a303e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(language):\n",
    "    return \\\n",
    "f'''You are a helpful agent designed to simplify code documentation for beginner programmers.\n",
    "You will be provided with a block of {language} code and the existing doucmentation that accompanies it.\n",
    "Simplify the given documentation, using the provided code as context, so that it is understandable\n",
    "to beginner programmers. Output absolutely nothing else besides the simplified documentation.\n",
    "Make sure to keep any documentation formatting codes present in the simplified documentation.\n",
    "If you feel that the existing documentation is simple enough and meaning would be lost by simplifying\n",
    "it further, feel free to keep the documentation as is. Here is the original documentation and code:'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b523e6",
   "metadata": {},
   "source": [
    "Creating the pipeline for the Llama 2 model using the HuggingFace transformers library. Modified from the example here: https://huggingface.co/docs/transformers/en/model_doc/llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa4efdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7be43c772824fa6808242be5353314e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 50 files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c2bee01b654832984891e735a86e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac1bfaf",
   "metadata": {},
   "source": [
    "Testing the pipeline. Modified from examples given here: https://huggingface.co/docs/transformers/main/en/chat_templating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6011ef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Documentation:\n",
      "Build the library mappings tables.\n",
      "\n",
      "Code:\n",
      "def build_mapping_tables(app):\n",
      "    \"\"\"Build the library mappings tables.\"\"\"\n",
      "    env = Environment(loader=FileSystemLoader(f\"{DIR_PATH}\"))\n",
      "    template_file = env.get_template(\"table_template.j2\")\n",
      "\n",
      "    LIST_OF_MAP_DICTS = []\n",
      "    for attr in dir(lib_mapper):\n",
      "        if (attr.endswith(\"MAPPER_REVERSE\") or attr.endswith(\"_MAPPER\")) and not (\n",
      "            attr.startswith(\"_\") or attr.startswith(\"NETMIKO\") or attr.startswith(\"MAIN\")\n",
      "        ):\n",
      "            LIST_OF_MAP_DICTS.append(attr)\n",
      "\n",
      "    for dict_name in LIST_OF_MAP_DICTS:\n",
      "        lib_name = dict_name.split(\"_\")[0]\n",
      "        filename = f\"{lib_name}_reverse\" if \"REVERSE\" in dict_name else lib_name\n",
      "        headers = [\"NORMALIZED\", lib_name] if \"REVERSE\" in dict_name else [lib_name, \"NORMALIZED\"]\n",
      "        rendered_template = template_file.render(lib_names=headers, mappings=getattr(lib_mapper, dict_name))\n",
      "        with open(f\"{DIR_PATH}/netutils/lib_mapping/{filename}_table.rst\", \"w\") as table_file:\n",
      "            table_file.write(rendered_template)\n",
      "\n",
      "Simplified Documentation:\n",
      "```python\n",
      "def build_mapping_tables(app):\n",
      "    \"\"\"\n",
      "    Creates library mapping tables used for data normalization.\n",
      "\n",
      "    This function generates tables based on predefined mappings in the `lib_mapper` module.\n",
      "    It uses a template to render the tables and saves them as.rst files.\n",
      "\n",
      "    Parameters:\n",
      "    app (object): The application object (not used in this function).\n",
      "    \"\"\"\n",
      "    env = Environment(loader=FileSystemLoader(f\"{DIR_PATH}\"))\n",
      "    template_file = env.get_template(\"table_template.j2\")\n",
      "\n",
      "    # Get a list of mapping dictionaries from the lib_mapper module\n",
      "    LIST_OF_MAP_DICTS = []\n",
      "    for attr in dir(lib_mapper):\n",
      "        if (attr.endswith(\"MAPPER_REVERSE\") or attr.endswith(\"_MAPPER\")) and not (\n",
      "            attr.startswith(\"_\") or attr.startswith(\"NETMIKO\") or attr.startswith(\"MAIN\")\n",
      "        ):\n",
      "            LIST_OF_MAP_DICTS.append(attr)\n",
      "\n",
      "    # Generate and save tables for each mapping dictionary\n",
      "    for dict_name in LIST_OF_MAP_DICTS:\n",
      "        lib_name = dict_name.split(\"_\")[0]\n",
      "        filename = f\"{lib_name}_reverse\" if \"REVERSE\" in dict_name else lib_name\n",
      "        headers = [\"NORMALIZED\", lib_name] if \"REVERSE\" in dict_name else [lib_name, \"NORMALIZED\"]\n",
      "        rendered_template = template_file.render(lib_names=headers, mappings=getattr(lib_mapper, dict_name))\n",
      "        with open(f\"{DIR_PATH}/netutils/lib_mapping/{filename}_table.rst\", \"w\") as table_file:\n",
      "            table_file.write(rendered_template)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": prompt(dataset['test'][0]['language'])},\n",
    "    {\"role\": \"user\", \"content\": f\"Documentation:\\n{dataset['test'][0]['original_docstring']}\\n\\nCode:\\n{dataset['test'][0]['original_string']}\"}\n",
    "]\n",
    "print(f\"Original Documentation:\\n{dataset['test'][0]['original_docstring']}\\n\")\n",
    "print(f\"Code:\\n{dataset['test'][0]['original_string']}\\n\")\n",
    "output = pipe(message, pad_token_id=pipe.tokenizer.eos_token_id, max_new_tokens=2000)\n",
    "print(\"Simplified Documentation:\\n\" + output[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da96447",
   "metadata": {},
   "source": [
    "Loading the evaluation model used for computing semantic similarity. Taken from example here: https://huggingface.co/tasks/sentence-similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf2e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2f233",
   "metadata": {},
   "source": [
    "Running inference on the dataset sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b05b3770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/j/jwoods03/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/j/jwoods03/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/j/jwoods03/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "200it [2:01:46, 36.53s/it]\n"
     ]
    }
   ],
   "source": [
    "semantic_similarities = []\n",
    "metrics = evaluate.combine(['rouge', 'meteor'])\n",
    "\n",
    "for instance in tqdm(dataset_sample.itertuples()):\n",
    "    message = [\n",
    "        {\"role\": \"system\", \"content\": prompt(instance.language)},\n",
    "        {\"role\": \"user\", \"content\": f\"Documentation:\\n{instance.original_docstring}\\n\\nCode:\\n{instance.original_string}\"}\n",
    "    ]\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    result = pipe(message, pad_token_id=pipe.tokenizer.eos_token_id, max_new_tokens=2000)[0]['generated_text'][-1]['content']\n",
    "\n",
    "    embedding_original = eval_model.encode(instance.original_docstring, convert_to_tensor=True)\n",
    "    embedding_predicted = eval_model.encode(result, convert_to_tensor=True)\n",
    "\n",
    "    semantic_similarities.append(util.pytorch_cos_sim(embedding_original, embedding_predicted).item())\n",
    "    metrics.add(predictions=result, references=instance.original_docstring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d144bea",
   "metadata": {},
   "source": [
    "Summary statistics for semantic similarity results (alternative model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2e295fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.745733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.177670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.066537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.654670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.793024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.874772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.985788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  200.000000\n",
       "mean     0.745733\n",
       "std      0.177670\n",
       "min     -0.066537\n",
       "25%      0.654670\n",
       "50%      0.793024\n",
       "75%      0.874772\n",
       "max      0.985788"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = pd.DataFrame(semantic_similarities)\n",
    "sims.to_excel('results/semantic_similarities_llama4.xlsx')\n",
    "sims.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07954701",
   "metadata": {},
   "source": [
    "ROUGE and METEOR results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b08bf030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results/rouge_meteor_llama4.json', 'w') as file:\n",
    "    mr = metrics.compute()\n",
    "    json.dump(mr, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cis532",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
