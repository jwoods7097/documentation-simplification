original_string,original_docstring,modified_short_docstring,
"IN_PROC_BROWSER_TEST_F(DriveAppProviderTest, MatchingChromeAppInstalled) {
  // Prepare a Drive app that matches the not-yet-installed kChromeAppId.
  fake_drive_service()->AddApp(
      kDriveAppId, kDriveAppName, kChromeAppId, kLaunchUrl);
  RefreshDriveAppRegistry();
  WaitForPendingDriveAppConverters();

  // An Url app should be created.
  const Extension* url_app =
      ExtensionRegistry::Get(profile())->GetExtensionById(
          mapping()->GetChromeApp(kDriveAppId), ExtensionRegistry::EVERYTHING);
  EXPECT_TRUE(url_app->is_hosted_app());
  EXPECT_TRUE(url_app->from_bookmark());

  const std::string url_app_id = url_app->id();
  EXPECT_NE(kChromeAppId, url_app_id);
  EXPECT_EQ(url_app_id, mapping()->GetChromeApp(kDriveAppId));
  EXPECT_TRUE(mapping()->IsChromeAppGenerated(url_app_id));

  // Installs a chrome app with matching id.
  InstallChromeApp(0);

  // The Drive app should be mapped to chrome app.
  EXPECT_EQ(kChromeAppId, mapping()->GetChromeApp(kDriveAppId));
  EXPECT_FALSE(mapping()->IsChromeAppGenerated(kChromeAppId));

  // Url app should be auto uninstalled.
  EXPECT_FALSE(ExtensionRegistry::Get(profile())->GetExtensionById(
      url_app_id, ExtensionRegistry::EVERYTHING));
}",// A matching Chrome app replaces the created URL app.,"/**
   * Modify Supplier (asynchronously) Modifies the specified supplier.
   *
   * @param companyId The ID of the company. (required)
   * @param supplierId The ID of the supplier. (required)
   * @param modifySupplierRequest The modified Supplier. First level parameters are managed in delta
   *     mode. (optional)
   * @param _callback The callback to be executed when the API call finishes
   * @return The request call
   * @throws ApiException If fail to process the API call, e.g. serializing the request body object
   * @http.response.details
   *     <table border=""1"">
   * <caption>Response Details</caption>
   * <tr><td> Status Code </td><td> Description </td><td> Response Headers </td></tr>
   * <tr><td> 200 </td><td> Example response </td><td>  -  </td></tr>
   * <tr><td> 401 </td><td> Unauthorized </td><td>  -  </td></tr>
   * <tr><td> 404 </td><td> Not Found </td><td>  -  </td></tr>
   * </table>
   */",
"void Zenject::Internal::LookupId::_set_Provider(Zenject::IProvider* value) {
  static auto ___internal__logger = ::Logger::get().WithContext(""Zenject::Internal::LookupId::_set_Provider"");
  auto ___internal__instance = this;
  static auto ___internal__field__offset = THROW_UNLESS(il2cpp_utils::FindField(___internal__instance, ""Provider""))->offset;
  *reinterpret_cast<Zenject::IProvider**>(reinterpret_cast<char*>(this) + ___internal__field__offset) = value;
}","// Autogenerated instance field setter
// Set instance field: public Zenject.IProvider Provider","/***************************************
	 * Registers an cleanup action that will be executed when this process
	 * fragment is finished, i.e. the fragment is removed, the process continues
	 * to the next step, or terminates (regularly or with an error). If a
	 * different finish action is already registered under a particular key it
	 * will be replaced. Therefore invoking code must make sure to use unique
	 * keys or handle the replacement of actions in appropriate ways.
	 *
	 * <p>When invoked a cleanup action receives the process fragment as it's
	 * argument to provide access to process parameters. Registered Actions can
	 * be removed with {@link #removeCleanupAction(String)}.</p>
	 *
	 * @param sKey    A key that identifies the action for later removal
	 * @param fAction The function to invoke on cleanup
	 */",
"TEST(Task_Wavelet_OpenCL, Decompose1D) {
  if (!cv::ocl::haveOpenCL()) GTEST_SKIP();
  cv::ocl::setUseOpenCL(true);

  cv::Mat input(1, 16, CV_32FC2);
  input = cv::Vec2f(0, 0);
  input(cv::Rect(8, 0, 8, 1)) = cv::Vec2f(1.0f, 0.0f);

  cv::Mat wavelet(1, 16, CV_32FC2);

  {
    cv::UMat uinput(1, 16, CV_32FC2);
    input.copyTo(uinput);

    cv::UMat uwavelet(1, 16, CV_32FC2);
    Wavelet<cv::UMat>::decompose_1d(uinput, uwavelet, false);
    uwavelet.copyTo(wavelet);
  }

  cv::Mat composed(1, 16, CV_32FC2);
  Wavelet<cv::Mat>::compose_1d(wavelet, composed, false);

  for (int i = 0; i < 16; i++)
  {
    float delta_re = std::abs(input.at<cv::Vec2f>(i)[0] - composed.at<cv::Vec2f>(i)[0]);
    float delta_im = std::abs(input.at<cv::Vec2f>(i)[1] - composed.at<cv::Vec2f>(i)[1]);

    ASSERT_LE(delta_re, 0.001f);
    ASSERT_LE(delta_im, 0.001f);
  }
}","// Decompose with OpenCL, compose with CPU","/**
     * Get full path and http method
     * @param method    HttpMethod
     * @param subPath   Sub path
     * @return          String
     */",
"HRESULT CDefView::NotifyAutomation(DISPID dispid)
{
    return IUnknown_CPContainerInvokeParam(m_pauto, DIID_DShellFolderViewEvents,
                                           dispid, NULL, 0);
}","//  There is almost always an automation client.  Usually the Address Bar
//  and Explorer Band are watching us.  Sometimes MSHTML is watching, too.
","/**
     * Utility method to help stop an alarm properly. Nothing will happen, if alarm is not firing
     * or using a different instance.
     *
     * @param context application context
     * @param instance you are trying to stop
     */",
"void GlobalNamespace::StandardLevelBuyInfoView::_set__buyPackButton(UnityEngine::UI::Button* value) {
  static auto ___internal__logger = ::Logger::get().WithContext(""GlobalNamespace::StandardLevelBuyInfoView::_set__buyPackButton"");
  auto ___internal__instance = this;
  static auto ___internal__field__offset = THROW_UNLESS(il2cpp_utils::FindField(___internal__instance, ""_buyPackButton""))->offset;
  *reinterpret_cast<UnityEngine::UI::Button**>(reinterpret_cast<char*>(this) + ___internal__field__offset) = value;
}","// Autogenerated instance field setter
// Set instance field: private UnityEngine.UI.Button _buyPackButton
","/**
   * Amount of usages remaining AFTER the current use.
   */",
"int Environment::has(lua_State* L)
{
    int rv = 0;
    try
    {
        const char* key = luaL_checkstring(L, 1);
        bool result = Poco::Environment::has(key);
        lua_pushboolean(L, result);
        rv = 1;
    }
    catch (const Poco::Exception& e)
    {
        rv = pushPocoException(L, e);
    }
    catch (...)
    {
        rv = pushUnknownException(L);
    }
    
    return rv;
}","/// checks for environment variables existence.
// @string name environment variable's name.
// @return boolean or nil. (error)
// @return error message.
// @function has","/**
     * Update an existing serviceCatalog
     *
     * @param serviceEntry ServiceCatalogInfo
     * @param tenantID     ID of the owner's tenant
     * @param userName     Logged in user name
     * @return serviceCatalogId
     * throws APIManagementException if failed to create service catalog
     */",
"static void network_request_lambda_function(void* userdata,
                                            const realm_http_request_t request,
                                            void* request_context) {
    auto jenv = get_env(true);

    // Initialize pointer to JVM class and methods
    jobject network_transport = static_cast<jobject>(userdata);

    try {
        jclass response_callback_class = jenv->FindClass(
                ""io/realm/internal/interop/sync/ResponseCallbackImpl"");
        static jmethodID response_callback_constructor = jenv->GetMethodID(response_callback_class,
                                                                           ""<init>"",
                                                                           ""(Lio/realm/internal/interop/sync/NetworkTransport;J)V"");
        jobject response_callback = jenv->NewObject(response_callback_class,
                                                    response_callback_constructor,
                                                    reinterpret_cast<jobject>(userdata),
                                                    reinterpret_cast<jlong>(request_context));

        send_request_via_jvm_transport(jenv, network_transport, request, response_callback);
    } catch (std::runtime_error &e) {
        // Runtime exception while processing the request/response
        realm_http_response_t response_error;
        // FIXME: validate we propagate the custom codes as an actual exception to the user
        // see: https://github.com/realm/realm-kotlin/issues/451
        response_error.custom_status_code = -4;
        response_error.num_headers = 0;
        response_error.body_size = 0;

        realm_http_transport_complete_request(request_context, &response_error);
    }
}","/**
 * Perform a network request on JVM
 *
 * 1. Cast userdata to the network transport
 * 2. Transform core request to JVM request
 * 3. Perform request
 * 4. Transform JVM response to core response
 */","/**
     * Rotates the vector's data by an float angle
     * @param angle Rotating angle.
     * @return Vector's data rotated.
     */",
"void RowAggregation::doStatistics(const Row& rowIn, int64_t colIn, int64_t colOut, int64_t colAux)
{
    int colDataType = (fRowGroupIn.getColTypes())[colIn];

    if (isNull(&fRowGroupIn, rowIn, colIn) == true)
        return;

    long double valIn = 0.0;

    switch (colDataType)
    {
        case execplan::CalpontSystemCatalog::TINYINT:
        case execplan::CalpontSystemCatalog::SMALLINT:
        case execplan::CalpontSystemCatalog::MEDINT:
        case execplan::CalpontSystemCatalog::INT:
        case execplan::CalpontSystemCatalog::BIGINT:
            valIn = (long double) rowIn.getIntField(colIn);
            break;

        case execplan::CalpontSystemCatalog::DECIMAL:   // handle scale later
        case execplan::CalpontSystemCatalog::UDECIMAL:  // handle scale later
            if (LIKELY(fRowGroupIn.getColumnWidth(colIn) == datatypes::MAXDECIMALWIDTH))
            {
                // To save from unaligned memory
                datatypes::TSInt128 val128In(rowIn.getBinaryField<int128_t>(colIn));
                valIn =  static_cast<long double>(val128In.toTFloat128());
            }
            else if (fRowGroupIn.getColumnWidth(colIn) <= datatypes::MAXLEGACYWIDTH)
            {
                valIn = (long double) rowIn.getIntField(colIn);
            }
            else
            {
                idbassert(false);
            }
            break;

        case execplan::CalpontSystemCatalog::UTINYINT:
        case execplan::CalpontSystemCatalog::USMALLINT:
        case execplan::CalpontSystemCatalog::UMEDINT:
        case execplan::CalpontSystemCatalog::UINT:
        case execplan::CalpontSystemCatalog::UBIGINT:
            valIn = (long double) rowIn.getUintField(colIn);
            break;

        case execplan::CalpontSystemCatalog::DOUBLE:
        case execplan::CalpontSystemCatalog::UDOUBLE:
            valIn = (long double) rowIn.getDoubleField(colIn);
            break;

        case execplan::CalpontSystemCatalog::FLOAT:
        case execplan::CalpontSystemCatalog::UFLOAT:
            valIn = (long double) rowIn.getFloatField(colIn);
            break;

        case execplan::CalpontSystemCatalog::LONGDOUBLE:
            valIn = rowIn.getLongDoubleField(colIn);
            break;

        default:
            std::ostringstream errmsg;
            errmsg << ""RowAggregation: no average for data type: "" << colDataType;
            cerr << errmsg.str() << endl;
            throw logging::QueryDataExcept(errmsg.str(), logging::aggregateFuncErr);
            break;
    }

    fRow.setDoubleField(fRow.getDoubleField(colOut) + 1.0, colOut);
    fRow.setLongDoubleField(fRow.getLongDoubleField(colAux) + valIn, colAux);
    fRow.setLongDoubleField(fRow.getLongDoubleField(colAux + 1) + valIn * valIn, colAux + 1);
}","//------------------------------------------------------------------------------
// Update the sum and count fields for average if input is not null.
// rowIn(in)  - Row to be included in aggregation.
// colIn(in)  - column in the input row group
// colOut(in) - column in the output row group stores the count
// colAux(in) - column in the output row group stores the sum(x)
// colAux + 1 - column in the output row group stores the sum(x**2)
//------------------------------------------------------------------------------","/**
	 * <p>Return the default graph location as specified by the <code>hgdbowl.defaultdb</code>
	 * property. If no such system property is specified, the default location will be
	 * <code>System.getProperty(""java.io.tmpdir"") + File.separator + ""hgdbowl.defaultdb""</code>.
	 */",
"GlobalNamespace::Signal* GlobalNamespace::TutorialSongController::TutorialObjectSpawnData::_get_signal() {
  static auto ___internal__logger = ::Logger::get().WithContext(""GlobalNamespace::TutorialSongController::TutorialObjectSpawnData::_get_signal"");
  auto ___internal__instance = this;
  static auto ___internal__field__offset = THROW_UNLESS(il2cpp_utils::FindField(___internal__instance, ""signal""))->offset;
  return *reinterpret_cast<GlobalNamespace::Signal**>(reinterpret_cast<char*>(this) + ___internal__field__offset);
}","// Completed includes
// Autogenerated instance field getter
// Get instance field: public readonly Signal signal
","/* goodB2G2() - reads in an integer data from the user and increments it.
use badsource and goodsink by reversing the blocks in the second switch  */",
"void Zenject::AnimatorInstaller::_set__animator(UnityEngine::Animator* value) {
  static auto ___internal__logger = ::Logger::get().WithContext(""Zenject::AnimatorInstaller::_set__animator"");
  auto ___internal__instance = this;
  static auto ___internal__field__offset = THROW_UNLESS(il2cpp_utils::FindField(___internal__instance, ""_animator""))->offset;
  *reinterpret_cast<UnityEngine::Animator**>(reinterpret_cast<char*>(this) + ___internal__field__offset) = value;
}","// Autogenerated instance field setter
// Set instance field: private readonly UnityEngine.Animator _animator","/**
     * Pushes a debug group described by the string {@code message} into the command stream. The value of {@code id} specifies the ID of messages generated.
     * The parameter {@code length} contains the number of characters in {@code message}. If {@code length} is negative, it is implied that {@code message}
     * contains a null terminated string. The message has the specified {@code source} and {@code id}, {@code type} {@link #GL_DEBUG_TYPE_PUSH_GROUP DEBUG_TYPE_PUSH_GROUP}, and
     * {@code severity} {@link #GL_DEBUG_SEVERITY_NOTIFICATION DEBUG_SEVERITY_NOTIFICATION}. The GL will put a new debug group on top of the debug group stack which inherits the control of the
     * volume of debug output of the debug group previously residing on the top of the debug group stack. Because debug groups are strictly hierarchical, any
     * additional control of the debug output volume will only apply within the active debug group and the debug groups pushed on top of the active debug group.
     * 
     * <p>An {@link GL11#GL_INVALID_ENUM INVALID_ENUM} error is generated if the value of {@code source} is neither {@link #GL_DEBUG_SOURCE_APPLICATION DEBUG_SOURCE_APPLICATION} nor {@link #GL_DEBUG_SOURCE_THIRD_PARTY DEBUG_SOURCE_THIRD_PARTY}. An
     * {@link GL11#GL_INVALID_VALUE INVALID_VALUE} error is generated if {@code length} is negative and the number of characters in {@code message}, excluding the null-terminator, is
     * not less than the value of {@link #GL_MAX_DEBUG_MESSAGE_LENGTH MAX_DEBUG_MESSAGE_LENGTH}.</p>
     *
     * @param source  the source of the debug message. One of:<br><table><tr><td>{@link #GL_DEBUG_SOURCE_APPLICATION DEBUG_SOURCE_APPLICATION}</td><td>{@link #GL_DEBUG_SOURCE_THIRD_PARTY DEBUG_SOURCE_THIRD_PARTY}</td></tr></table>
     * @param id      the identifier of the message
     * @param message a string containing the message to be sent to the debug output stream
     * 
     * @see <a target=""_blank"" href=""http://docs.gl/gl4/glPushDebugGroup"">Reference Page</a>
     */",
"::Array<::Il2CppString*>* UnityEngine::AddressableAssets::ResourceLocators::ContentCatalogData::_get_m_ProviderIds() {
  static auto ___internal__logger = ::Logger::get().WithContext(""UnityEngine::AddressableAssets::ResourceLocators::ContentCatalogData::_get_m_ProviderIds"");
  auto ___internal__instance = this;
  static auto ___internal__field__offset = THROW_UNLESS(il2cpp_utils::FindField(___internal__instance, ""m_ProviderIds""))->offset;
  return *reinterpret_cast<::Array<::Il2CppString*>**>(reinterpret_cast<char*>(this) + ___internal__field__offset);
}","// Autogenerated instance field getter
// Get instance field: System.String[] m_ProviderIds
","/**
   * Check whether the specified name matches a value in the specified array.
   * @param name the name to check.
   * @param ignoreCase determines whether name comparisons should ignore case.
   * @param array the filter array to check against.
   * @param returnValueIfEmpty the value to return if the array is null or empty.
   * @return true if the name matches one of the values in the array, false otherwise.
   */",
"bool System::Collections::Hashtable::_get_isWriterInProgress() {
  static auto ___internal__logger = ::Logger::get().WithContext(""System::Collections::Hashtable::_get_isWriterInProgress"");
  auto ___internal__instance = this;
  static auto ___internal__field__offset = THROW_UNLESS(il2cpp_utils::FindField(___internal__instance, ""isWriterInProgress""))->offset;
  return *reinterpret_cast<bool*>(reinterpret_cast<char*>(this) + ___internal__field__offset);
}","// Autogenerated instance field getter
// Get instance field: private System.Boolean isWriterInProgress","/**
     * Returns the average log (base 2) joint probability of the
     * response category for cases of the specified reference
     * category.  If there are no cases matching the reference
     * category, the result is <code>Double.NaN</code>.
     *
     * <P>Better classifiers return high values when the reference
     * and response categories are the same and lower values
     * when they are different.  Unlike the conditional probability
     * values, joint probability averages are not particularly
     * useful because they are not normalized by input length.  For
     * the language model classifiers, the scores are normalized
     * by length, and provide a better cross-case view.
     *
     * @param refCategory Reference category.
     * @param responseCategory Response category.
     * @return Average log (base 2) conditional probability of
     * response category in cases for specified reference category.
     * @throws IllegalArgumentException If the either category is unknown.
     * @throws ClassCastException if the classifications are not joint
     * classifications.
     */",
"void Org::BouncyCastle::Crypto::Engines::XteaEngine::_set__sum0(::Array<uint>* value) {
  static auto ___internal__logger = ::Logger::get().WithContext(""Org::BouncyCastle::Crypto::Engines::XteaEngine::_set__sum0"");
  auto ___internal__instance = this;
  static auto ___internal__field__offset = THROW_UNLESS(il2cpp_utils::FindField(___internal__instance, ""_sum0""))->offset;
  *reinterpret_cast<::Array<uint>**>(reinterpret_cast<char*>(this) + ___internal__field__offset) = value;
}","// Autogenerated instance field setter
// Set instance field: private System.UInt32[] _sum0
",// Check if a TTL based K/V pair for a given key exists in the global area of the back-end data store.,
"bool delete_mutation(mutation_type which_mutation, const string &reason,
                     bool failMsg,
                     bool force_mutation, bool god_gift,
                     bool disallow_mismatch)
{
    god_gift |= crawl_state.is_god_acting();

    mutation_type mutat = which_mutation;

    if (!force_mutation)
    {
        if (!god_gift)
        {
            if (you.get_mutation_level(MUT_MUTATION_RESISTANCE) > 1
                && (you.get_mutation_level(MUT_MUTATION_RESISTANCE) == 3
                    || coinflip()))
            {
                if (failMsg)
                    mprf(MSGCH_MUTATION, ""You feel rather odd for a moment."");
                return false;
            }
        }

        if (undead_mutation_rot())
            return false;
    }

    if (which_mutation == RANDOM_MUTATION
        || which_mutation == RANDOM_XOM_MUTATION
        || which_mutation == RANDOM_GOOD_MUTATION
        || which_mutation == RANDOM_BAD_MUTATION
        || which_mutation == RANDOM_NON_SLIME_MUTATION
        || which_mutation == RANDOM_CORRUPT_MUTATION
        || which_mutation == RANDOM_QAZLAL_MUTATION)
    {
        while (true)
        {
            if (one_chance_in(1000))
                return false;

            mutat = static_cast<mutation_type>(random2(NUM_MUTATIONS));

            if (you.mutation[mutat] == 0
                && mutat != MUT_STRONG
                && mutat != MUT_CLEVER
                && mutat != MUT_AGILE
                && mutat != MUT_WEAK
                && mutat != MUT_DOPEY
                && mutat != MUT_CLUMSY)
            {
                continue;
            }

            if (which_mutation == RANDOM_NON_SLIME_MUTATION
                && is_slime_mutation(mutat))
            {
                continue;
            }

            // Check whether there is a non-innate level of `mutat` to delete
            if (you.get_base_mutation_level(mutat, false, true, true) == 0)
                continue;

            // MUT_ANTENNAE is 0, and you.attribute[] is initialized to 0.
            if (mutat && mutat == you.attribute[ATTR_APPENDAGE])
                continue;

            const mutation_def& mdef = _get_mutation_def(mutat);

            if (random2(10) >= mdef.weight && !is_slime_mutation(mutat))
                continue;

            const bool mismatch =
                (which_mutation == RANDOM_GOOD_MUTATION
                 && MUT_BAD(mdef))
                    || (which_mutation == RANDOM_BAD_MUTATION
                        && MUT_GOOD(mdef));

            if (mismatch && (disallow_mismatch || !one_chance_in(10)))
                continue;

            if (you.get_base_mutation_level(mutat, true, false, true) == 0)
                continue; // No non-transient mutations in this category to cure

            break;
        }
    }
    else if (which_mutation == RANDOM_SLIME_MUTATION)
    {
        mutat = _delete_random_slime_mutation();

        if (mutat == NUM_MUTATIONS)
            return false;
    }

    return _delete_single_mutation_level(mutat, reason, false); // won't delete temp mutations
}","/*
 * Delete a mutation level, accepting random mutation types and checking mutation resistance.
 * This will not delete temporary or innate mutations.
 *
 * @param which_mutation    a mutation, including random
 * @param reason            the reason for deletion
 * @param failMsg           whether to message the player on failure
 * @param force_mutation    whether to try to override certain cases where the mutation would otherwise fail
 * @param god_gift          is the mutation a god gift?  Will also override certain cases.
 * @param disallow_mismatch for random mutations, do we override good/bad designations in `which_mutation`? (??)
 *
 * @return true iff a mutation was applied.
 */","/**
     * Test making a simple torrent with a single file
     *
     * @throws IOException on failure
     */",
"bool ReadINode(int dirfd, const char* path, const char* prefix, ino_t* inode) {
  char linkbuf[64];
  ssize_t nread = readlinkat(dirfd, path, linkbuf, sizeof(linkbuf));
  if (nread <= 0 || nread >= ssizeof(linkbuf) - 1) return false;
  linkbuf[nread] = '\0';
  if (linkbuf[nread - 1] != ']') return false;

  size_t prefix_len = std::strlen(prefix);
  if (std::strncmp(linkbuf, prefix, prefix_len) != 0) return false;
  if (std::strncmp(linkbuf + prefix_len, "":["", 2) != 0) return false;

  // Parse inode value as decimal
  char* endp;
  uintmax_t parsed = std::strtoumax(linkbuf + prefix_len + 2, &endp, 10);
  if (*endp != ']') return false;
  *inode = static_cast<ino_t>(parsed);
  return true;
}","// ReadINode reads the inode from the symlink of form '<prefix>:[<inode>]' of the given path. If an error is encountered
// or the inode symlink doesn't have the correct prefix, false is returned.","/**
     * Parses properties from task result
     *
     * @param taskResult - JSON formatted set of properties
     */",
"std::string TokenCountsToString(const std::map<ObjectIdentifier, int>& token_counts) {
  std::ostringstream stream;
  for (const auto& token : token_counts) {
    stream << ""\n"" << token.first << "" "" << token.second;
  }
  return stream.str();
}",// Converts a map of ObjectIdentifier counts to a string listing them.,"/*
        Test whenever some/or all parameters were not passed within the
        Endpoint
     */",
"int EpollEventToString(const uint32_t event, char* const events_string, const size_t n) {
    if (!(events_string && n)) return 0;

#define DOALL(DOONE) \
    DOONE(IN)\
    DOONE(PRI)\
    DOONE(OUT)\
    DOONE(ERR)\
    DOONE(HUP)\
    DOONE(RDNORM)\
    DOONE(RDBAND)\
    DOONE(WRNORM)\
    DOONE(WRBAND)\
    DOONE(MSG)\
    DOONE(RDHUP)\
    DOONE(EXCLUSIVE)\
    DOONE(WAKEUP)\
    DOONE(ONESHOT)\
    DOONE(ET)\

#define EVENT_TO_STRING(x) \
    if (EPOLL##x == event) {\
        static const char s[] = #x;\
        const size_t num_bytes_to_write = sizeof(s);\
        if (n < num_bytes_to_write) return 0;\
        strncpy(events_string, s, num_bytes_to_write);\
        return num_bytes_to_write - 1;\
    }

    DOALL(EVENT_TO_STRING)
#undef EVENT_TO_STRING
#undef DOALL

    // snprintf returns the number of characters in the desired rendered string, EXCLUDING null-terminator.
    // Therefore, string has been completely written only if returned value is positive and less than n.
    const int e = snprintf(events_string, n, ""%x"", event);

    return ((e > 0) && (static_cast<size_t>(e) < n)) ? e : 0;
}","// returns number of characters correctly written, EXCLUDING null-terminator
// events_string will contain the null-terminator if return value > 0.
","/**
     * Start the current sensor selected by user
     *
     * @param evt
     */",
"void BinaryEncodingCNL::DoAll()
{
    std::vector<ForwardJmpOffset> offsetVector;
    FixInst();
    BinaryEncodingBase::InitPlatform();
    // BDW/CHV/SKL/BXT/CNL use the same compaction tables except from 3src.
    for (uint8_t i=0; i<(int)COMPACT_TABLE_SIZE; i++)
    {
        BDWCompactControlTable.AddIndex(IVBCompactControlTable[i], i);
        BDWCompactSourceTable.AddIndex(IVBCompactSourceTable[i], i);
        BDWCompactSubRegTable.AddIndex(IVBCompactSubRegTable[i], i);
        BDWCompactSubRegTable.AddIndex1(IVBCompactSubRegTable[i] & 0x1F, i);
        BDWCompactSubRegTable.AddIndex2(IVBCompactSubRegTable[i] & 0x3FF, i);
        if (getGenxPlatform() >= GENX_ICLLP)
        {
            BDWCompactDataTypeTableStr.AddIndex(ICLCompactDataTypeTable[i], i);
        }
        else
        {
            BDWCompactDataTypeTableStr.AddIndex(BDWCompactDataTypeTable[i], i);
        }
    }

    int globalInstNum = 0;
    int globalHalfInstNum = 0;
    int numCompactedInst = 0;
    int numCompacted3SrcInst = 0;

    BB_LIST_ITER ib, bend(kernel.fg.end());
    for (ib = kernel.fg.begin(); ib != bend; ++ib)
    {
        G4_BB *bb = *ib;
        int localInstNum = 0;
        int localHalfInstNum = 0;

        /**
         * Traverse the instruction lists
         */
        INST_LIST_ITER ii, iend(bb->end());
        for (ii = bb->begin(); ii != iend; ++ii)
        {
            /* do detailed encoding here */
            G4_INST *inst = *ii;
            G4_opcode opcode = inst->opcode();

            if (opcode == G4_label)
            {
                inst->setBinInst(NULL);
            } else {
                // reuse ""BinInst"" from BinaryEncoding.h which can be simplified
                BinInst *bin = new (mem) BinInst();
                inst->setBinInst(bin);

                bin->DWords[0] = 0;
                bin->DWords[1] = 0;
                bin->DWords[2] = 0;
                bin->DWords[3] = 0;

                DoAllEncoding(inst);

                if (inst->opcode() == G4_pseudo_fc_call ||
                   inst->opcode() == G4_pseudo_fc_ret)
                {
                    inst->getBinInst()->SetDontCompactFlag(true);
                }

                if (doCompaction())
                {
                    inst->getBinInst()->SetMustCompactFlag(false);
                    inst->getBinInst()->SetDontCompactFlag(inst->isNoCompactedInst());

                    /**
                     * handling switch/case for gen6: jump table should not be compacted
                     */
                    bool compacted;
                    {
                        TIME_SCOPE(ENCODE_COMPACTION);
                        compacted = BinaryEncodingBase::compactOneInstruction(inst);
                    }

                    if (compacted)
                    {
                        if (kernel.getOption(vISA_OptReport))
                        {
                            numCompactedInst++;
                            if (inst->getBinInst()->GetIs3Src())
                                numCompacted3SrcInst++;
                        }
                        inst->setCompacted();
                    }
                }
                binInstList.push_back(inst->getBinInst());

                if (inst->opcode() >= G4_jmpi && inst->opcode() <= G4_join)
                {
                    if (!EncodeConditionalBranches(inst, globalHalfInstNum))
                    {
                        offsetVector.push_back(ForwardJmpOffset(inst, globalHalfInstNum));
                    }
                }
            } //else

            BuildLabelMap(inst, localHalfInstNum, localInstNum,
                          globalHalfInstNum, globalInstNum);
        } // for inst
    } // for bb

    kernel.setAsmCount(globalInstNum);
    SetInstCounts((uint32_t)globalHalfInstNum);

    EncodingHelper::dumpOptReport(globalInstNum, numCompactedInst, numCompacted3SrcInst, kernel);
    for (auto x = offsetVector.begin(), vEnd = offsetVector.end(); x != vEnd; x++)
    {
        if (!EncodeConditionalBranches(x->inst, x->offset))
        {
            MUST_BE_TRUE(false, ""invalid label!"");
        }
    }
}","/// \brief Entry point.
///
/// This is counterpart of ProduceBinaryInstructions from 'old' encoder","// Wrapper for the C function void glClipPlanef ( GLenum plane, const GLfloat *equation )",
"System::IDisposable* Zenject::DisposableManager::DisposableInfo::_get_Disposable() {
  static auto ___internal__logger = ::Logger::get().WithContext(""Zenject::DisposableManager::DisposableInfo::_get_Disposable"");
  auto ___internal__instance = *this;
  static auto ___internal__field__offset = THROW_UNLESS(il2cpp_utils::FindField(___internal__instance, ""Disposable""))->offset;
  return *reinterpret_cast<System::IDisposable**>(reinterpret_cast<char*>(this) + ___internal__field__offset);
}","// Completed includes
// Autogenerated instance field getter
// Get instance field: public System.IDisposable Disposable",/** Returns the entry with greatest key less than or equal to given key(if any)*/,
"void GlobalNamespace::PlayerSaveDataV1_0_1::PlayerSpecificSettings::_set_advancedHud(bool value) {
  static auto ___internal__logger = ::Logger::get().WithContext(""GlobalNamespace::PlayerSaveDataV1_0_1::PlayerSpecificSettings::_set_advancedHud"");
  auto ___internal__instance = this;
  static auto ___internal__field__offset = THROW_UNLESS(il2cpp_utils::FindField(___internal__instance, ""advancedHud""))->offset;
  *reinterpret_cast<bool*>(reinterpret_cast<char*>(this) + ___internal__field__offset) = value;
}","// Autogenerated instance field setter
// Set instance field: public System.Boolean advancedHud
","/**
	 * Write material to the output stream
	 * @param output stream
	 * @param material
	 * @throws IOException
	 */",
"func (b *Board) GetIsValid() bool {
	if b == nil || b.IsValid == nil {
		return false
	}
	return *b.IsValid
}","// GetIsValid returns the IsValid field if it's non-nil, zero value otherwise.","#
# Get the schema configuration at the API level.
#
# @param next_page_link [String] The NextLink from the previous successful call
# to List operation.
# @param [Hash{String => String}] A hash of custom headers that will be added
# to the HTTP request.
#
# @return [Concurrent::Promise] Promise object which holds the HTTP response.
#",
"func (client BaseClient) Unsubscribe4Responder(resp *http.Response) (result autorest.Response, err error) {
        err = autorest.Respond(
        resp,
        client.ByInspecting(),
        azure.WithErrorUnlessStatusCode(http.StatusOK),
        autorest.ByClosing())
        result.Response = resp
            return
        }","// Unsubscribe4Responder handles the response to the Unsubscribe4 request. The method always
// closes the http.Response Body.","# gets a range inbetween min and max by key
# could guarantee lexographical order when limit not used",
"func buildGVKPredicate(logger *log.Log) predicate.Funcs {
	logger = logger.WithName(""buildGVKPredicate"")
	return predicate.Funcs{
		UpdateFunc: updateFunc(logger),
		DeleteFunc: func(e event.DeleteEvent) bool {
			// evaluates to false if the object has been confirmed deleted
			return !e.DeleteStateUnknown
		},
	}
}","// buildGVKPredicate construct the predicates for all other GVKs, unless SBR.",# Sign a cert if one is waiting,
"func (mmGetMessagesFilters *PublisherServerMock) GetMessagesFilters(ctx context.Context, ep1 *mm_introproto.EmptyArgs) (ap1 *mm_introproto.AllMessageFilterStats, err error) {
	mm_atomic.AddUint64(&mmGetMessagesFilters.beforeGetMessagesFiltersCounter, 1)
	defer mm_atomic.AddUint64(&mmGetMessagesFilters.afterGetMessagesFiltersCounter, 1)

	if mmGetMessagesFilters.inspectFuncGetMessagesFilters != nil {
		mmGetMessagesFilters.inspectFuncGetMessagesFilters(ctx, ep1)
	}

	mm_params := &PublisherServerMockGetMessagesFiltersParams{ctx, ep1}

	// Record call args
	mmGetMessagesFilters.GetMessagesFiltersMock.mutex.Lock()
	mmGetMessagesFilters.GetMessagesFiltersMock.callArgs = append(mmGetMessagesFilters.GetMessagesFiltersMock.callArgs, mm_params)
	mmGetMessagesFilters.GetMessagesFiltersMock.mutex.Unlock()

	for _, e := range mmGetMessagesFilters.GetMessagesFiltersMock.expectations {
		if minimock.Equal(e.params, mm_params) {
			mm_atomic.AddUint64(&e.Counter, 1)
			return e.results.ap1, e.results.err
		}
	}

	if mmGetMessagesFilters.GetMessagesFiltersMock.defaultExpectation != nil {
		mm_atomic.AddUint64(&mmGetMessagesFilters.GetMessagesFiltersMock.defaultExpectation.Counter, 1)
		mm_want := mmGetMessagesFilters.GetMessagesFiltersMock.defaultExpectation.params
		mm_got := PublisherServerMockGetMessagesFiltersParams{ctx, ep1}
		if mm_want != nil && !minimock.Equal(*mm_want, mm_got) {
			mmGetMessagesFilters.t.Errorf(""PublisherServerMock.GetMessagesFilters got unexpected parameters, want: %#v, got: %#v%s\n"", *mm_want, mm_got, minimock.Diff(*mm_want, mm_got))
		}

		mm_results := mmGetMessagesFilters.GetMessagesFiltersMock.defaultExpectation.results
		if mm_results == nil {
			mmGetMessagesFilters.t.Fatal(""No results are set for the PublisherServerMock.GetMessagesFilters"")
		}
		return (*mm_results).ap1, (*mm_results).err
	}
	if mmGetMessagesFilters.funcGetMessagesFilters != nil {
		return mmGetMessagesFilters.funcGetMessagesFilters(ctx, ep1)
	}
	mmGetMessagesFilters.t.Fatalf(""Unexpected call to PublisherServerMock.GetMessagesFilters. %v %v"", ctx, ep1)
	return
}",// GetMessagesFilters implements introproto.PublisherServer,"# self_training_sample = pd.read_excel('self_training_sample.xlsx')
for sample in self_training_sample.iterrows():
    print(sample[1]['original_docstring'])
    print(sample[1]['original_string'])
    print('\n\n')",
"func (s *QueuedElementStore) Save(record *QueuedElement) (updated bool, err error) {
	if !record.IsPersisted() {
		return false, s.Insert(record)
	}

	rowsUpdated, err := s.Update(record)
	if err != nil {
		return false, err
	}

	return rowsUpdated > 0, nil
}","// Save inserts the object if the record is not persisted, otherwise it updates
// it. Same rules of Update and Insert apply depending on the case.","#
# Get the used, available, and total worker capacity an App Service
# Environment.
#
# Get the used, available, and total worker capacity an App Service
# Environment.
#
# @param next_page_link [String] The NextLink from the previous successful call
# to List operation.
# @param [Hash{String => String}] A hash of custom headers that will be added
# to the HTTP request.
#
# @return [Concurrent::Promise] Promise object which holds the HTTP response.
#",
"func (b *backend) Create(path riposo.Path, obj *schema.Object) error {
	if !path.IsNode() {
		return storage.ErrInvalidPath
	}

	now := riposo.EpochFromTime(b.cc.Now())
	ns, _ := path.Split()

	b.mu.Lock()
	defer b.mu.Unlock()

	if obj.ID != """" {
		if exst := b.tree.Get(ns, obj.ID); exst != nil {
			return storage.ErrObjectExists
		}
	} else {
		obj.ID = b.hlp.NextID()
	}

	if len(obj.Extra) == 0 {
		obj.Extra = append(obj.Extra, '{', '}')
	}

	b.dead.Unlink(ns, obj.ID)
	b.tree.FetchNode(ns, 0).Put(obj, now)
	return nil
}",// Create implements Transaction interface.,"#  Add a required authority for this auth handler
# @param [String] authority the authority
# @return [self]",
"func (m *CloudStateHashMock) MinimockSignWithDone() bool {
	for _, e := range m.SignWithMock.expectations {
		if mm_atomic.LoadUint64(&e.Counter) < 1 {
			return false
		}
	}

	// if default expectation was set then invocations count should be greater than zero
	if m.SignWithMock.defaultExpectation != nil && mm_atomic.LoadUint64(&m.afterSignWithCounter) < 1 {
		return false
	}
	// if func was set then invocations count should be greater than zero
	if m.funcSignWith != nil && mm_atomic.LoadUint64(&m.afterSignWithCounter) < 1 {
		return false
	}
	return true
}","// MinimockSignWithDone returns true if the count of the SignWith invocations corresponds
// the number of defined expectations","#
# Write the <sheetPr> element for Sheet level properties.
#",
"func (fs *fileSystem) calculateDxFileMetadata(path storage.DxPath, filename string) (*metadataForUpdate, error) {
	// Deal with the file names. Input path is the DxPath of the target directory.
	// filename is the system filename of the dxfile.
	filenameNoSuffix := strings.TrimSuffix(filename, storage.DxFileExt)
	fileDxPath, err := path.Join(filenameNoSuffix)
	if err != nil {
		return nil, err
	}
	// Open the DxPath
	file, err := fs.fileSet.Open(fileDxPath)
	if err != nil {
		return nil, fmt.Errorf(""cannot open DxPath %v: %v"", fileDxPath.Path, err)
	}
	defer file.Close()

	// Get the healthInfoMap, mark all healthy as unstuck, and then calculate the health
	healthInfoTable := fs.contractManager.HostHealthMapByID(file.HostIDs())
	if err = file.MarkAllUnhealthySegmentsAsStuck(healthInfoTable); err != nil {
		return nil, fmt.Errorf(""cannot mark stuck segments for file %v: %v"", fileDxPath.Path, err)
	}
	if err = file.MarkAllHealthySegmentsAsUnstuck(healthInfoTable); err != nil {
		return nil, fmt.Errorf(""cannot mark unstuck segments for file %v: %v"", fileDxPath.Path, err)
	}
	health, stuckHealth, numStuckSegments := file.Health(healthInfoTable)
	redundancy := file.Redundancy(healthInfoTable)

	// Update TimeLastHealthCheck
	if err := file.SetTimeLastHealthCheck(time.Now()); err != nil {
		return nil, fmt.Errorf(""cannot SetTimeLastHealthCheck for file %v: %v"", fileDxPath.Path, err)
	}
	cachedMetadata := dxfile.CachedHealthMetadata{
		Health:      health,
		StuckHealth: stuckHealth,
		Redundancy:  redundancy,
	}
	// apply cached metadata and return
	return &metadataForUpdate{
		numFiles:            1,
		totalSize:           file.FileSize(),
		health:              health,
		stuckHealth:         stuckHealth,
		minRedundancy:       redundancy,
		numStuckSegments:    numStuckSegments,
		timeLastHealthCheck: time.Now(),
	}, file.ApplyCachedHealthMetadata(cachedMetadata)
}","// calculateDxFileMetadata update, calculate and apply the health related field of a dxfile.",# Returns a Rack response triple for the uploaded file.,
"func (lp *JournalPersister) Recover() (chan []byte, error) {
	log.Info().Msg(""JournalPersister:Recover starting recovery"")

	// Get a reader from the store
	sr, err := lp.storage.Reader()
	if err != nil {
		err = errors.Wrap(err, ""Failed to open store"")
		log.Error().Err(err).Msg(""JournalPersister:Recover"")
		return nil, err
	}
	// Get a new journal reader wrapping the store reader
	r := journal.NewReader(sr, nil, false, true)
	if err != nil {
		err = errors.Wrap(err, ""Failed to open peristence journal reader"")
		log.Error().Err(err).Msg(""JournalPersister:Recover"")
		return nil, err
	}

	log.Info().Msg(""JournalPersister:Recover streaming items for recovery"")
	bufC := make(chan []byte)
	go func() {
		defer close(bufC)
		// Close storage reader
		defer sr.Close()

		for {
			j, err := r.Next()
			if err == io.EOF {
				break
			}
			if err != nil {
				err = errors.Wrap(err, ""Failed to fetch next journal reader"")
				log.Error().Err(err).Msg(""JournalPersister:Recover"")
				break
			}
			buf, err := ioutil.ReadAll(j)
			if err != nil {
				log.Debug().Err(err).Msg(""JournalPersister:Recover error reading from journal"")
				continue
			}
			bufC <- buf
		}
		log.Info().Msg(""JournalPersister:Recover finished recovery stream"")
	}()

	return bufC, nil
}",// Recover reads back persisted data and emits entries,"#
# GenerateAnswer call to query the knowledgebase.
#
# @param kb_id [String] Knowledgebase id.
# @param generate_answer_payload [QueryDTO] Post body of the request.
# @param [Hash{String => String}] A hash of custom headers that will be added
# to the HTTP request.
#
# @return [Concurrent::Promise] Promise object which holds the HTTP response.
#",
"func extractFromOwnerReference(v reflect.Value, o *metatypes.OwnerReference) error {
	if err := runtime.Field(v, ""APIVersion"", &o.APIVersion); err != nil {
		return err
	}
	if err := runtime.Field(v, ""Kind"", &o.Kind); err != nil {
		return err
	}
	if err := runtime.Field(v, ""Name"", &o.Name); err != nil {
		return err
	}
	if err := runtime.Field(v, ""UID"", &o.UID); err != nil {
		return err
	}
	var controllerPtr *bool
	if err := runtime.Field(v, ""Controller"", &controllerPtr); err != nil {
		return err
	}
	if controllerPtr != nil {
		controller := *controllerPtr
		o.Controller = &controller
	}
	return nil
}",// extractFromOwnerReference extracts v to o. v is the OwnerReferences field of an object.,# puts the screenshot into the frame,
"func cleanUpMigrateGitConfig(configPath string) error {
	cfg, err := ini.Load(configPath)
	if err != nil {
		return fmt.Errorf(""open config file: %v"", err)
	}
	cfg.DeleteSection(""remote \""origin\"""")
	if err = cfg.SaveToIndent(configPath, ""\t""); err != nil {
		return fmt.Errorf(""save config file: %v"", err)
	}
	return nil
}","// cleanUpMigrateGitConfig removes mirror info which prevents ""push --all"".
// This also removes possible user credentials.","#
# Describe all snapshots
#
# List all snapshots associated with the volume
#
# @param resource_group_name [String] The name of the resource group.
# @param account_name [String] The name of the NetApp account
# @param pool_name [String] The name of the capacity pool
# @param volume_name [String] The name of the volume
# @param [Hash{String => String}] A hash of custom headers that will be added
# to the HTTP request.
#
# @return [Concurrent::Promise] Promise object which holds the HTTP response.
#",
"func (c *Config) QueryConfig() *exec.QueryConfig {
	return &exec.QueryConfig{
		Database: c.Database,
		Location: c.Location,
		Encrypt:  c.Encrypt,
		KMS:      c.KMS,
	}
}",// QueryConfig creates an exec.QueryConfig struct based on c.,"# override destroy to get soft delete like acts_as_paranoid style delete
# Note: delete_all (used in helper) bypasses this and deletes all rows.",
"func NewRandomInUnitSphereVec3() *Vec3 {
	for true {
		p := NewRandomInRangeVec3(-1, 1)
		if p.SquaredLen() >= 1 {
			continue
		}
		return p
	}
	return nil
}",// NewRandomInUnitSphereVec3 generate a random vector in unit sphere.,"# Typecasts the given enumerator to its actual value stored in the
# database.  This will only convert symbols to strings.  All other values
# will remain in the same type.",
"func (recv *Settings) ConnectChanged(callback SettingsSignalChangedCallback) int {
	signalSettingsChangedLock.Lock()
	defer signalSettingsChangedLock.Unlock()

	signalSettingsChangedId++
	instance := C.gpointer(recv.native)
	handlerID := C.Settings_signal_connect_changed(instance, C.gpointer(uintptr(signalSettingsChangedId)))

	detail := signalSettingsChangedDetail{callback, handlerID}
	signalSettingsChangedMap[signalSettingsChangedId] = detail

	return signalSettingsChangedId
}","/*
ConnectChanged connects the callback to the 'changed' signal for the Settings.

The returned value represents the connection, and may be passed to DisconnectChanged to remove it.
*/","#
# Creates a new managed application.
#
# @param resource_group_name [String] The name of the resource group. The name
# is case insensitive.
# @param application_name [String] The name of the managed application.
# @param parameters [Application] Parameters supplied to the create or update a
# managed application.
# @param [Hash{String => String}] A hash of custom headers that will be added
# to the HTTP request.
#
# @return [Concurrent::Promise] Promise object which holds the HTTP response.
#",
"func (s * Snowflake) Msg1() {
	log.Infof(""Enter Msg1"")
	s.slot++
	s.log[s.slot] = &entry {
		request: s.request,
		quorum: paxi.NewQuorum(),
		timestamp: time.Now(),
	}

	for i := 0; i < s.noOfSamples; i++ {
		log.Infof(""For round: %v"", i)
		s.MulticastToSample((i % 3) + 1, Msg1{ID: s.ID(), Col: s.Col})
	}
	log.Infof(""Exit Msg1"")
}","/*
Starts gossip by multicasting to the random sample of nodes
 */","# gets first env from the config 
# should be able to drop a lot of this once everyone's on /configs",
"func (g GenOp) Update(ctx context.Context, scope string, channel *meta.Channel) error {
	logger.Info(""operator trying to update channel"",
		zap.Any(""channel"", channel.Meta.Name),
		zap.Any(""scope"", scope),
	)
	op, err := g.getOperator(scope, channel.Meta.Name, false)
	if err != nil {
		return err
	}
	return op.Update(ctx, scope, channel)
}",//Update executes Update method of correct operator given the desired channel's broker,# Set the :type option to :select if it hasn't been set.,
"func (l *LakeFsSdk) ObjectMetaData(repository string, ref string, path string) (*Metadata, error) {
	var resp Metadata
	err := l.auth(urllib.Get(fmt.Sprintf(""%s/api/v1/repositories/%s/refs/%s/objects/stat"", l.addr, repository, ref))).Queries(""path"", path).FromJsonByCode(&resp, 200)
	if err != nil {
		return nil, err
	}

	return &resp, nil
}","// ObjectMetaData ObjectMetaData, (ref: branch or commit id)","# moves and rotates camera back to player
# this thing NEEDS to be module!",
"func (o *ServiceRevision) GetChildId() string {
	if o == nil || o.ChildId == nil {
		var ret string
		return ret
	}
	return *o.ChildId
}","// GetChildId returns the ChildId field value if set, zero value otherwise.",# verify that ship type and number matches with initial requirements,
"func (o *OpenStackCloudSpaceRequestAllOf) GetDnsServerIpAddressesOk() (*string, bool) {
	if o == nil || o.DnsServerIpAddresses == nil {
		return nil, false
	}
	return o.DnsServerIpAddresses, true
}","// GetDnsServerIpAddressesOk returns a tuple with the DnsServerIpAddresses field value if set, nil otherwise
// and a boolean to check if the value has been set.","# Find a plan by its ID.
# Params:
# +plan_id+:: ID of the plan
# +options+:: +Hash+ of options",
"func parseKernxSubtable2(data []byte, headerLength int, extended bool, numGlyphs int, tupleCount int) (out Kern2, err error) {
	subHeaderLength := 8
	if extended {
		subHeaderLength = 16
	}
	if len(data) < headerLength+subHeaderLength {
		return out, errors.New(""invalid kern/x subtable format 2 (EOF)"")
	}

	out.hasTuples = tupleCount != 0

	var leftOffset, rightOffset, arrayOffset uint32
	if extended {
		// out.rowWidth = binary.BigEndian.Uint32(data[headerLength:])
		leftOffset = binary.BigEndian.Uint32(data[headerLength+4:])
		rightOffset = binary.BigEndian.Uint32(data[headerLength+8:])
		arrayOffset = binary.BigEndian.Uint32(data[headerLength+12:])
	} else {
		// out.rowWidth = uint32(binary.BigEndian.Uint16(data[headerLength:]))
		leftOffset = uint32(binary.BigEndian.Uint16(data[headerLength+2:]))
		rightOffset = uint32(binary.BigEndian.Uint16(data[headerLength+4:]))
		arrayOffset = uint32(binary.BigEndian.Uint16(data[headerLength+6:]))
	}

	if len(data) < int(arrayOffset) {
		return out, errors.New(""invalid kerx subtable format 2 (EOF)"")
	}

	if extended {
		out.left, err = parseAATLookupTable(data, leftOffset, numGlyphs, false)
		if err != nil {
			return out, err
		}
		out.right, err = parseAATLookupTable(data, rightOffset, numGlyphs, false)
		if err != nil {
			return out, err
		}
	} else {
		out.left, err = parseClassFormat1(data[leftOffset:], 2)
		if err != nil {
			return out, fmt.Errorf(""invalid kern subtable format 2: %s"", err)
		}
		out.right, err = parseClassFormat1(data[rightOffset:], 2)
		if err != nil {
			return out, fmt.Errorf(""invalid kern subtable format 2: %s"", err)
		}
	}
	out.tableData = data                      // since the class already has the offset, just store the raw slice
	out.kerningArrayOffset = int(arrayOffset) // store it to check for invalid offset values
	return out, err
}",// data starts at the subtable header,"# load first from users binary directory. then load built-in commands if
# available",
"public okhttp3.Call modifySupplierAsync(
      Integer companyId,
      Integer supplierId,
      ModifySupplierRequest modifySupplierRequest,
      final ApiCallback<ModifySupplierResponse> _callback)
      throws ApiException {

    okhttp3.Call localVarCall =
        modifySupplierValidateBeforeCall(companyId, supplierId, modifySupplierRequest, _callback);
    Type localVarReturnType = new TypeToken<ModifySupplierResponse>() {}.getType();
    localVarApiClient.executeAsync(localVarCall, localVarReturnType, _callback);
    return localVarCall;
  }","/**
   * Modify Supplier (asynchronously) Modifies the specified supplier.
   *
   * @param companyId The ID of the company. (required)
   * @param supplierId The ID of the supplier. (required)
   * @param modifySupplierRequest The modified Supplier. First level parameters are managed in delta
   *     mode. (optional)
   * @param _callback The callback to be executed when the API call finishes
   * @return The request call
   * @throws ApiException If fail to process the API call, e.g. serializing the request body object
   * @http.response.details
   *     <table border=""1"">
   * <caption>Response Details</caption>
   * <tr><td> Status Code </td><td> Description </td><td> Response Headers </td></tr>
   * <tr><td> 200 </td><td> Example response </td><td>  -  </td></tr>
   * <tr><td> 401 </td><td> Unauthorized </td><td>  -  </td></tr>
   * <tr><td> 404 </td><td> Not Found </td><td>  -  </td></tr>
   * </table>
   */","/// Verify that the contents of the file match the specified pattern.
/// Someday this should support `std::str::pattern::Pattern` so that we
/// can support both strings and regular expressions, but that hasn't
/// been stabilized yet.",
"public void addCleanupAction(String					   sKey,
								 Consumer<ProcessFragment> fAction)
	{
		aCleanupActions.put(sKey, fAction);
	}","/***************************************
	 * Registers an cleanup action that will be executed when this process
	 * fragment is finished, i.e. the fragment is removed, the process continues
	 * to the next step, or terminates (regularly or with an error). If a
	 * different finish action is already registered under a particular key it
	 * will be replaced. Therefore invoking code must make sure to use unique
	 * keys or handle the replacement of actions in appropriate ways.
	 *
	 * <p>When invoked a cleanup action receives the process fragment as it's
	 * argument to provide access to process parameters. Registered Actions can
	 * be removed with {@link #removeCleanupAction(String)}.</p>
	 *
	 * @param sKey    A key that identifies the action for later removal
	 * @param fAction The function to invoke on cleanup
	 */","/// Calls the script's `schema` function.
/// Typically used to create database schema.",
"private String path(HttpMethod method, String subPath) {
        return method.name().concat("" "")
                .concat(InetAddress.getLoopbackAddress().getCanonicalHostName())
                .concat(SERVICE_PATH).concat(subPath);
    }","/**
     * Get full path and http method
     * @param method    HttpMethod
     * @param subPath   Sub path
     * @return          String
     */",/// Compress a conditional expression if cons and alt is simillar,
"public static void stopAlarm(Context context, AlarmInstance instance) {
        final Intent intent = AlarmInstance.createIntent(context, AlarmService.class, instance.mId)
                .setAction(STOP_ALARM_ACTION);

        // We don't need a wake lock here, since we are trying to kill an alarm
        context.startService(intent);
    }","/**
     * Utility method to help stop an alarm properly. Nothing will happen, if alarm is not firing
     * or using a different instance.
     *
     * @param context application context
     * @param instance you are trying to stop
     */",/// Create an edit for the given location and text.,
"public GadgetUpdateQuery usagesRemaining() {
    startField(""usages_remaining"");

    return this;
  }","/**
   * Amount of usages remaining AFTER the current use.
   */","///Optionally returns the binding power of an infix operator.
///
///This is at the core of Pratt's method for parsing, because
///it totally orders the precedence of each operator.
///
///If the operator is not valid, returns None.",
"public String updateServiceCatalog(ServiceEntry serviceEntry, int tenantID, String userName)
            throws APIManagementException {
        try (Connection connection = APIMgtDBUtil.getConnection();
             PreparedStatement ps = connection
                     .prepareStatement(SQLConstants.ServiceCatalogConstants.UPDATE_SERVICE_BY_KEY)) {
            boolean initialAutoCommit = connection.getAutoCommit();

            try {
                connection.setAutoCommit(false);
                setUpdateServiceParams(ps, serviceEntry, tenantID, userName);
                ps.executeUpdate();
                connection.commit();
            } catch (SQLException e) {
                connection.rollback();
                handleException(""Failed to rollback updating endpoint information"", e);
            } finally {
                APIMgtDBUtil.setAutoCommit(connection, initialAutoCommit);
            }
        } catch (SQLException e) {
            handleException(""Failed to update service catalog of tenant ""
                    + APIUtil.getTenantDomainFromTenantId(tenantID), e);
        }
        return serviceEntry.getKey();
    }","/**
     * Update an existing serviceCatalog
     *
     * @param serviceEntry ServiceCatalogInfo
     * @param tenantID     ID of the owner's tenant
     * @param userName     Logged in user name
     * @return serviceCatalogId
     * throws APIManagementException if failed to create service catalog
     */","// An internal function to generate a ranking of players based on their best bet.
// It uses the accuracy to calculate how good a bet is and the ranking is so that
// the best players are higher up in the returning vector.",
"public Vector2f rotate(float angle) {
        double rad = Math.toRadians(angle);
        double cos = Math.cos(rad);
        double sin = Math.sin(rad);

        return new Vector2f((float) (x * cos - y * sin), (float) (x * sin + y * cos));
    }","/**
     * Rotates the vector's data by an float angle
     * @param angle Rotating angle.
     * @return Vector's data rotated.
     */",/// Append a file to our model.,
"public static String graphLocation()
	{
		String location = System.getProperty(""hgdbowl.defaultdb"");
		if (location == null)
			return System.getProperty(""java.io.tmpdir"") + File.separator + ""hgdbowl.defaultdb"";
		else
			return location;
	}","/**
	 * <p>Return the default graph location as specified by the <code>hgdbowl.defaultdb</code>
	 * property. If no such system property is specified, the default location will be
	 * <code>System.getProperty(""java.io.tmpdir"") + File.separator + ""hgdbowl.defaultdb""</code>.
	 */","/// Get the nth (positive) laguerre polynomial.
///
/// Gets the nth (positive) laguerre polynomial over a specified field. This is
/// done using the direct formula and is properly normalized.
///
/// # Examples
/// ```
/// use bacon_sci::special::laguerre;
/// fn example() {
///     let p_3 = laguerre::<f64>(3, 1e-8).unwrap();
///     assert_eq!(p_3.order(), 3);
///     assert!((p_3.get_coefficient(0) - 1.0).abs() < 0.00001);
///     assert!((p_3.get_coefficient(1) + 3.0).abs() < 0.00001);
///     assert!((p_3.get_coefficient(2) - 9.0/6.0).abs() < 0.00001);
///     assert!((p_3.get_coefficient(3) + 1.0/6.0).abs() < 0.00001);
/// }
///",
"private void goodB2G2() throws Throwable
    {
        int data;

        switch (6)
        {
        case 6:
            data = Integer.MIN_VALUE; /* Initialize data */
            {
                InputStreamReader readerInputStream = null;
                BufferedReader readerBuffered = null;
                /* read user input from console with readLine */
                try
                {
                    readerInputStream = new InputStreamReader(System.in, ""UTF-8"");
                    readerBuffered = new BufferedReader(readerInputStream);
                    /* POTENTIAL FLAW: Read data from the console using readLine */
                    String stringNumber = readerBuffered.readLine();
                    if (stringNumber != null) // avoid NPD incidental warnings
                    {
                        try
                        {
                            data = Integer.parseInt(stringNumber.trim());
                        }
                        catch(NumberFormatException exceptNumberFormat)
                        {
                            IO.logger.log(Level.WARNING, ""Number format exception parsing data from string"", exceptNumberFormat);
                        }
                    }
                }
                catch (IOException exceptIO)
                {
                    IO.logger.log(Level.WARNING, ""Error with stream reading"", exceptIO);
                }
                finally
                {
                    try
                    {
                        if (readerBuffered != null)
                        {
                            readerBuffered.close();
                        }
                    }
                    catch (IOException exceptIO)
                    {
                        IO.logger.log(Level.WARNING, ""Error closing BufferedReader"", exceptIO);
                    }

                    try
                    {
                        if (readerInputStream != null)
                        {
                            readerInputStream.close();
                        }
                    }
                    catch (IOException exceptIO)
                    {
                        IO.logger.log(Level.WARNING, ""Error closing InputStreamReader"", exceptIO);
                    }
                }
            }
            /* NOTE: Tools may report a flaw here because readerBuffered and readerInputStream are not closed.  Unfortunately, closing those will close System.in, which will cause any future attempts to read from the console to fail and throw an exception */
            break;
        default:
            /* INCIDENTAL: CWE 561 Dead Code, the code below will never run
             * but ensure data is inititialized before the Sink to avoid compiler errors */
            data = 0;
            break;
        }

        switch (7)
        {
        case 7:
            /* FIX: Add a check to prevent an overflow from occurring */
            if (data < Integer.MAX_VALUE)
            {
                data++;
                int result = (int)(data);
                IO.writeLine(""result: "" + result);
            }
            else
            {
                IO.writeLine(""data value is too large to increment."");
            }
            break;
        default:
            /* INCIDENTAL: CWE 561 Dead Code, the code below will never run */
            IO.writeLine(""Benign, fixed string"");
            break;
        }
    }",/* goodB2G2() - use badsource and goodsink by reversing the blocks in the second switch  */,/// Process a block with the SHA-256 algorithm.,
"public static void glPushDebugGroup(@NativeType(""GLenum"") int source, @NativeType(""GLuint"") int id, @NativeType(""GLchar const *"") CharSequence message) {
        MemoryStack stack = stackGet(); int stackPointer = stack.getPointer();
        try {
            int messageEncodedLength = stack.nUTF8(message, false);
            long messageEncoded = stack.getPointerAddress();
            nglPushDebugGroup(source, id, messageEncodedLength, messageEncoded);
        } finally {
            stack.setPointer(stackPointer);
        }
    }","/**
     * Pushes a debug group described by the string {@code message} into the command stream. The value of {@code id} specifies the ID of messages generated.
     * The parameter {@code length} contains the number of characters in {@code message}. If {@code length} is negative, it is implied that {@code message}
     * contains a null terminated string. The message has the specified {@code source} and {@code id}, {@code type} {@link #GL_DEBUG_TYPE_PUSH_GROUP DEBUG_TYPE_PUSH_GROUP}, and
     * {@code severity} {@link #GL_DEBUG_SEVERITY_NOTIFICATION DEBUG_SEVERITY_NOTIFICATION}. The GL will put a new debug group on top of the debug group stack which inherits the control of the
     * volume of debug output of the debug group previously residing on the top of the debug group stack. Because debug groups are strictly hierarchical, any
     * additional control of the debug output volume will only apply within the active debug group and the debug groups pushed on top of the active debug group.
     * 
     * <p>An {@link GL11#GL_INVALID_ENUM INVALID_ENUM} error is generated if the value of {@code source} is neither {@link #GL_DEBUG_SOURCE_APPLICATION DEBUG_SOURCE_APPLICATION} nor {@link #GL_DEBUG_SOURCE_THIRD_PARTY DEBUG_SOURCE_THIRD_PARTY}. An
     * {@link GL11#GL_INVALID_VALUE INVALID_VALUE} error is generated if {@code length} is negative and the number of characters in {@code message}, excluding the null-terminator, is
     * not less than the value of {@link #GL_MAX_DEBUG_MESSAGE_LENGTH MAX_DEBUG_MESSAGE_LENGTH}.</p>
     *
     * @param source  the source of the debug message. One of:<br><table><tr><td>{@link #GL_DEBUG_SOURCE_APPLICATION DEBUG_SOURCE_APPLICATION}</td><td>{@link #GL_DEBUG_SOURCE_THIRD_PARTY DEBUG_SOURCE_THIRD_PARTY}</td></tr></table>
     * @param id      the identifier of the message
     * @param message a string containing the message to be sent to the debug output stream
     * 
     * @see <a target=""_blank"" href=""http://docs.gl/gl4/glPushDebugGroup"">Reference Page</a>
     */","// Polyfill until slice_fill feature stabilizes
// TODO: check if it optimizes to memset",
"private static boolean checkFilter(final String name, final boolean ignoreCase, final String[] array, final boolean returnValueIfEmpty) {
    if ((array == null) || (array.length == 0)) return returnValueIfEmpty;
    for (final String s: array) {
      if (s == null) continue;
      final boolean b = ignoreCase ? s.equalsIgnoreCase(name) : s.equals(name);
      if (b) return true;
    }
    return false;
  }","/**
   * Check whether the specified name matches a value in the specified array.
   * @param name the name to check.
   * @param ignoreCase determines whether name comparisons should ignore case.
   * @param array the filter array to check against.
   * @param returnValueIfEmpty the value to return if the array is null or empty.
   * @return true if the name matches one of the values in the array, false otherwise.
   */",/// Explore to get latest release information,
"public double averageLog2JointProbability(String refCategory,
                                              String responseCategory) {
        validateCategory(refCategory);
        validateCategory(responseCategory);
        double sum = 0.0;
        int count = 0;
        for (int i = 0; i < mReferenceCategories.size(); ++i) {
            if (mReferenceCategories.get(i).equals(refCategory)) {
                JointClassification c
                    = (JointClassification) mClassifications.get(i);
                for (int rank = 0; rank < c.size(); ++rank) {
                    if (c.category(rank).equals(responseCategory)) {
                        sum += c.jointLog2Probability(rank);
                        ++count;
                        break;
                    }
                }
            }
        }
        return sum / (double) count;
    }","/**
     * Returns the average log (base 2) joint probability of the
     * response category for cases of the specified reference
     * category.  If there are no cases matching the reference
     * category, the result is <code>Double.NaN</code>.
     *
     * <P>Better classifiers return high values when the reference
     * and response categories are the same and lower values
     * when they are different.  Unlike the conditional probability
     * values, joint probability averages are not particularly
     * useful because they are not normalized by input length.  For
     * the language model classifiers, the scores are normalized
     * by length, and provide a better cross-case view.
     *
     * @param refCategory Reference category.
     * @param responseCategory Response category.
     * @return Average log (base 2) conditional probability of
     * response category in cases for specified reference category.
     * @throws IllegalArgumentException If the either category is unknown.
     * @throws ClassCastException if the classifications are not joint
     * classifications.
     */","/// broadcast messages from the leader to layer 1 nodes
/// # Remarks",
"public <T1> boolean hasTTL(T1 key, String keySplTypeName, boolean encodeKey) throws StoreFactoryException {
		boolean result;
		long[] err = new long[1];
		DpsHelper dps = null;

                // Validate that the caller can ask us not to encode the key only for rstring data type.
                if (encodeKey == false && keySplTypeName != ""rstring"") {
                   // It is non-rstring data type. We must base64 encode the data.
                   encodeKey = true;
                }
		
		try {
			dps = DpsHelperHolder.getDpsHelper();
			result = dps.dpsHasTTL(key, keySplTypeName, """", err, encodeKey);
		} catch(Exception e) {
			// Either dps cannot be initialized or hasTTL went wrong.
			throw new StoreFactoryException(65535, e.getMessage());
		}
		
		if (err[0] != 0) {
			throw new StoreFactoryException(dps.dpsGetLastErrorCodeTTL(), 
				dps.dpsGetLastErrorStringTTL());
		}
		
		return(result);		
	}",// Check if a TTL based K/V pair for a given key exists in the global area of the back-end data store.,/// Check if all tasks for this group have been completed,
"@Test
    public void testPrivateTorrent() throws IOException {
        TorrentBuilder torrentBuilder = makeTorrentBuilder(CREATED_BY, Collections.singletonList(testfileName));
        torrentBuilder.privateFlag(true);

        Torrent decodedTorrent = buildAndDecodeTorrent(torrentBuilder);
        Assert.assertTrue(decodedTorrent.isPrivate());
    }","/**
     * Test making a simple torrent with a single file
     *
     * @throws IOException on failure
     */",/// This constructs a Config struct with all None,
"public static GetSnapshotContentsTaskResult deserialize(String taskResult) {
        JaxbJsonSerializer<GetSnapshotContentsTaskResult> serializer =
            new JaxbJsonSerializer<>(GetSnapshotContentsTaskResult.class);
        try {
            return serializer.deserialize(taskResult);
        } catch (IOException e) {
            throw new SnapshotDataException(
                ""Unable to create task result due to: "" + e.getMessage());
        }
    }","/**
     * Parses properties from task result
     *
     * @param taskResult - JSON formatted set of properties
     */",/// Wait until the services of the given device have been resolved.,
"@Test
    void whenNotParemetersGiven_thenReturnBadRequest() throws Exception {
        mvc.perform(get(BASE_QUALITY_URI).contentType(MediaType.APPLICATION_JSON))
                .andExpect(status().isBadRequest());
    }","/*
        Test whenever some/or all parameters were not passed within the
        Endpoint
     */","/// Returns the length of the common prefix with the `other` name; e. g.
/// the when `other = 11110000` and `self = 11111111` this is 4.",
"private void StartSensorButtonActionPerformed(java.awt.event.ActionEvent evt) {//GEN-FIRST:event_StartSensorButtonActionPerformed

        try {
            if (!currentSensor.getStatus().equals(""ON"")) {
                station.startSensor(currentSensor);
                StatusTextField.setText(currentSensor.getStatus());

            } else {
                JOptionPane.showMessageDialog(WindComboBox, ""Sensor is already on"");
            }

        } catch (Exception e) {
            JOptionPane.showMessageDialog(WindComboBox, ""No sensor Selected"");
        }
//        System.out.println(currentSensor.getSensorData().toString());


    }","/**
     * Start the current sensor selected by user
     *
     * @param evt
     */","/// Provides an API similar to `Stream`, except that it cannot error.",
"public static native void glClipPlanef(
        int plane,
        float[] equation,
        int offset
    );","// C function void glClipPlanef ( GLenum plane, const GLfloat *equation )","/// Shows a button with the given color.
/// If the user clicks the button, a full color picker is shown.
/// The given color is in `sRGBA` space with premultiplied alpha",
"@Override
    public Entry floorEntry(K key) throws IllegalArgumentException {
        int j=findIndex(key);
        if(j==size()|| !key.equals(table.get(j).getKey()))
            // look one earlier unless we had found a perfect match
            {j--;}
        return safeEntry(j);
    }",/** Returns the entry with greatest key less than or equal to given key(if any)*/,/// Sets a custom storage adapter to be used.,
"private static void writeMaterial(OutputStream os, Material m) throws IOException {
		writeString(os, m.getId());
		writeFloatArray(os, m.getAmbientColor().getArray());
		writeFloatArray(os, m.getDiffuseColor().getArray());
		writeFloatArray(os, m.getSpecularColor().getArray());
		writeFloatArray(os, m.getEmissionColor().getArray());
		writeFloat(os, m.getShininess());
		writeString(os, m.getDiffuseTexturePathName());
		writeString(os, m.getDiffuseTextureFileName());
		writeString(os, m.getSpecularTexturePathName());
		writeString(os, m.getSpecularTextureFileName());
		writeString(os, m.getNormalTexturePathName());
		writeString(os, m.getNormalTextureFileName());
		writeString(os, m.getDisplacementTexturePathName());
		writeString(os, m.getDisplacementTextureFileName());
	}","/**
	 * Write material 
	 * @param output stream
	 * @param material
	 * @throws IOException
	 */",/// hexcode-to-rgb converter. panics on malformed RGB hex codes,
"def list_by_api_next_async(next_page_link, custom_headers:nil)
      fail ArgumentError, 'next_page_link is nil' if next_page_link.nil?


      request_headers = {}
      request_headers['Content-Type'] = 'application/json; charset=utf-8'

      # Set Headers
      request_headers['x-ms-client-request-id'] = SecureRandom.uuid
      request_headers['accept-language'] = @client.accept_language unless @client.accept_language.nil?
      path_template = '{nextLink}'

      request_url = @base_url || @client.base_url

      options = {
          middlewares: [[MsRest::RetryPolicyMiddleware, times: 3, retry: 0.02], [:cookie_jar]],
          skip_encoding_path_params: {'nextLink' => next_page_link},
          headers: request_headers.merge(custom_headers || {}),
          base_url: request_url
      }
      promise = @client.make_request_async(:get, path_template, options)

      promise = promise.then do |result|
        http_response = result.response
        status_code = http_response.status
        response_content = http_response.body
        unless status_code == 200
          error_model = JSON.load(response_content)
          fail MsRest::HttpOperationError.new(result.request, http_response, error_model)
        end

        result.request_id = http_response['x-ms-request-id'] unless http_response['x-ms-request-id'].nil?
        result.correlation_request_id = http_response['x-ms-correlation-request-id'] unless http_response['x-ms-correlation-request-id'].nil?
        result.client_request_id = http_response['x-ms-client-request-id'] unless http_response['x-ms-client-request-id'].nil?
        # Deserialize Response
        if status_code == 200
          begin
            parsed_response = response_content.to_s.empty? ? nil : JSON.load(response_content)
            result_mapper = Azure::ApiManagement::Mgmt::V2019_12_01::Models::SchemaCollection.mapper()
            result.body = @client.deserialize(result_mapper, parsed_response)
          rescue Exception => e
            fail MsRest::DeserializationError.new('Error occurred in deserializing the response', e.message, e.backtrace, result)
          end
        end

        result
      end

      promise.execute
    end","#
# Get the schema configuration at the API level.
#
# @param next_page_link [String] The NextLink from the previous successful call
# to List operation.
# @param [Hash{String => String}] A hash of custom headers that will be added
# to the HTTP request.
#
# @return [Concurrent::Promise] Promise object which holds the HTTP response.
#",,
"def zrevrangebyscore(key, max, min, withscores: false, with_scores: withscores, limit: nil)
      on_result(@redis.zrevrangebyscore(encrypt_key(key), max, min, with_scores: with_scores, limit: limit)) do |res|
        if with_scores
          res.map { |v, s| [decrypt_member(v), s] }
        else
          res.map { |v| decrypt_member(v) }
        end
      end
    end",# could guarantee lexographical order when limit not used,,
"def sign(certname)
        waiting, signed = certificates

        raise('Already have a certificate for %s. Not attempting to sign again' % certname) if signed.include?(certname)

        if waiting.include?(certname)
          output = ''
          Shell.new('%s sign %s --color=none' % [@puppetca, certname], :stdout => output).runcommand
          return output
        else
          raise('Could not find certificate to sign: %s' % certname)
        end
      end",# Sign a cert if one is waiting,,
"def all_validation_errors
      additional_errors.each do |field, error|
        errors.add(field, error)
      end

      errors.each do |field|
        errors[field].uniq!
      end

      errors
    end","# A list of automatically and manually added errors for the model.
#
# @return [ActiveModel::Errors] A list of automatically and manually added errors for the model.",,
"def list_capacities_next_async(next_page_link, custom_headers:nil)
      fail ArgumentError, 'next_page_link is nil' if next_page_link.nil?


      request_headers = {}
      request_headers['Content-Type'] = 'application/json; charset=utf-8'

      # Set Headers
      request_headers['x-ms-client-request-id'] = SecureRandom.uuid
      request_headers['accept-language'] = @client.accept_language unless @client.accept_language.nil?
      path_template = '{nextLink}'

      request_url = @base_url || @client.base_url

      options = {
          middlewares: [[MsRest::RetryPolicyMiddleware, times: 3, retry: 0.02], [:cookie_jar]],
          skip_encoding_path_params: {'nextLink' => next_page_link},
          headers: request_headers.merge(custom_headers || {}),
          base_url: request_url
      }
      promise = @client.make_request_async(:get, path_template, options)

      promise = promise.then do |result|
        http_response = result.response
        status_code = http_response.status
        response_content = http_response.body
        unless status_code == 200
          error_model = JSON.load(response_content)
          fail MsRest::HttpOperationError.new(result.request, http_response, error_model)
        end

        result.request_id = http_response['x-ms-request-id'] unless http_response['x-ms-request-id'].nil?
        result.correlation_request_id = http_response['x-ms-correlation-request-id'] unless http_response['x-ms-correlation-request-id'].nil?
        result.client_request_id = http_response['x-ms-client-request-id'] unless http_response['x-ms-client-request-id'].nil?
        # Deserialize Response
        if status_code == 200
          begin
            parsed_response = response_content.to_s.empty? ? nil : JSON.load(response_content)
            result_mapper = Azure::Web::Mgmt::V2018_02_01::Models::StampCapacityCollection.mapper()
            result.body = @client.deserialize(result_mapper, parsed_response)
          rescue Exception => e
            fail MsRest::DeserializationError.new('Error occurred in deserializing the response', e.message, e.backtrace, result)
          end
        end

        result
      end

      promise.execute
    end","#
# Get the used, available, and total worker capacity an App Service
# Environment.
#
# Get the used, available, and total worker capacity an App Service
# Environment.
#
# @param next_page_link [String] The NextLink from the previous successful call
# to List operation.
# @param [Hash{String => String}] A hash of custom headers that will be added
# to the HTTP request.
#
# @return [Concurrent::Promise] Promise object which holds the HTTP response.
#",,
"def add_authority(authority=nil)
      if authority.class == String && !block_given?
        @j_del.java_method(:addAuthority, [Java::java.lang.String.java_class]).call(authority)
        return self
      end
      raise ArgumentError, ""Invalid arguments when calling add_authority(authority)""
    end","#  Add a required authority for this auth handler
# @param [String] authority the authority
# @return [self]",,
"def write_sheet_pr #:nodoc:
      return unless tab_outline_fit? || vba_codename? || filter_on?

      attributes = []
      attributes << ['codeName',   @vba_codename] if vba_codename?
      attributes << ['filterMode', 1]             if filter_on?

      if tab_outline_fit?
        @writer.tag_elements('sheetPr', attributes) do
          write_tab_color
          write_outline_pr
          write_page_set_up_pr
        end
      else
        @writer.empty_tag('sheetPr', attributes)
      end
    end","#
# Write the <sheetPr> element for Sheet level properties.
#",,
"def to_rack_response(disposition: ""inline"", range: false)
          range = parse_http_range(range) if range

          status  = range ? 206 : 200
          headers = rack_headers(disposition: disposition, range: range)
          body    = rack_body(range: range)

          [status, headers, body]
        end",# Returns a Rack response triple for the uploaded file.,,
"def generate_answer_async(kb_id, generate_answer_payload, custom_headers:nil)
      fail ArgumentError, '@client.runtime_endpoint is nil' if @client.runtime_endpoint.nil?
      fail ArgumentError, 'kb_id is nil' if kb_id.nil?
      fail ArgumentError, 'generate_answer_payload is nil' if generate_answer_payload.nil?


      request_headers = {}
      request_headers['Content-Type'] = 'application/json; charset=utf-8'

      # Set Headers
      request_headers['x-ms-client-request-id'] = SecureRandom.uuid
      request_headers['accept-language'] = @client.accept_language unless @client.accept_language.nil?

      # Serialize Request
      request_mapper = Azure::CognitiveServices::QnamakerRuntime::V4_0::Models::QueryDTO.mapper()
      request_content = @client.serialize(request_mapper,  generate_answer_payload)
      request_content = request_content != nil ? JSON.generate(request_content, quirks_mode: true) : nil

      path_template = 'knowledgebases/{kbId}/generateAnswer'

      request_url = @base_url || @client.base_url
    request_url = request_url.gsub('{RuntimeEndpoint}', @client.runtime_endpoint)

      options = {
          middlewares: [[MsRest::RetryPolicyMiddleware, times: 3, retry: 0.02], [:cookie_jar]],
          path_params: {'kbId' => kb_id},
          body: request_content,
          headers: request_headers.merge(custom_headers || {}),
          base_url: request_url
      }
      promise = @client.make_request_async(:post, path_template, options)

      promise = promise.then do |result|
        http_response = result.response
        status_code = http_response.status
        response_content = http_response.body
        unless status_code == 200
          error_model = JSON.load(response_content)
          fail MsRest::HttpOperationError.new(result.request, http_response, error_model)
        end

        result.request_id = http_response['x-ms-request-id'] unless http_response['x-ms-request-id'].nil?
        result.correlation_request_id = http_response['x-ms-correlation-request-id'] unless http_response['x-ms-correlation-request-id'].nil?
        result.client_request_id = http_response['x-ms-client-request-id'] unless http_response['x-ms-client-request-id'].nil?
        # Deserialize Response
        if status_code == 200
          begin
            parsed_response = response_content.to_s.empty? ? nil : JSON.load(response_content)
            result_mapper = Azure::CognitiveServices::QnamakerRuntime::V4_0::Models::QnASearchResultList.mapper()
            result.body = @client.deserialize(result_mapper, parsed_response)
          rescue Exception => e
            fail MsRest::DeserializationError.new('Error occurred in deserializing the response', e.message, e.backtrace, result)
          end
        end

        result
      end

      promise.execute
    end","#
# GenerateAnswer call to query the knowledgebase.
#
# @param kb_id [String] Knowledgebase id.
# @param generate_answer_payload [QueryDTO] Post body of the request.
# @param [Hash{String => String}] A hash of custom headers that will be added
# to the HTTP request.
#
# @return [Concurrent::Promise] Promise object which holds the HTTP response.
#",,
"def put_into_frame
      @image = frame.composite(image, ""png"") do |c|
        c.compose ""Over""
        c.geometry offset['offset']
      end
    end",# puts the screenshot into the frame,,
"def list_async(resource_group_name, account_name, pool_name, volume_name, custom_headers:nil)
      fail ArgumentError, '@client.subscription_id is nil' if @client.subscription_id.nil?
      fail ArgumentError, 'resource_group_name is nil' if resource_group_name.nil?
      fail ArgumentError, ""'resource_group_name' should satisfy the constraint - 'MaxLength': '90'"" if !resource_group_name.nil? && resource_group_name.length > 90
      fail ArgumentError, ""'resource_group_name' should satisfy the constraint - 'MinLength': '1'"" if !resource_group_name.nil? && resource_group_name.length < 1
      fail ArgumentError, ""'resource_group_name' should satisfy the constraint - 'Pattern': '^[-\w\._\(\)]+$'"" if !resource_group_name.nil? && resource_group_name.match(Regexp.new('^^[-\w\._\(\)]+$$')).nil?
      fail ArgumentError, 'account_name is nil' if account_name.nil?
      fail ArgumentError, 'pool_name is nil' if pool_name.nil?
      fail ArgumentError, 'volume_name is nil' if volume_name.nil?
      fail ArgumentError, '@client.api_version is nil' if @client.api_version.nil?


      request_headers = {}
      request_headers['Content-Type'] = 'application/json; charset=utf-8'

      # Set Headers
      request_headers['x-ms-client-request-id'] = SecureRandom.uuid
      request_headers['accept-language'] = @client.accept_language unless @client.accept_language.nil?
      path_template = 'subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.NetApp/netAppAccounts/{accountName}/capacityPools/{poolName}/volumes/{volumeName}/snapshots'

      request_url = @base_url || @client.base_url

      options = {
          middlewares: [[MsRest::RetryPolicyMiddleware, times: 3, retry: 0.02], [:cookie_jar]],
          path_params: {'subscriptionId' => @client.subscription_id,'resourceGroupName' => resource_group_name,'accountName' => account_name,'poolName' => pool_name,'volumeName' => volume_name},
          query_params: {'api-version' => @client.api_version},
          headers: request_headers.merge(custom_headers || {}),
          base_url: request_url
      }
      promise = @client.make_request_async(:get, path_template, options)

      promise = promise.then do |result|
        http_response = result.response
        status_code = http_response.status
        response_content = http_response.body
        unless status_code == 200
          error_model = JSON.load(response_content)
          fail MsRestAzure::AzureOperationError.new(result.request, http_response, error_model)
        end

        result.request_id = http_response['x-ms-request-id'] unless http_response['x-ms-request-id'].nil?
        result.correlation_request_id = http_response['x-ms-correlation-request-id'] unless http_response['x-ms-correlation-request-id'].nil?
        result.client_request_id = http_response['x-ms-client-request-id'] unless http_response['x-ms-client-request-id'].nil?
        # Deserialize Response
        if status_code == 200
          begin
            parsed_response = response_content.to_s.empty? ? nil : JSON.load(response_content)
            result_mapper = Azure::NetApp::Mgmt::V2019_10_01::Models::SnapshotsList.mapper()
            result.body = @client.deserialize(result_mapper, parsed_response)
          rescue Exception => e
            fail MsRest::DeserializationError.new('Error occurred in deserializing the response', e.message, e.backtrace, result)
          end
        end

        result
      end

      promise.execute
    end","#
# Describe all snapshots
#
# List all snapshots associated with the volume
#
# @param resource_group_name [String] The name of the resource group.
# @param account_name [String] The name of the NetApp account
# @param pool_name [String] The name of the capacity pool
# @param volume_name [String] The name of the volume
# @param [Hash{String => String}] A hash of custom headers that will be added
# to the HTTP request.
#
# @return [Concurrent::Promise] Promise object which holds the HTTP response.
#",,
"def destroy
    run_callbacks :destroy do
      self.deleted_at = DateTime.now
    end
  end","# override destroy to get soft delete like acts_as_paranoid style delete
# Note: delete_all (used in helper) bypasses this and deletes all rows.",,
"def typecast_enumerator(enumerator)
        if enumerator.is_a?(Array)
          enumerator.flatten!
          enumerator.map! {|value| typecast_enumerator(value)}
          enumerator
        else
          enumerator.is_a?(Symbol) ? enumerator.to_s : enumerator
        end
      end","# Typecasts the given enumerator to its actual value stored in the
# database.  This will only convert symbols to strings.  All other values
# will remain in the same type.",,
"def begin_create_or_update_async(resource_group_name, application_name, parameters, custom_headers:nil)
      fail ArgumentError, 'resource_group_name is nil' if resource_group_name.nil?
      fail ArgumentError, ""'resource_group_name' should satisfy the constraint - 'MaxLength': '90'"" if !resource_group_name.nil? && resource_group_name.length > 90
      fail ArgumentError, ""'resource_group_name' should satisfy the constraint - 'MinLength': '1'"" if !resource_group_name.nil? && resource_group_name.length < 1
      fail ArgumentError, ""'resource_group_name' should satisfy the constraint - 'Pattern': '^[-\w\._\(\)]+$'"" if !resource_group_name.nil? && resource_group_name.match(Regexp.new('^^[-\w\._\(\)]+$$')).nil?
      fail ArgumentError, 'application_name is nil' if application_name.nil?
      fail ArgumentError, ""'application_name' should satisfy the constraint - 'MaxLength': '64'"" if !application_name.nil? && application_name.length > 64
      fail ArgumentError, ""'application_name' should satisfy the constraint - 'MinLength': '3'"" if !application_name.nil? && application_name.length < 3
      fail ArgumentError, 'parameters is nil' if parameters.nil?
      fail ArgumentError, '@client.api_version is nil' if @client.api_version.nil?
      fail ArgumentError, '@client.subscription_id is nil' if @client.subscription_id.nil?


      request_headers = {}
      request_headers['Content-Type'] = 'application/json; charset=utf-8'

      # Set Headers
      request_headers['x-ms-client-request-id'] = SecureRandom.uuid
      request_headers['accept-language'] = @client.accept_language unless @client.accept_language.nil?

      # Serialize Request
      request_mapper = Azure::Resources::Mgmt::V2018_06_01::Models::Application.mapper()
      request_content = @client.serialize(request_mapper,  parameters)
      request_content = request_content != nil ? JSON.generate(request_content, quirks_mode: true) : nil

      path_template = 'subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Solutions/applications/{applicationName}'

      request_url = @base_url || @client.base_url

      options = {
          middlewares: [[MsRest::RetryPolicyMiddleware, times: 3, retry: 0.02], [:cookie_jar]],
          path_params: {'resourceGroupName' => resource_group_name,'applicationName' => application_name,'subscriptionId' => @client.subscription_id},
          query_params: {'api-version' => @client.api_version},
          body: request_content,
          headers: request_headers.merge(custom_headers || {}),
          base_url: request_url
      }
      promise = @client.make_request_async(:put, path_template, options)

      promise = promise.then do |result|
        http_response = result.response
        status_code = http_response.status
        response_content = http_response.body
        unless status_code == 200 || status_code == 201
          error_model = JSON.load(response_content)
          fail MsRest::HttpOperationError.new(result.request, http_response, error_model)
        end

        result.request_id = http_response['x-ms-request-id'] unless http_response['x-ms-request-id'].nil?
        result.correlation_request_id = http_response['x-ms-correlation-request-id'] unless http_response['x-ms-correlation-request-id'].nil?
        result.client_request_id = http_response['x-ms-client-request-id'] unless http_response['x-ms-client-request-id'].nil?
        # Deserialize Response
        if status_code == 200
          begin
            parsed_response = response_content.to_s.empty? ? nil : JSON.load(response_content)
            result_mapper = Azure::Resources::Mgmt::V2018_06_01::Models::Application.mapper()
            result.body = @client.deserialize(result_mapper, parsed_response)
          rescue Exception => e
            fail MsRest::DeserializationError.new('Error occurred in deserializing the response', e.message, e.backtrace, result)
          end
        end
        # Deserialize Response
        if status_code == 201
          begin
            parsed_response = response_content.to_s.empty? ? nil : JSON.load(response_content)
            result_mapper = Azure::Resources::Mgmt::V2018_06_01::Models::Application.mapper()
            result.body = @client.deserialize(result_mapper, parsed_response)
          rescue Exception => e
            fail MsRest::DeserializationError.new('Error occurred in deserializing the response', e.message, e.backtrace, result)
          end
        end

        result
      end

      promise.execute
    end","#
# Creates a new managed application.
#
# @param resource_group_name [String] The name of the resource group. The name
# is case insensitive.
# @param application_name [String] The name of the managed application.
# @param parameters [Application] Parameters supplied to the create or update a
# managed application.
# @param [Hash{String => String}] A hash of custom headers that will be added
# to the HTTP request.
#
# @return [Concurrent::Promise] Promise object which holds the HTTP response.
#",,
"def first_env
          case env = config[:env]
          when Array
            env.first
          when Hash
            env.fetch(:jobs, nil)&.first
          end
        rescue nil
        end",# should be able to drop a lot of this once everyone's on /configs,,
"def execute(sql, opts=OPTS)
        opts = Hash[opts]
        opts[:type] = :select
        opts[:stream] = @opts[:stream]
        super
      end",# Set the :type option to :select if it hasn't been set.,,
"def update_camera
    #warning! This function calls are ORDERED!
    #if you don't have any idea what you're editing, don't do it!
    reload_camera #TODO: Fix to make camera up right. very inefficient algorithm.
    
    vec = @camera_data.get_camera_x_axis_vector
    glRotatef(@camera_data.xspin, vec.x, 0.0, vec.z) #X rotate
    glRotatef(@camera_data.yspin, 0.0, 1.0, 0.0) #Y rotate
    
    # camera back to player pos and calculate speed 
    #TODO: Why do this guy calculate speed?
    glTranslatef(-(@camera_data.pos.x + @camera_data.speed.x), -(@camera_data.pos.y + @camera_data.speed.y), -(@camera_data.pos.z + @camera_data.speed.z)) 
  end",#this thing NEEDS to be module!,,
"def ship_numbers_match?
      single_ship_count == SINGLE_SHIP[:count] &&
        cruiser_count   == CRUISER[:count]     &&
        destroyer_count == DESTROYER[:count]   &&
        submarine_count == SUBMARINE[:count]
    end",# verify that ship type and number matches with initial requirements,,
"def find(plan_id, options = {})
      self.prefill(options)

      request = Request.new(@client)
      path    = ""/plans/"" + CGI.escape(plan_id) + """"
      data    = {

      }

      response = Response.new(request.get(path, data, options))
      return_values = Array.new
      
      body = response.body
      body = body[""plan""]
      
      
      obj = Plan.new(@client)
      return_values.push(obj.fill_with_data(body))
      

      
      return_values[0]
    end","# Find a plan by its ID.
# Params:
# +plan_id+:: ID of the plan
# +options+:: +Hash+ of options",,
"def binary_filename_for(name)
      user_file = File.join(binary_directory, ""#{basename}-#{name}"") 
      return user_file if File.exists?(user_file)
      built_in = File.join(built_in_commands_directory, ""#{name}.rb"") 
      return built_in if File.exists?(built_in)
      user_file
    end","# load first from users binary directory. then load built-in commands if
# available",,
"pub fn expect_contains<P>(&self, path: P, pattern: &str)
        where P: AsRef<Path>
    {
        let path = self.dir.join(path);
        let contents = self.read_file(&path);
        assert!(contents.contains(pattern),
                format!(""expected {} to match {:?}, but it contained {:?}"",
                        path.display(), pattern, contents));
    }","/// Verify that the contents of the file match the specified pattern.
/// Someday this should support `std::str::pattern::Pattern` so that we
/// can support both strings and regular expressions, but that hasn't
/// been stabilized yet.",,
"pub async fn schema(&mut self, context: &mut Context) -> Result<(), LatteError> {
        let context = ContextRefMut::new(context);
        self.async_call(FnRef::new(SCHEMA_FN), (context,)).await?;
        Ok(())
    }","/// Calls the script's `schema` function.
/// Typically used to create database schema.",,
"pub(super) fn compress_cond_expr_if_simillar(&mut self, e: &mut Expr) {
        if !self.options.conditionals {
            return;
        }

        let cond = match e {
            Expr::Cond(expr) => expr,
            _ => return,
        };

        let compressed =
            self.compress_similar_cons_alt(&mut cond.test, &mut cond.cons, &mut cond.alt, false);

        match compressed {
            Some(v) => {
                *e = v;
                self.changed = true;
                return;
            }
            None => {}
        }

        // x ? x : y => x || y
        if cond.test.is_ident() && cond.test.eq_ignore_span(&cond.cons) {
            log::trace!(""Compressing `x ? x : y` as `x || y`"");
            self.changed = true;
            *e = Expr::Bin(BinExpr {
                span: cond.span,
                op: op!(""||""),
                left: cond.test.take(),
                right: cond.alt.take(),
            });
            return;
        }
    }",/// Compress a conditional expression if cons and alt is simillar,,
"pub fn make_workspace_edit(location: Location, new_text: String) -> WorkspaceEdit {
    let changes = vec![(
        location.uri,
        vec![TextEdit {
            range: location.range,
            new_text,
        }],
    )].into_iter()
    .collect();

    WorkspaceEdit {
        changes: Some(changes),
        document_changes: None,
    }
}",/// Create an edit for the given location and text.,,
"fn infix_binding_power(op: &Token) -> Option<(u8, u8)> {
    let res = match op {
        Token::Add | Token::Minus => (1, 2),
        Token::Mul | Token::Div => (3, 4),
        Token::PlusMinus => (7,8),
        _ => return None,
    };
    Some(res)
}","///Optionally returns the binding power of an infix operator.
///
///This is at the core of Pratt's method for parsing, because
///it totally orders the precedence of each operator.
///
///If the operator is not valid, returns None.",,
"pub fn do_players_ranking(f: &EndGameContext, ctx: &ScFuncContext) -> Vec<Better> {
    // 'valid_bets' stores all the bets placed, including zero value ones (with the player,
    // the accuracy of the tag and, for the moment, a total bet equal to zero)
    let mut valid_bets: Vec<Better> = Vec::new();
    // fill the 'valid_bets' with the bets. The bet amount will be filled later
    for i in 0..f.state.valid_tags().length() as usize {
        let valid_tag = f.state.valid_tags().get_valid_tag(i as u32).value();

        let tgd_img = f
            .state
            .tgd_imgs()
            .get_tgd_img(valid_tag.tgd_img)
            .value();
        let tgd_img_coords = input_tgimg_to_vecs(&tgd_img, ctx);
        let tgd_img_point = &tgd_img_coords[valid_tag.play_tag_id as usize];
        let boost = input_str_to_vecu8(&tgd_img.boost, ctx)[valid_tag.play_tag_id as usize];

        let clusters_centers = f
            .state
            .processed_images()
            .get_tgd_img(tgd_img.image_id as u32)
            .value();
        let cluster_center_coords = input_tgimg_to_vecs(&clusters_centers, ctx);
        let mut distance_to_cluster_center =
            euclidean_distance(tgd_img_point.clone(), cluster_center_coords[0].clone());
        for j in 1..cluster_center_coords.len() {
            let cluster_center_point = &cluster_center_coords[j];
            let distance =
                euclidean_distance(tgd_img_point.to_vec(), cluster_center_point.to_vec());
            if distance < distance_to_cluster_center {
                distance_to_cluster_center = distance;
            }
        }
        valid_bets.push(Better::new(
            distance_to_cluster_center,
            valid_tag.clone().player,
            0,
            boost,
        ));
    }

    // Next, we make a list with all the betters that made a valid tag, leaving their best one
    // and calculating how much they betted in total
    let mut betters_top: Vec<Better> = Vec::new();
    'all: for i in 0..valid_bets.len() {
        let valid_bet = valid_bets[i].clone();

        for better in 0..betters_top.len() {
            if valid_bet.player == betters_top[better].player {
                // replace the accuracy for the player's best one
                if valid_bet.accuracy < betters_top[better].accuracy {
                    betters_top[better].boost = valid_bet.boost;
                    betters_top[better].accuracy = valid_bet.accuracy;
                }

                // skip to next iteration of the outer loop to avoid adding the player to the 'betters_top' again
                continue 'all;
            }
        }
        let new_better = Better {
            accuracy: valid_bet.accuracy,
            amount: valid_bet.amount,
            boost: valid_bet.boost,
            player: valid_bet.player,
        };

        betters_top.push(new_better);
    }

    // Here we calculate the amount of iotas betted by each player in the 'betters_top' list
    'bet: for i in 0..f.state.bets().length() {
        let bet = f.state.bets().get_bet(i).value();
        for better in 0..betters_top.len() {
            if betters_top[better].player == bet.player {
                betters_top[better].amount += bet.amount;
                continue 'bet;
            }
        }
    }

    // sort the 'top_betters' by accuracy
    betters_top.sort_by(|a, b| b.accuracy.partial_cmp(&a.accuracy).unwrap());

    return betters_top;
}","// An internal function to generate a ranking of players based on their best bet.
// It uses the accuracy to calculate how good a bet is and the ranking is so that
// the best players are higher up in the returning vector.",,
"fn append_model_part<W: Write>(&self,
                                   builder: &mut tar::Builder<W>,
                                   path: &str,
                                   counts: &HashMap<String, u64>)
                                   -> Result<()> {
        let mut csv = vec![];
        self.frequencies(counts, &mut csv)?;

        let mut header = tar::Header::new_old();
        header.set_path(path)?;
        // TODO: Can this fail with a cast error?
        header.set_size(csv.len() as u64);
        header.set_mode(0o600);
        header.set_cksum();
        builder.append(&header, io::Cursor::new(&csv))?;
        Ok(())
    }",/// Append a file to our model.,,
"pub fn laguerre<N: ComplexField + Copy + FromPrimitive>(
    n: u32,
    tol: N::RealField,
) -> Result<Polynomial<N>, String>
where
    <N as ComplexField>::RealField: FromPrimitive + Copy,
{
    let mut coefficients = Vec::with_capacity(n as usize + 1);
    for k in 0..=n {
        coefficients.push(
            choose::<N>(n, k) / factorial::<N>(k) * if k % 2 == 0 { N::one() } else { -N::one() },
        );
    }

    let mut poly: Polynomial<N> = coefficients.iter().copied().collect();
    poly.set_tolerance(tol)?;
    Ok(poly)
}","/// Get the nth (positive) laguerre polynomial.
///
/// Gets the nth (positive) laguerre polynomial over a specified field. This is
/// done using the direct formula and is properly normalized.
///
/// # Examples
/// ```
/// use bacon_sci::special::laguerre;
/// fn example() {
///     let p_3 = laguerre::<f64>(3, 1e-8).unwrap();
///     assert_eq!(p_3.order(), 3);
///     assert!((p_3.get_coefficient(0) - 1.0).abs() < 0.00001);
///     assert!((p_3.get_coefficient(1) + 3.0).abs() < 0.00001);
///     assert!((p_3.get_coefficient(2) - 9.0/6.0).abs() < 0.00001);
///     assert!((p_3.get_coefficient(3) + 1.0/6.0).abs() < 0.00001);
/// }
///",,
"fn sha256_digest_block_u32(state: &mut [u32; 8], block: &[u32; 16]) {
    let k = &K32X4;

    macro_rules! schedule {
        ($v0:expr, $v1:expr, $v2:expr, $v3:expr) => {
            sha256msg2(sha256msg1($v0, $v1) + sha256load($v2, $v3), $v3)
        };
    }

    macro_rules! rounds4 {
        ($abef:ident, $cdgh:ident, $rest:expr) => {{
            $cdgh = sha256_digest_round_x2($cdgh, $abef, $rest);
            $abef = sha256_digest_round_x2($abef, $cdgh, sha256swap($rest));
        }};
    }

    let mut abef = u32x4(state[0], state[1], state[4], state[5]);
    let mut cdgh = u32x4(state[2], state[3], state[6], state[7]);

    // Rounds 0..64
    let mut w0 = u32x4(block[3], block[2], block[1], block[0]);
    rounds4!(abef, cdgh, k[0] + w0);
    let mut w1 = u32x4(block[7], block[6], block[5], block[4]);
    rounds4!(abef, cdgh, k[1] + w1);
    let mut w2 = u32x4(block[11], block[10], block[9], block[8]);
    rounds4!(abef, cdgh, k[2] + w2);
    let mut w3 = u32x4(block[15], block[14], block[13], block[12]);
    rounds4!(abef, cdgh, k[3] + w3);
    let mut w4 = schedule!(w0, w1, w2, w3);
    rounds4!(abef, cdgh, k[4] + w4);
    w0 = schedule!(w1, w2, w3, w4);
    rounds4!(abef, cdgh, k[5] + w0);
    w1 = schedule!(w2, w3, w4, w0);
    rounds4!(abef, cdgh, k[6] + w1);
    w2 = schedule!(w3, w4, w0, w1);
    rounds4!(abef, cdgh, k[7] + w2);
    w3 = schedule!(w4, w0, w1, w2);
    rounds4!(abef, cdgh, k[8] + w3);
    w4 = schedule!(w0, w1, w2, w3);
    rounds4!(abef, cdgh, k[9] + w4);
    w0 = schedule!(w1, w2, w3, w4);
    rounds4!(abef, cdgh, k[10] + w0);
    w1 = schedule!(w2, w3, w4, w0);
    rounds4!(abef, cdgh, k[11] + w1);
    w2 = schedule!(w3, w4, w0, w1);
    rounds4!(abef, cdgh, k[12] + w2);
    w3 = schedule!(w4, w0, w1, w2);
    rounds4!(abef, cdgh, k[13] + w3);
    w4 = schedule!(w0, w1, w2, w3);
    rounds4!(abef, cdgh, k[14] + w4);
    w0 = schedule!(w1, w2, w3, w4);
    rounds4!(abef, cdgh, k[15] + w0);

    let u32x4(a, b, e, f) = abef;
    let u32x4(c, d, g, h) = cdgh;

    state[0] = state[0].wrapping_add(a);
    state[1] = state[1].wrapping_add(b);
    state[2] = state[2].wrapping_add(c);
    state[3] = state[3].wrapping_add(d);
    state[4] = state[4].wrapping_add(e);
    state[5] = state[5].wrapping_add(f);
    state[6] = state[6].wrapping_add(g);
    state[7] = state[7].wrapping_add(h);
}",/// Process a block with the SHA-256 algorithm.,,
"pub(crate) fn fill_slice<T: Clone>(slice: &mut [T], value: T) {
    for el in slice {
        *el = value.clone();
    }
}","// Polyfill until slice_fill feature stabilizes
// TODO: check if it optimizes to memset",,
"fn release_info(&self, version: &Option<String>) -> Result<ReleaseInfo> {
        let gitdir = self.path.join("".git"");
        let url = get_repo_url(&gitdir)?;
        let version = match version.as_ref() {
            Some(version) => version.clone(),
            None => get_repo_latest_version(&gitdir)?,
        };
        let sem_version = if !self.loose {
            extract_semantic(&version)?
        } else {
            &version
        };
        let project = get_project_name(&url)
            .ok_or(anyhow!(""Failed to extract project name from URL""))?;
        let mut path = PathBuf::from(&self.path);
        path.push(&self.changelog);
        let changelog = get_repo_changelog(&path, sem_version)?;
        let info = ReleaseInfo {
            project: project,
            url: url,
            version: version,
            changelog: changelog,
        };
        Ok(info)
    }",/// Explore to get latest release information,,
"pub fn broadcast(
        id: &Pubkey,
        contains_last_tick: bool,
        broadcast_table: &[NodeInfo],
        s: &UdpSocket,
        blobs: &[SharedBlob],
    ) -> Result<()> {
        if broadcast_table.is_empty() {
            debug!(""{}:not enough peers in cluster_info table"", id);
            inc_new_counter_info!(""cluster_info-broadcast-not_enough_peers_error"", 1);
            Err(ClusterInfoError::NoPeers)?;
        }

        let orders = Self::create_broadcast_orders(contains_last_tick, blobs, broadcast_table);

        trace!(""broadcast orders table {}"", orders.len());

        let errs = Self::send_orders(id, s, orders);

        for e in errs {
            if let Err(e) = &e {
                trace!(""{}: broadcast result {:?}"", id, e);
            }
            e?;
        }

        inc_new_counter_info!(""cluster_info-broadcast-max_idx"", blobs.len());

        Ok(())
    }","/// broadcast messages from the leader to layer 1 nodes
/// # Remarks",,
"pub fn is_finished(&self) -> bool {
        let finished = self.chunks_finished.load(atomic::Ordering::SeqCst);
        let launched = self.chunks_launched.load(atomic::Ordering::SeqCst);
        let start = self.start.load(atomic::Ordering::SeqCst);
        // This shouldn't happen, if it does some bad threading voodoo is afoot
        assert!(finished <= launched);
        finished == launched && start >= self.end
    }",/// Check if all tasks for this group have been completed,,
"pub fn with_none() -> Self {
        Self {
            classic: None,
            blocks: None,
            color: None,
            date: None,
            dereference: None,
            display: None,
            icons: None,
            ignore_globs: None,
            indicators: None,
            layout: None,
            recursion: None,
            size: None,
            sorting: None,
            no_symlink: None,
            total_size: None,
            symlink_arrow: None,
        }
    }",/// This constructs a Config struct with all None,,
"async fn await_service_discovery(&self, device_id: &DeviceId) -> Result<(), BluetoothError> {
        // We need to subscribe to events before checking current value to avoid a race condition.
        let mut events = self.device_event_stream(device_id).await?;
        if self
            .device(device_id, DBUS_METHOD_CALL_TIMEOUT)
            .services_resolved()
            .await?
        {
            log::info!(""Services already resolved."");
            return Ok(());
        }
        timeout(SERVICE_DISCOVERY_TIMEOUT, async {
            while let Some(event) = events.next().await {
                if matches!(event, BluetoothEvent::Device {
                    id,
                    event: DeviceEvent::ServicesResolved,
                } if device_id == &id)
                {
                    return Ok(());
                }
            }

            // Stream ended prematurely. This shouldn't happen, so something has gone wrong.
            Err(BluetoothError::ServiceDiscoveryTimedOut)
        })
        .await
        .unwrap_or(Err(BluetoothError::ServiceDiscoveryTimedOut))
    }",/// Wait until the services of the given device have been resolved.,,
"fn common_prefix(&self, other: &Self) -> usize {
        for byte_index in 0..XOR_NAME_LEN {
            if self[byte_index] != other[byte_index] {
                return (byte_index * 8)
                    + (self[byte_index] ^ other[byte_index]).leading_zeros() as usize;
            }
        }
        8 * XOR_NAME_LEN
    }","/// Returns the length of the common prefix with the `other` name; e. g.
/// the when `other = 11110000` and `self = 11111111` this is 4.",,
"pub fn poll<'a>(&'a mut self, cx: &mut Context) -> Poll<NetworkEvent<'a, TTrans, TInEvent, TOutEvent, THandler, TConnInfo, TPeerId>>
    where
        TTrans: Transport<Output = (TConnInfo, TMuxer)>,
        TTrans::Error: Send + 'static,
        TTrans::Dial: Send + 'static,
        TTrans::ListenerUpgrade: Send + 'static,
        TMuxer: Send + Sync + 'static,
        TMuxer::OutboundSubstream: Send,
        TInEvent: Send + 'static,
        TOutEvent: Send + 'static,
        THandler: IntoConnectionHandler<TConnInfo> + Send + 'static,
        THandler::Handler: ConnectionHandler<Substream = Substream<TMuxer>, InEvent = TInEvent, OutEvent = TOutEvent> + Send + 'static,
        <THandler::Handler as ConnectionHandler>::Error: error::Error + Send + 'static,
        TConnInfo: Clone,
        TPeerId: Send + 'static,
    {
        // Poll the listener(s) for new connections.
        match ListenersStream::poll(Pin::new(&mut self.listeners), cx) {
            Poll::Pending => (),
            Poll::Ready(ListenersEvent::Incoming {
                listener_id,
                upgrade,
                local_addr,
                send_back_addr
            }) => {
                return Poll::Ready(NetworkEvent::IncomingConnection(
                    IncomingConnectionEvent {
                        listener_id,
                        upgrade,
                        local_addr,
                        send_back_addr,
                        pool: &mut self.pool,
                    }))
            }
            Poll::Ready(ListenersEvent::NewAddress { listener_id, listen_addr }) => {
                return Poll::Ready(NetworkEvent::NewListenerAddress { listener_id, listen_addr })
            }
            Poll::Ready(ListenersEvent::AddressExpired { listener_id, listen_addr }) => {
                return Poll::Ready(NetworkEvent::ExpiredListenerAddress { listener_id, listen_addr })
            }
            Poll::Ready(ListenersEvent::Closed { listener_id, addresses, reason }) => {
                return Poll::Ready(NetworkEvent::ListenerClosed { listener_id, addresses, reason })
            }
            Poll::Ready(ListenersEvent::Error { listener_id, error }) => {
                return Poll::Ready(NetworkEvent::ListenerError { listener_id, error })
            }
        }

        // Poll the known peers.
        let event = match self.pool.poll(cx) {
            Poll::Pending => return Poll::Pending,
            Poll::Ready(PoolEvent::ConnectionEstablished { connection, num_established }) => {
                match self.dialing.entry(connection.peer_id().clone()) {
                    hash_map::Entry::Occupied(mut e) => {
                        e.get_mut().retain(|s| s.current.0 != connection.id());
                        if e.get().is_empty() {
                            e.remove();
                        }
                    },
                    _ => {}
                }

                NetworkEvent::ConnectionEstablished {
                    connection,
                    num_established,
                }
            }
            Poll::Ready(PoolEvent::PendingConnectionError { id, endpoint, error, handler, pool, .. }) => {
                let dialing = &mut self.dialing;
                let (next, event) = on_connection_failed(dialing, id, endpoint, error, handler);
                if let Some(dial) = next {
                    let transport = self.listeners.transport().clone();
                    if let Err(e) = dial_peer_impl(transport, pool, dialing, dial) {
                        log::warn!(""Dialing aborted: {:?}"", e);
                    }
                }
                event
            }
            Poll::Ready(PoolEvent::ConnectionError { id, connected, error, num_established, .. }) => {
                NetworkEvent::ConnectionError {
                    id,
                    connected,
                    error,
                    num_established,
                }
            }
            Poll::Ready(PoolEvent::ConnectionEvent { connection, event }) => {
                NetworkEvent::ConnectionEvent {
                    connection,
                    event,
                }
            }
            Poll::Ready(PoolEvent::AddressChange { connection, new_endpoint, old_endpoint }) => {
                NetworkEvent::AddressChange {
                    connection,
                    new_endpoint,
                    old_endpoint,
                }
            }
        };

        Poll::Ready(event)
    }","/// Provides an API similar to `Stream`, except that it cannot error.",,
"pub fn color_edit_button_srgba_premultiplied(&mut self, srgba: &mut [u8; 4]) -> Response {
        let mut color = Color32::from_rgba_premultiplied(srgba[0], srgba[1], srgba[2], srgba[3]);
        let response = self.color_edit_button_srgba(&mut color);
        *srgba = color.to_array();
        response
    }","/// Shows a button with the given color.
/// If the user clicks the button, a full color picker is shown.
/// The given color is in `sRGBA` space with premultiplied alpha",,
"pub fn with_storage<S: StorageAdapter + Sync + Send + 'static>(
        mut self,
        storage_path: impl AsRef<Path>,
        adapter: S,
    ) -> Self {
        crate::storage::set_adapter(&storage_path, adapter);
        self.storage_path = storage_path.as_ref().to_path_buf();
        self.default_storage = false;
        self
    }",/// Sets a custom storage adapter to be used.,,
"pub fn hexcode_to_rgb(s: &str) -> Rgb {
    Rgb {
        r: convert_hex(&s[0..2]),
        g: convert_hex(&s[2..4]),
        b: convert_hex(&s[4..6]),
    }
}",/// hexcode-to-rgb converter. panics on malformed RGB hex codes,,
"int
iperf_time_compare(struct iperf_time *time1, struct iperf_time *time2)
{
    if (time1->secs < time2->secs)
        return -1;
    if (time1->secs > time2->secs)
        return 1;
    if (time1->usecs < time2->usecs)
        return -1;
    if (time1->usecs > time2->usecs)
        return 1;
    return 0;
}","/* iperf_time_compare
 *
 * Compare two timestamps
 *
 * Returns -1 if time1 is earlier, 1 if time1 is later,
 * or 0 if the timestamps are equal.
 */","/* Accepts two iperf_time structs that represent two timestamps. 
Compares the two timestaps and returns -1 if time1 is earlier, 
1 if time1 is later, and 0 if they are equal.*/",
"static int npu_table_search(struct npu2 *p, uint64_t table_addr, int stride,
    int table_size, uint64_t *value, uint64_t mask)
{
int i;
uint64_t val;

assert(value);

for (i = 0; i < table_size; i++) {
val = npu2_read(p, table_addr + i*stride);
if ((val & mask) == *value) {
*value = val;
return i;
}
}

return -1;
}","/*
 * Search a table for an entry with matching value under mask. Returns
 * the index and the current value in *value.
 */","/*Accepts a sctruct, 2 signed ints, 2 unsigned ints. 
Search a table for an entry with matching value under mask. 
Returns the index and the current value in *value. */",
"static int cpr_reduce_ceiling_voltage(struct cpr_regulator *cpr_vreg,
struct device *dev)
{
bool reduce_to_fuse_open_loop, reduce_to_interpolated_open_loop;
int i;

reduce_to_fuse_open_loop = of_property_read_bool(dev->of_node,
""qcom,cpr-init-voltage-as-ceiling"");
reduce_to_interpolated_open_loop = of_property_read_bool(dev->of_node,
""qcom,cpr-scaled-init-voltage-as-ceiling"");

if (!reduce_to_fuse_open_loop && !reduce_to_interpolated_open_loop)
return 0;

for (i = CPR_CORNER_MIN; i <= cpr_vreg->num_corners; i++) {
if (reduce_to_interpolated_open_loop &&
    cpr_vreg->open_loop_volt[i] < cpr_vreg->ceiling_volt[i])
cpr_vreg->ceiling_volt[i] = cpr_vreg->open_loop_volt[i];
else if (reduce_to_fuse_open_loop &&
cpr_vreg->pvs_corner_v[cpr_vreg->corner_map[i]]
< cpr_vreg->ceiling_volt[i])
cpr_vreg->ceiling_volt[i]
= max((u32)cpr_vreg->floor_volt[i],
       cpr_vreg->pvs_corner_v[cpr_vreg->corner_map[i]]);
cpr_debug(cpr_vreg, ""lowered ceiling[%d] = %d uV\n"",
i, cpr_vreg->ceiling_volt[i]);
}

return 0;
}","/*
 * Conditionally reduce the per-virtual-corner ceiling voltages if certain
 * device tree flags are present.  This must be called only after the ceiling
 * array has been initialized and the open_loop_volt array values have been
 * initialized and limited to the existing floor to ceiling voltage range.
 */","/*Accepts two structs. Checks If ceiling array and the open_loop_volt array values have been initialized, 
check if certain device tree flags are present. If so conditionally reduce the 
Per-virtual-corner ceiling voltages in the range of the existing floor to ceiling voltage.*/",
"int iommu_dma_map_sg(struct device *dev, struct scatterlist *sg,
int nents, int prot)
{
struct iommu_domain *domain = iommu_get_domain_for_dev(dev);
struct iommu_dma_cookie *cookie = domain->iova_cookie;
struct iova_domain *iovad = &cookie->iovad;
struct scatterlist *s, *prev = NULL;
dma_addr_t iova;
size_t iova_len = 0;
unsigned long mask = dma_get_seg_boundary(dev);
int i;

/*
 * Work out how much IOVA space we need, and align the segments to
 * IOVA granules for the IOMMU driver to handle. With some clever
 * trickery we can modify the list in-place, but reversibly, by
 * stashing the unaligned parts in the as-yet-unused DMA fields.
 */
for_each_sg(sg, s, nents, i) {
size_t s_iova_off = iova_offset(iovad, s->offset);
size_t s_length = s->length;
size_t pad_len = (mask - iova_len + 1) & mask;

sg_dma_address(s) = s_iova_off;
sg_dma_len(s) = s_length;
s->offset -= s_iova_off;
s_length = iova_align(iovad, s_length + s_iova_off);
s->length = s_length;

/*
 * Due to the alignment of our single IOVA allocation, we can
 * depend on these assumptions about the segment boundary mask:
 * - If mask size >= IOVA size, then the IOVA range cannot
 *   possibly fall across a boundary, so we don't care.
 * - If mask size < IOVA size, then the IOVA range must start
 *   exactly on a boundary, therefore we can lay things out
 *   based purely on segment lengths without needing to know
 *   the actual addresses beforehand.
 * - The mask must be a power of 2, so pad_len == 0 if
 *   iova_len == 0, thus we cannot dereference prev the first
 *   time through here (i.e. before it has a meaningful value).
 */
if (pad_len && pad_len < s_length - 1) {
prev->length += pad_len;
iova_len += pad_len;
}

iova_len += s_length;
prev = s;
}

iova = iommu_dma_alloc_iova(domain, iova_len, dma_get_mask(dev), dev);
if (!iova)
goto out_restore_sg;

/*
 * We'll leave any physical concatenation to the IOMMU driver's
 * implementation - it knows better than we do.
 */
if (iommu_map_sg(domain, iova, sg, nents, prot) < iova_len)
goto out_free_iova;

return __finalise_sg(dev, sg, nents, iova);

out_free_iova:
iommu_dma_free_iova(domain, cookie, iova, iova_len);
out_restore_sg:
__invalidate_sg(sg, nents);
return 0;
}","/*
 * The DMA API client is passing in a scatterlist which could describe
 * any old buffer layout, but the IOMMU API requires everything to be
 * aligned to IOMMU pages. Hence the need for this complicated bit of
 * impedance-matching, to be able to hand off a suitably-aligned list,
 * but still preserve the original offsets and sizes for the caller.
 */","/*Accepts 2 structs and 2 integers. Through impedance-matching, 
ensures that the passed in scatterlist is aligned to IOMMU pages 
while still preserving the original offsets and sizes for the caller*/",
"void
init_instance_handle_signals (struct context *c, const struct env_set *env, const unsigned int flags)
{
  pre_init_signal_catch ();
  init_instance (c, env, flags);
  post_init_signal_catch ();

  /*
   * This is done so that signals thrown during
   * initialization can bring us back to
   * a management hold.
   */
  if (IS_SIG (c))
    {
      remap_signal (c);
      uninit_management_callback ();  
    }
}","/*
 * Initialize a tunnel instance, handle pre and post-init
 * signal settings.
 */","/*Accepts 2 structs, and an unsigned int. 
Initialize a tunnel instance, handle pre and post-init signal settings.*/",
"void carrega_matriz_serial()
{
    for (int i=0; i<tamanho; i++) {
        for (int j=0; j<tamanho; j++) {
            matrizA[posicao(i, j, tamanho)] = rand() % 10 + 1;
            matrizB[posicao(i, j, tamanho)] = rand() % 10 + 1;
            matrizC[posicao(i, j, tamanho)] = rand() % 10 + 1;
        }
    }
}",// aqui realizamos a leitura das matrizes de forma Serial,/*Read the matrices in a serial manner using nested for loops*/,
"eModuleError_t eGattCommsSend( eCommsChannel_t eChannel, xUnifiedCommsMessage_t *pxMessage )
{
eModuleError_t   eError;
xGattLocalCharacteristic_t xLocalChar;
bool   bAcked = ( eChannel == (eCommsChannel_t) COMMS_CHANNEL_GATT_ACKED );
uint8_t *pucEncryptionKey;
uint8_t pucInitVector[AES128_IV_LENGTH];

/* Only a chance to send if the connection is up */
if ( pxCurrentConnection == NULL ) {
/* Remote address is not available */
return ERROR_INVALID_ADDRESS;
}
xAddress_t xRemote = xAddressUnpack( pxCurrentConnection->xRemoteAddress.pucAddress );
if ( pxMessage->xDestination != xRemote ) {
/* We are not connected to desired destination */
return ERROR_INVALID_ADDRESS;
}
if (pxMessage->usPayloadLen > (BLUETOOTH_GATT_MAX_MTU - sizeof(xGattEncryptedHeader_t))) {
/* Packet too large for GATT */
return ERROR_INVALID_DATA;
}

xSemaphoreTake( xGattBuffer, portMAX_DELAY );

if ( pxMessage->xPayloadType & DESCRIPTOR_ENCRYPTED_MASK ) {
/* Payload is already encrypted, transmit it as is */
xGattUnencryptedHeader_t *pxHeader = (xGattUnencryptedHeader_t *)pucCharacteristicBuffer;
uint8_t *pucData = pucCharacteristicBuffer + sizeof(xGattEncryptedHeader_t);

pxHeader->xPayloadType = pxMessage->xPayloadType;
pvMemcpy( pucData, pxMessage->pucPayload, pxMessage->usPayloadLen );
}
else if ( bUnifiedCommsEncryptionKey( &xGattComms, pxMessage->xPayloadType, pxMessage->xDestination, &pucEncryptionKey ) ) {
/* Payload is not currently encrypted, but an encryption key was provided, therefore encrypt the data */
xGattEncryptedHeader_t *pxHeader = (xGattEncryptedHeader_t *)pucCharacteristicBuffer;
uint8_t *pucData = pucCharacteristicBuffer + sizeof(xGattEncryptedHeader_t);
/* Populate header and payload */ 
pxHeader->xPayloadType = pxMessage->xPayloadType;
pxHeader->ucPayloadLength = pxMessage->usPayloadLen;
eRandomGenerate(pxHeader->pucInitVector, AES128_IV_LENGTH);
pvMemcpy( pucData, pxMessage->pucPayload, pxMessage->usPayloadLen );
/* Pad data with 0's to a multiple of the block size */
uint8_t ucEncryptLength = ROUND_UP(pxMessage->usPayloadLen, AES128_BLOCK_LENGTH);
pvMemset(pucData + pxMessage->usPayloadLen, 0x00, ucEncryptLength - pxMessage->usPayloadLen);
/* Copy IV into scratch space */
pvMemcpy(pucInitVector, pxHeader->pucInitVector, AES128_IV_LENGTH );
/* Apply encryption in place */
vAes128Crypt( ENCRYPT, pucEncryptionKey, pucInitVector, pucData, ucEncryptLength / AES128_IV_LENGTH, pucData );
} else {
/* Transmit unencrypted */
xGattUnencryptedHeader_t *pxHeader = (xGattUnencryptedHeader_t *)pucCharacteristicBuffer;
uint8_t *pucData = pucCharacteristicBuffer + sizeof(xGattEncryptedHeader_t);

pxHeader->xPayloadType = pxMessage->xPayloadType;
pvMemcpy( pucData, pxMessage->pucPayload, pxMessage->usPayloadLen );
}

/* If we know the other side has a data_in characteristic */
if ( pxRemoteDataIn != NULL ) {
pxRemoteDataIn->pucData   = pucCharacteristicBuffer;
pxRemoteDataIn->usDataLen = pxMessage->usPayloadLen + 1;
eError  = eBluetoothWriteRemoteCharacteristic( pxCurrentConnection, pxRemoteDataIn, bAcked );
}
/* Otherwise, make the data available on our own characteristic */
else {
xLocalChar.pucData  = pucCharacteristicBuffer;
xLocalChar.usDataLen  = pxMessage->usPayloadLen + 1;
xLocalChar.usCharacteristicHandle = UINT8_MAX;
if ( bAcked && bAckedSubscribed ) {
xLocalChar.usCharacteristicHandle = gattdb_csiro_out_acked;
xLocalChar.usCCCDValue  = BLE_CLIENT_CHARACTERISTIC_CONFIGURATION_INDICATION;
}
if ( !bAcked && bNackedSubscribed ) {
xLocalChar.usCharacteristicHandle = gattdb_csiro_out_nacked;
xLocalChar.usCCCDValue  = BLE_CLIENT_CHARACTERISTIC_CONFIGURATION_NOTIFICATION;
}
if ( xLocalChar.usCharacteristicHandle == UINT8_MAX ) {
/* Remote device has not subscribed to the characteristic on which the data will be served */
xSemaphoreGive( xGattBuffer );
return ERROR_INVALID_STATE;
}
eError = eBluetoothDistributeLocalCharacteristic( pxCurrentConnection, &xLocalChar );
}
xSemaphoreGive( xGattBuffer );
return eError;
}","/**
 * @brief Transfering data to a remote device
 *
 * The default behaviour is if the remote device has a data_in characteristic we have found, send it there.
 * This will be the typical mode for bases that have connected to deployed devices 
 *
 * Otherwise we check if the appropriate local channel (ACKED / NACKED) has been subscribed to as dictated by eChannel
 * If yes, we make the new data available on that characteristic, otherwise we drop the packet as there is nothing to send it to.
 *
 * A local buffer is used so we can append the payload to our header rather than some unholy ritual of sending data in two parts.
 */","/*Accepts a eCommsChannel_t  and a xUnifiedCommsMessage_t.  
The default behavior, typically when bases that have connected to deployed devices, is if the remote device has a data_in characteristic we have found, send it there. 
Otherwise check if eChannel dictates that the correct local channel has been subscribed to (ACKED/ NACKED). If yes make the new data available on that characteristic, 
otherwise drop the packet as there is nothing to send to. Due to local buffer being uses the payload will be appended to the header.*/",
"static art_leaf* maximum(art_node *n) {
    // Handle base cases
    if (!n) return NULL;
    if (IS_LEAF(n)) return LEAF_RAW(n);

    int idx;
    switch (n->type) {
        case NODE4:
            return maximum(((art_node4*)n)->children[n->num_children-1]);
        case NODE16:
            return maximum(((art_node16*)n)->children[n->num_children-1]);
        case NODE48:
            idx=255;
            while (!((art_node48*)n)->keys[idx]) idx--;
            idx = ((art_node48*)n)->keys[idx] - 1;
            return maximum(((art_node48*)n)->children[idx]);
        case NODE256:
            idx=255;
            while (!((art_node256*)n)->children[idx]) idx--;
            return maximum(((art_node256*)n)->children[idx]);
        default:
            abort();
    }
}",// Find the maximum leaf under a node,"/*Accepts a node.
Find the maximum leaf under a node.
Returns the maximum leaf if found or returns Null*/",
"void cache_mngt_core_remove_from_cache(ocf_core_t core)
{
ocf_cache_t cache = ocf_core_get_cache(core);
ocf_core_id_t core_id = ocf_core_get_id(core);

ocf_core_seq_cutoff_deinit(core);
env_free(core->counters);
core->counters = NULL;
core->added = false;
env_bit_clear(core_id, cache->conf_meta->valid_core_bitmap);

if (!core->opened && --cache->ocf_core_inactive_count == 0)
env_bit_clear(ocf_cache_state_incomplete, &cache->cache_state);

cache->conf_meta->core_count--;
}",/* Deinit in-memory structures related to this core */,// Accepts a ocf_core_t. Deinitialize in-memory structures related to this core,
"static int rv_cache_mrc_put(struct rv_mr_cache *cache,
    void *arg, struct rv_mr_cached *mrc)
{
int refcount;

refcount = atomic_dec_return(&mrc->refcount);
if (!refcount) {
cache->stats.inuse--;
cache->stats.inuse_bytes -= mrc->len;
}
return refcount;
}",/* called with cache->lock */,"/* Accepts 2 structs and a void pointer.
called with cache?lock*/",
"struct VendorList* vendor_list_new() {
struct VendorList* list = malloc(sizeof(struct VendorList));
if (list != NULL) {
list->next = NULL;
}
return list;
}","/**
 * Create a new struct VendorList
 * @returns a pointer to the newly allocated struct VendorList
 */",/*Create a new struct VendorList and returns a pointer to it*/,
"void*
ExecEvalExprCodegenEnroll(ExecEvalExprFn regular_func_ptr,
                          ExecEvalExprFn* ptr_to_regular_func_ptr,
                          struct ExprState *exprstate,
                          struct ExprContext *econtext)
{
  *ptr_to_regular_func_ptr = regular_func_ptr;
   elog(ERROR, ""mock implementation of ExecEvalExprCodegenEnroll called"");
   return NULL;
}",// Enroll and returns the pointer to ExecEvalExprGenerator,// Accepts a ExecEvalExprFn. Enroll and returns the pointer to ExecEvalExprGenerator,
"void
btr_compress(
/*=========*/
btr_cur_t*cursor,/* in: cursor on the page to merge or lift;
the page must not be empty: in record delete
use btr_discard_page if the page would become
empty */
mtr_t*mtr)/* in: mtr */
{
dict_index_t*index;
ulintspace;
ulintleft_page_no;
ulintright_page_no;
page_t*merge_page;
page_t*father_page;
iboolis_left;
page_t*page;
rec_t*orig_pred;
rec_t*orig_succ;
rec_t*node_ptr;
ulintdata_size;
ulintn_recs;
ulintmax_ins_size;
ulintmax_ins_size_reorg;
ulintcomp;

page = btr_cur_get_page(cursor);
index = btr_cur_get_index(cursor);
comp = page_is_comp(page);
ut_a((ibool)!!comp == dict_table_is_comp(index->table));

ut_ad(mtr_memo_contains(mtr, dict_index_get_lock(index),
MTR_MEMO_X_LOCK));
ut_ad(mtr_memo_contains(mtr, buf_block_align(page),
MTR_MEMO_PAGE_X_FIX));
space = dict_index_get_space(index);

left_page_no = btr_page_get_prev(page, mtr);
right_page_no = btr_page_get_next(page, mtr);

#if 0
fprintf(stderr, ""Merge left page %lu right %lu \n"",
left_page_no, right_page_no);
#endif

node_ptr = btr_page_get_father_node_ptr(index, page, mtr);
ut_ad(!comp || rec_get_status(node_ptr) == REC_STATUS_NODE_PTR);
father_page = buf_frame_align(node_ptr);
ut_a(comp == page_is_comp(father_page));

/* Decide the page to which we try to merge and which will inherit
the locks */

is_left = left_page_no != FIL_NULL;

if (is_left) {

merge_page = btr_page_get(space, left_page_no, RW_X_LATCH,
  mtr);
#ifdef UNIV_BTR_DEBUG
ut_a(btr_page_get_next(merge_page, mtr)
     == buf_frame_get_page_no(page));
#endif /* UNIV_BTR_DEBUG */
} else if (right_page_no != FIL_NULL) {

merge_page = btr_page_get(space, right_page_no, RW_X_LATCH,
  mtr);
#ifdef UNIV_BTR_DEBUG
ut_a(btr_page_get_prev(merge_page, mtr)
     == buf_frame_get_page_no(page));
#endif /* UNIV_BTR_DEBUG */
} else {
/* The page is the only one on the level, lift the records
to the father */
btr_lift_page_up(index, page, mtr);

return;
}

n_recs = page_get_n_recs(page);
data_size = page_get_data_size(page);
ut_a(page_is_comp(merge_page) == comp);

max_ins_size_reorg = page_get_max_insert_size_after_reorganize(
merge_page, n_recs);
if (data_size > max_ins_size_reorg) {

/* No space for merge */

return;
}

ut_ad(page_validate(merge_page, index));

max_ins_size = page_get_max_insert_size(merge_page, n_recs);

if (data_size > max_ins_size) {

/* We have to reorganize merge_page */

btr_page_reorganize(merge_page, index, mtr);

max_ins_size = page_get_max_insert_size(merge_page, n_recs);

ut_ad(page_validate(merge_page, index));
ut_ad(page_get_max_insert_size(merge_page, n_recs)
      == max_ins_size_reorg);
}

if (data_size > max_ins_size) {

/* Add fault tolerance, though this should never happen */

return;
}

btr_search_drop_page_hash_index(page);

/* Remove the page from the level list */
btr_level_list_remove(page, mtr);

if (is_left) {
btr_node_ptr_delete(index, page, mtr);
} else {
mem_heap_t*heap= NULL;
ulintoffsets_[REC_OFFS_NORMAL_SIZE];
*offsets_ = (sizeof offsets_) / sizeof *offsets_;
/* Replace the address of the old child node (= page) with the
address of the merge page to the right */

btr_node_ptr_set_child_page_no(node_ptr,
       rec_get_offsets(
       node_ptr, index,
       offsets_,
       ULINT_UNDEFINED,
       &heap),
       right_page_no, mtr);
if (UNIV_LIKELY_NULL(heap)) {
mem_heap_free(heap);
}
btr_node_ptr_delete(index, merge_page, mtr);
}

/* Move records to the merge page */
if (is_left) {
orig_pred = page_rec_get_prev(
page_get_supremum_rec(merge_page));
page_copy_rec_list_start(merge_page, page,
 page_get_supremum_rec(page),
 index, mtr);

lock_update_merge_left(merge_page, orig_pred, page);
} else {
orig_succ = page_rec_get_next(
page_get_infimum_rec(merge_page));
page_copy_rec_list_end(merge_page, page,
       page_get_infimum_rec(page),
       index, mtr);

lock_update_merge_right(orig_succ, page);
}

/* We have added new records to merge_page: update its free bits */
ibuf_update_free_bits_if_full(index, merge_page,
      UNIV_PAGE_SIZE, ULINT_UNDEFINED);

ut_ad(page_validate(merge_page, index));

/* Free the file page */
btr_page_free(index, page, mtr);

ut_ad(btr_check_node_ptr(index, merge_page, mtr));
}","/*****************************************************************
Tries to merge the page first to the left immediate brother if such a
brother exists, and the node pointers to the current page and to the brother
reside on the same page. If the left brother does not satisfy these
conditions, looks at the right brother. If the page is the only one on that
level lifts the records of the page to the father page, thus reducing the
tree height. It is assumed that mtr holds an x-latch on the tree and on the
page. If cursor is on the leaf level, mtr must also hold x-latches to the
brothers, if they exist. NOTE: it is assumed that the caller has reserved
enough free extents so that the compression will always succeed if done! */","/*Accepts a btr_cur_t pointer and a mtr_t pointer. 
Tries to merge page to left immediate brother or right immediate brother in that order. 
Also checks if the node pointers to the current page and the brother reside on the same page. 
If the page is the only one on that level lifts the records of the page to the father page, thus reducing the tree height.*/",
"void blk_queue_split(struct bio **bio)
{
unsigned int nr_segs;

__blk_queue_split(bio, &nr_segs);
}","/**
 * blk_queue_split - split a bio and submit the second half
 * @bio: [in, out] bio to be split
 *
 * Split a bio into two bios, chains the two bios, submit the second half and
 * store a pointer to the first half in *@bio. Since this function may allocate
 * a new bio from @bio->bi_disk->queue->bio_split, it is the responsibility of
 * the caller to ensure that @bio->bi_disk->queue->bio_split is only released
 * after processing of the split bio has finished.
 */","/* Accepts a struct which represnts a bio.
Recursively split a bio and submit the second half*/",
"int stab_fun(string str){
if(str!=""puppet""){
tell_object(TP,""You should try to <stab puppet>!"");
return 0;
}
else{
if(puppet){
tell_room(TO,""%^BOLD%^%^RED%^""+TP->QCN+"" %^BOLD%^%^RED%^strikes the puppet solidly in the bullseye!%^RESET%^"",TP);
tell_object(TP,""%^BOLD%^%^RED%^You strike the puppet solidly in the bullseye!%^RESET%^"");
tell_room(ETP,"""");
tell_room(ETP,""%^BOLD%^%^BLACK%^The puppet starts to convulse violently, miming in great detail a horrible and excruciating death! It finally collapses on the ground off to the side of the room.%^RESET%^"");
puppet=0;
}
else{
tell_object(TP,""%^BOLD%^%^RED%^Stop, it's already dead!%^RESET%^"");
return 1;
}
}
return 1;
}","// This lets the player stab the guardian puppet, removing the first barrier to exiting.","/*Accepts a string argument.
This lets the player stab the guardian puppet, removing the first barrier to exiting.
Returns 0 if user tries to stab other objects, 1 if puppet is already dead */",
"void ReverseCharacters(char *string){
int count = NumberOfCharacters(string);
int s = 1;
//char display;
while (s){
if (count == -1){
s = 0;
}
else{
//display = *string+count;
//printf(""%c"", display);
// must put *(string+count)
printf(""%c"", *(string+count));
count--;
}
}
}","// must minus one to count because of the null '\0' character
","/* Accepts a character string
Reverse the character string by printing the characters in reverse */",
"static inline void soc_eu_selEventTimer(unsigned int timer_id, unsigned int mask) {
  pulp_write32 (ARCHI_SOC_EU_ADDR + SOC_TIMER_SEL_HI + timer_id * 4
            ,  (pulp_read32(ARCHI_SOC_EU_ADDR + SOC_TIMER_SEL_HI + timer_id * 4) & ~(SOC_TIMER_SEL_EVT_MASK << SOC_TIMER_SEL_EVT_SHIFT)) 
              | (mask & SOC_TIMER_SEL_EVT_MASK));
}","/** \brief Select event to be propagated to timer unit event input.
 *
 * \param timer_id Choice timer Low (1) or High (0).
 * \param mask Value on 8 bit to select the event id to be forwarded to the selected timer.
 */","/*Accepts 2 unsigned integers.
Selects the event to be propagated to the timer unsigned int event input. 
The event is determined by the passed in mask value and the timer is determined by the passed in timer_id*/",
"LABIRINTO* alocalabirinto(void){
    LABIRINTO* labirinto = (LABIRINTO*)malloc(sizeof(LABIRINTO));
        labirinto->linhas = NULL;
        labirinto->numeros = (int*)malloc(4 * sizeof(int));
        labirinto->pessoas = 0;
        labirinto->espacos_livres = 0;
        labirinto->espacos_visitados = 0;
        labirinto->porcentagem = 0;
    return labirinto;
}",//aloca a quantidade de memoria necessaria para um struct labirinto,// Allocates the amount of memory needed for a maze struct,
"int
pocl_cpuinfo_detect_max_clock_frequency() {
  int cpufreq=-1;
  
  // First try to get the result from cpufreq interface.
  cpufreq = pocl_cpufreq_get_max();
  if( cpufreq != -1 )
    return cpufreq;

  if (access (cpuinfo, R_OK) != 0) 
      return -1;
  else 
    {
      FILE *f = fopen (cpuinfo, ""r"");
      char contents[MAX_CPUINFO_SIZE];
      int num_read = fread (contents, 1, MAX_CPUINFO_SIZE - 1, f);            
      float freq = 0.0f;
      fclose (f);
      contents[num_read] = '\0';

      /* Count the number of times 'processor' keyword is found which
         should give the number of cores overall in a multiprocessor
         system. In Meego Harmattan on ARM it prints Processor instead of
         processor */
      char* p = contents;
      if ((p = strstr (p, FREQSTRING)) != NULL &&
          (p = strstr (p, "": "")) != NULL)
        {
          if (sscanf (p, "": %f"", &freq) == 0)
            {
#ifdef DEBUG_POCL_CPUINFO
              printf (""could not parse the cpu MHz field %f\n"", freq);
              puts (p);
#endif
              return -1;
            }           
          else 
            {
#ifdef DEBUG_POCL_CPUINFO
              printf (""max_freq %d\n"", (int)freq);
#endif
              return (int)freq;
            }
        }
    } 
  return -1;  
}","/**
 * Detects the maximum clock frequency of the CPU by parsing the cpuinfo.
 *
 * Assumes all cores have the same max clock freq. On some platforms, 
 * /proc/cpuinfo does not provide max CPU frequecny (ARM pandaboard), 
 * others give the *current* frequency, not the max (x86_64).
 *
 * @return The clock frequency in MHz, or -1 if couldn't figure it out.
 */","/*Detects the maximum clock frequency of the CPU by parsing the cpuinfo.
If error occurs before determining max clock frequency, prints error*/",
"static int
sensor_set_auto_brightness(struct usb_ov511 *ov, int enable)
{
int rc;

PDEBUG(4, "" (%s)"", enable ? ""turn on"" : ""turn off"");

if (ov->sensor == SEN_KS0127 || ov->sensor == SEN_KS0127B
|| ov->sensor == SEN_SAA7111A) {
PDEBUG(5, ""Unsupported with this sensor"");
return -EPERM;
}

rc = i2c_w_mask(ov, 0x2d, enable?0x10:0x00, 0x10);
if (rc < 0)
return rc;

ov->auto_brt = enable;

return 0;
}","/* If enable is true, turn on the sensor's auto brightness control, otherwise
 * turn it off.
 *
 * Unsupported: KS0127, KS0127B, SAA7111A
 * Returns: 0 for success
 */","/*Accept a struct and an integer.
If enable is true, turn on the sensor's auto brightness control, otherwise turn it off.
If enabled return 0, otherwise return -EPERM*/",
"public byte[] Extract(byte[] ikm, byte[] salt = null)
        {
            if (ikm == null)
                throw new ArgumentNullException(nameof(ikm));
            if (salt == null)
                salt = new byte[HashLength];
            if (_disposed)
                throw new ObjectDisposedException(GetType().FullName);

            InitializeHMAC(salt);

            return _hmac.ComputeHash(ikm);
        }","/// <summary>
/// Performs the HKDF-Extract function.
/// </summary>
/// <param name=""ikm"">The input keying material for HKDF-Extract.</param>
/// <param name=""salt"">An optional salt value (a non-secret random value); if not provided, it is set to a string of HashLen zeros.</param>
/// <returns>a pseudorandom key of <see cref=""HashLength""/> bytes</returns>","/* Accepts 2 byte arrays one for the salt and one for the key.
Performs the extract function in the Hash-based Key Derivation Function process.
Returns the computed Hash */",
"public static KubeClientOptions FromPodServiceAccount(string serviceAccountPath = KubeClientConstants.DefaultServiceAccountPath)
        {
            string kubeServiceHost = Environment.GetEnvironmentVariable(KubeClientConstants.KubernetesServiceHost);
            string kubeServicePort = Environment.GetEnvironmentVariable(KubeClientConstants.KubernetesServicePort);
            if (String.IsNullOrWhiteSpace(kubeServiceHost) || String.IsNullOrWhiteSpace(kubeServicePort))
                throw new InvalidOperationException($""KubeApiClient.CreateFromPodServiceAccount can only be called when running in a Kubernetes Pod ({KubeClientConstants.KubernetesServiceHost} and/or {KubeClientConstants.KubernetesServicePort} environment variable is not defined)."");

            string defaultNamespace = File.ReadAllText(Path.Combine(serviceAccountPath, ""namespace"")).Trim();

            string apiEndPoint = $""https://{kubeServiceHost}:{kubeServicePort}/"";
            string accessToken = File.ReadAllText(Path.Combine(serviceAccountPath, ""token""));
            var kubeCACertificate = new X509Certificate2(
                File.ReadAllBytes(Path.Combine(serviceAccountPath, ""ca.crt""))
            );

            return new KubeClientOptions
            {
                ApiEndPoint = new Uri(apiEndPoint),
                AuthStrategy = KubeAuthStrategy.BearerToken,
                AccessToken = accessToken,
                CertificationAuthorityCertificate = kubeCACertificate,
                KubeNamespace = defaultNamespace
            };
        }","/// <summary>
///     Create new <see cref=""KubeClientOptions""/> using pod-level configuration. 
/// </summary>
/// <param name=""serviceAccountPath"">
///     The location of the volume containing service account token, CA certificate, and default namespace.
/// </param>
/// <returns>
///     The configured <see cref=""KubeClientOptions""/>.
/// </returns>
/// <remarks>
///     Only works from within a container running in a Kubernetes Pod.
/// </remarks>
/// <exception cref=""InvalidOperationException""></exception>",// Accepts a serviceAccountPath string. Create and return new KubeClientOptions using pod-level configuration.,
"public void SetSiloType(Silo.SiloType siloType)
        {
            logger.Info(ErrorCode.SiloSetSiloType, ""Setting silo type {0}"", siloType);
            Type = siloType;
        }","/// <summary>
/// Set the type of this silo. Default is Secondary.
/// </summary>
/// <param name=""siloType"">Type of this silo.</param>",// Set the type of this silo if possible otherwise set to Default type.,
"[MethodImpl(MethodImplOptions.AggressiveInlining)]
        public TValue GetOrAdd(TKey key, Func<TKey, TValue> valueFactory)
        {
            if (dictionary.TryGetValue(key, out TValue value))
            {
                return value;
            }
            lock (GetLock(key))
            {
                return dictionary.GetOrAdd(key, valueFactory);
            }
        }","/// <summary>
/// Adds a key/value pair to the dictionary if it does not exist.
/// </summary>","/* Accepts a key, and a dictionary. 
Attempts to get and return a value based on passed in key.
Adds a key/value pair to the dictionary if it does not exist. */",
"public override object Evaluate()
        {
            if (string.Compare(TypeName, ""Date"", false) == 0)
                return new DateTime(DateTime.Now.Ticks);
            
            object[] constructorArgs = null;
            if (ParamListExpressions != null && ParamListExpressions.Count > 0)
            {
                ParamList = new List<object>();
                FunctionHelper.ResolveParameters(ParamListExpressions, ParamList);
                constructorArgs = ParamList.ToArray();
            }
            return Ctx.Types.Create(TypeName, constructorArgs);
        }","/// <summary>
/// Creates new instance of the type.
/// </summary>
/// <returns></returns>",// Creates and returns new instance of DateTime type or of the different types in the constructorArgs array.,
"[MethodImpl(MethodImplOptions.AggressiveInlining)]
        public TrinityErrorCode AddCell(CellAccessOptions writeAheadLogOptions, long cellId, byte* buff, int offset, int cellSize)
        {
            return CLocalMemoryStorage.CAddCell(cellId, buff + offset, cellSize, ushort.MaxValue, writeAheadLogOptions);
        }","/// <summary>
/// Adds a new cell to the Trinity key-value store.
/// </summary>
/// <param name=""writeAheadLogOptions"">Specifies write-ahead logging behavior. Valid values are CellAccessOptions.StrongLogAhead(default) and CellAccessOptions.WeakLogAhead. Other values are ignored.</param>
/// <param name=""cellId"">A 64-bit cell Id.</param>
/// <param name=""buff"">A memory buffer that contains the cell content.</param>
/// <param name=""offset"">The byte offset into the buff.</param>
/// <param name=""cellSize"">The size of the cell.</param>
/// <returns>true if adding succeeds; otherwise, false.</returns>","/* Accepts CellAccessOptions long, byte pointer and 2 integer. 
Adds a new cell to the Trinity key-value store using passed in values */",
"public Publisher<M> advertise<M>(string topic, int queue_size, SubscriberStatusCallback connectcallback,
            SubscriberStatusCallback disconnectcallback)
            where M : IRosMessage, new()
        {
            return advertise<M>(topic, queue_size, connectcallback, disconnectcallback, false);
        }","/// <summary>
///     Creates a publisher with connect and disconnect callbacks
/// </summary>
/// <typeparam name=""M"">Type of topic</typeparam>
/// <param name=""topic"">Name of topic</param>
/// <param name=""queue_size"">How many messages to enqueue if asynchrinous</param>
/// <param name=""connectcallback"">Callback to fire when this node connects</param>
/// <param name=""disconnectcallback"">Callback to fire when this node disconnects</param>
/// <returns>A publisher with the specified topic type, name and options</returns>","// Accepts string, integer, SubscriberStatusCallback. Creates and returns a publisher with connect and disconnect callbacks",
"[HttpPut(""{leaseId:long}/payment/{paymentId:long}"")]
        [HasPermission(Permissions.LeaseEdit)]
        [Produces(""application/json"")]
        [ProducesResponseType(typeof(IEnumerable<Models.Lease.LeaseModel>), 200)]
        [SwaggerOperation(Tags = new[] { ""lease"" })]
        public IActionResult UpdatePayment(long leaseId, long paymentId, [FromBody] Models.Lease.PaymentModel paymentModel)
        {
            var paymentEntity = _mapper.Map<PimsLeasePayment>(paymentModel);
            var updatedLease = _pimsService.LeasePaymentService.UpdatePayment(leaseId, paymentId, paymentModel.LeaseRowVersion, paymentEntity);

            return new JsonResult(_mapper.Map<Models.Lease.LeaseModel>(updatedLease));
        }","/// <summary>
/// Update the specified payment on the passed lease.
/// </summary>
/// <returns></returns>","/* Accepts leaseId, paymentId, and Lease Payment Model. 
Update the specified payment on the passed lease */",
"public static Wavenumber? Average(this IEnumerable<Wavenumber?> source)
        {
            if (source is null)
            {
                throw new ArgumentNullException(""source"");
            }

            double sum = 0;
            long count = 0;
            checked
            {
                foreach (var v in source)
                {
                    if (v != null)
                    {
                        sum += v.Value.reciprocalMetres;
                        count++;
                    }
                }
            }

            if (count > 0)
            {
                return Wavenumber.FromReciprocalMetres(sum / count);
            }

            return null;
        }","/// <summary>
/// Calculates the average <see cref=""Nullable{Wavenumber}""/> for the values in <paramref name=""source""/>
/// </summary>
/// <param name=""source""><see cref=""IEnumerable{Wavenumber}""/></param>
/// <returns>The average</returns>",// Accepts IEnumerable source. Calculates the average  for the values in source,
"public static bool LoadFromFile(string fileName, System.Text.Encoding encoding, out IssueDateType obj, out System.Exception exception)
        {
            exception = null;
            obj = default(IssueDateType);
            try
            {
                obj = LoadFromFile(fileName, encoding);
                return true;
            }
            catch (System.Exception ex)
            {
                exception = ex;
                return false;
            }
        }","/// <summary>
/// Deserializes xml markup from file into an IssueDateType object
/// </summary>
/// <param name=""fileName"">string xml file to load and deserialize</param>
/// <param name=""obj"">Output IssueDateType object</param>
/// <param name=""exception"">output Exception value if deserialize failed</param>
/// <returns>true if this XmlSerializer can deserialize the object; otherwise, false</returns>","/* Accepts filename, encoding, IssueDataType objects and an exception.
Converts xml markup file into an IssueDateType Object.
Returns true of false indicating if conversion was succesful */",
"public static bool IsValidBrazilianCorporationDocument(this string document)
        {
            document = document.RemoveNonNumeric();
            var digits = document.CalculateBrazilianCorporationDocument();
            return digits.Length == 2 && document.EndsWith(digits);
        }","/// <summary>
/// Determines whether [is valid brazilian corporation document].
/// </summary>
/// <param name=""document"">The document.</param>
/// <returns>
///   <c>true</c> if [is valid brazilian corporation document] [the specified document]; otherwise, <c>false</c>.
/// </returns>",// Accepts string document. Determines whether [is valid brazilian corporation document].,
"public void Delete(Uri location)
        {
            if (location == null) throw Error.ArgumentNull(""location"");

            var id = verifyResourceIdentity(location, needId: true, needVid: false);
            var tx = new TransactionBuilder(Endpoint).Delete(id.ResourceType, id.Id).ToBundle();

            execute<Resource>(tx, HttpStatusCode.NoContent);

            return;
        }","/// <summary>
/// Delete a resource at the given endpoint.
/// </summary>
/// <param name=""location"">endpoint of the resource to delete</param>
/// <returns>Throws an exception when the delete failed, though this might
/// just mean the server returned 404 (the resource didn't exist before) or 410 (the resource was
/// already deleted).</returns>",// Accepts a Uri. Deletes a resource at the given endpoint.,
"public ApiResponse< LabelsResponse > GetDashboardsIDLabelsWithHttpInfo (string dashboardID, string zapTraceSpan = null)
        {
            // verify the required parameter 'dashboardID' is set
            if (dashboardID == null)
                throw new ApiException(400, ""Missing required parameter 'dashboardID' when calling DashboardsService->GetDashboardsIDLabels"");

            var localVarPath = ""/api/v2/dashboards/{dashboardID}/labels"";
            var localVarPathParams = new Dictionary<String, String>();
            var localVarQueryParams = new List<KeyValuePair<String, String>>();
            var localVarHeaderParams = new Dictionary<String, String>(this.Configuration.DefaultHeader);
            var localVarFormParams = new Dictionary<String, String>();
            var localVarFileParams = new Dictionary<String, FileParameter>();
            Object localVarPostBody = null;

            // to determine the Content-Type header
            String[] localVarHttpContentTypes = new String[] {
            };
            String localVarHttpContentType = this.Configuration.ApiClient.SelectHeaderContentType(localVarHttpContentTypes);

            if (dashboardID != null) localVarPathParams.Add(""dashboardID"", this.Configuration.ApiClient.ParameterToString(dashboardID)); // path parameter
            if (zapTraceSpan != null) localVarHeaderParams.Add(""Zap-Trace-Span"", this.Configuration.ApiClient.ParameterToString(zapTraceSpan)); // header parameter

            // to determine the Accept header
            String[] localVarHttpHeaderAccepts = new String[] {
                ""application/json""
            };

            String localVarHttpHeaderAccept = this.Configuration.ApiClient.SelectHeaderAccept(localVarHttpHeaderAccepts);
            if (localVarHttpHeaderAccept != null && !localVarHeaderParams.ContainsKey(""Accept""))
                localVarHeaderParams.Add(""Accept"", localVarHttpHeaderAccept);


            // make the HTTP request
            IRestResponse localVarResponse = (IRestResponse) this.Configuration.ApiClient.CallApi(localVarPath,
                Method.GET, localVarQueryParams, localVarPostBody, localVarHeaderParams, localVarFormParams, localVarFileParams,
                localVarPathParams, localVarHttpContentType);

            int localVarStatusCode = (int) localVarResponse.StatusCode;

            if (ExceptionFactory != null)
            {
                Exception exception = ExceptionFactory(""GetDashboardsIDLabels"", localVarResponse);
                if (exception != null) throw exception;
            }

            return new ApiResponse<LabelsResponse>(localVarStatusCode,
                localVarResponse.Headers.ToDictionary(x => x.Name, x => x.Value.ToString()),
                (LabelsResponse) this.Configuration.ApiClient.Deserialize(localVarResponse, typeof(LabelsResponse)));
        }","/// <summary>
/// List all labels for a dashboard 
/// </summary>
/// <exception cref=""InfluxDB.Client.Api.Client.ApiException"">Thrown when fails to make API call</exception>
/// <param name=""dashboardID"">The dashboard ID.</param>
/// <param name=""zapTraceSpan"">OpenTracing span context (optional)</param>
/// <returns>ApiResponse of LabelsResponse</returns>","/* Accepts a dashboard ID and a zapTracespan.
 List all labels for dashboard associated with ID */",
"[ExcludeFromCodeCoverage]
public void Update([NotNull] string key, [NotNull] string data,
DatastoreOperation operation, Credentials credentials = null, Action<Response<string>> callback = null) {
Wrap(UpdateAsync(key, data, operation, credentials), callback);
}","/// <summary>
/// Updates data in the data store. On success, the new value of the data item is returned.
/// Caution: GameJolt has a hard limit of 16MB per key-value pair and a soft limit of 1MB per post-request.
/// </summary>
/// <param name=""key"">The key of the data item you'd like to update.</param>
/// <param name=""data"">The value you'd like to apply to the data store item. </param>
/// <param name=""operation"">The operation you'd like to perform. 
/// Add, Subtract, Multiply and Divide are only applicable to numeric values.</param>
/// <param name=""credentials"">If you pass in the user information, this function will use the user's data store. 
/// If you leave the user information empty, it will use the game's global data store.</param>
/// <param name=""callback"">Action that is called on completion or error.</param>
/// <returns></returns>","/* Accepts string key and data, DatastoreOperation, credentials, and an Action<Response<string>> callback. 
Updates data in the data store. On success, the new value of the data item is returned. */",
"[HttpPost]
        [Route(""get-campaign-report-by-filter-excel"")]
        public async Task<IActionResult> GetByFilterExcel(CampaignReportRequest request, [FromHeader(Name = ""userid"")][Required] string userId)
        {
            var result = await _reportService.GetCampaignReportExcelAsync(request, General.GetUserIdFromHeader(Request));
            return Ok(result);
        }","/// <summary>
/// Returns the campaign report excel file data by selected filter options
/// </summary>
/// <param name=""request""></param>
/// <returns></returns>","// Accepts request, string userId. Returns the campaign report excel file data by userID",
"public void Ping() {
            if (spLcd == null)
                throw new InvalidOperationException(""Not connected to an LCD module"");
            byte[] someData = new byte[] { 0xab, 0xac, 0xad, 0x01 };
            try {
                var receive = disp.Transaction(new Packet() {
                    Type = 0x0,
                    Data = someData
                }, 0x40);
                if (receive.Data.Length != someData.Length)
                    throw new CommunicationException(""Received ping data does not match what was sent"", CommunicationException.ErrorCodes.GeneralError);
                for (int i = 0; i < someData.Length; i++) {
                    if (someData[i] != receive.Data[i])
                        throw new CommunicationException(""Received ping data does not match what was sent"", CommunicationException.ErrorCodes.GeneralError);
                }
                // all looks good
            }
            catch (Exception ex) {
                throw new InvalidOperationException(""Failed to execute Ping command, look for details in InnerException"", ex);
            }
        }","/// <summary>
/// Sends a ping packet to the LCD and expects a ping reply. If no reply is received, throws a CommunicationException
/// Timeout: 5 seconds
/// </summary>","/* Sends a ping packet to the LCD and expects a ping reply. If no 
reply is received within 5 seconds or received data does not match 
specific requirements throws a CommunicationException */",
"public int ReadBrep(StreamReader reader, SceneBrep scene)
        {
            if (reader == null) return SceneBrep.NULL;

            Debug.Assert(scene != null);
            scene.Reset();
            return ReadBrep(reader, scene, Matrix4.Identity);
        }","/// <summary>
/// Reads one 3D scene from a given stream (containing text variant of Wavefront OBJ format).
/// </summary>
/// <param name=""reader"">Already open text reader</param>
/// <param name=""scene"">Scene to be modified</param>
/// <returns>Number of faces read</returns>","/* Accepts a reader and a scene.
Continually reads a stream and asserts 3D scenes in the stream are not null */",
"static public void GleanContributions_Helper(ICollection<ToolStripMenuItem> items, Type type, Control control, Object obj)
        {
            if (control == null) return;
            if (control is Button)
            {
                Button button = (Button)control;
                string newVariable = obj.GetType() == type
                                         ? type.Name
                                         : string.Format(""{0} -> {1}"", obj.GetType().Name, type.Name);
                items.Add(new ToolStripMenuItem(button.Text, null,
                                                (sender, e) =>
                                                {
                                                    button.PerformClick();
                                                    Logger.DebugLog(String.Format(""Button: {0} {1} {2} {3}"",
                                                                                  newVariable, e, sender, obj));
                                                })
                              {
                                  Enabled = button.Enabled,
                                  Visible = button.Visible,
                                  ToolTipText = """" + obj
                              });
                return;
            }
            foreach (object v in control.Controls)
                GleanContributions_Helper(items, type, (Control)v, obj);
        }","/// <summary>
/// Used by GleanContributions to add new ContextMenu Items gleaned from Parent control that has buttons on it
/// </summary>
/// <param name=""items"">Collection of Items to be added to</param>        
/// <param name=""type"">The type it will target</param>
/// <param name=""control"">Parent control that has buttons on it</param>
/// <param name=""obj"">Will becvome the button's target when </param>","/* Accepts items collection, a type, a control and an object. 
Used by GleanContributions to add new control to the collection of items,
 if the controls are buttons*/",
"private void SetupVideoPositionScrollBar()
        {
            // the thumb itself has width, this is the LargeChange value. If we set it
            // up like this we get 0 to 1000 as the user drags it over the range of the
            // scrollbar
            scrollBarVideoPosition.Maximum = TantaWMFUtils.MAX_DURATION_RANGE + scrollBarVideoPosition.LargeChange-1;
            scrollBarVideoPosition.Minimum = 0;
        }","/// +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=
/// <summary>
/// Sync's the settings on the horizontal scroll bar to the video file duration
/// </summary>
/// <history>
///    01 Nov 18  Cynic - Originally Written
/// </history>",// Sync's the settings on the horizontal scroll bar to the video file duration,
"public TimeSpan Next(TimeSpan min, TimeSpan max)
        {
            if (max <= min)
            {
                throw new ArgumentException(""Max must be greater than min."");
            }

            long minTicks = min.Ticks;
            long maxTicks = max.Ticks;
            double rn = (Convert.ToDouble(maxTicks)
                - Convert.ToDouble(minTicks)) * _rnf.NextDouble()
                + Convert.ToDouble(minTicks);
            return new TimeSpan(Convert.ToInt64(rn));
        }","/// <summary>
/// Returns TimeSpan in the range [min, max)
/// </summary>","/* Accepts minimum and maximum TimeSpan.
Returns TimeSpan in the range [min, max) */",
"function waitForForkToBeReady(fork) {
  return new Promise((resolve, reject) => {
    // Check to see if we can access the fork files once every second. Give up
    // after 15 seconds.
    const checkInterval = 1000;
    const maxAttempts = 30;
    let attempts = 15;

    function checkFork() {
      fork.contents('').fetch()
        .then((contents) => {
          resolve(fork);
        })
        .catch((err) => {
          if (attempts >= maxAttempts) {
            return reject(
              new Error(`Gave up forking after ${maxAttempts} seconds`)
            );
          }

          attempts++;
          setTimeout(checkFork, checkInterval);
        });
    }

    setTimeout(checkFork, checkInterval);
  });
}",// modification. fork is expected to be a repo object from Octokat.,"/* Accepts a fork. Check to see if we can access the fork files once every second. 
If not found within 15 seconds gives up.*/",
"function isInViewPort(element){
    let elementTop = $(element).offset().top;
    let elementBottom = elementTop + $(element).outerHeight();
    let viewPortTop = $(window).scrollTop();
    let viewPortBottom = viewPortTop + $(window).height();
    return elementBottom > viewPortTop + $(""#mynav"").height() + 10 && elementTop < viewPortBottom;
  }",// Check whether element is in viewport,"/* Accepts an element. 
Checks whether the element passed into the function is within the viewport. 
Returns True or False*/",
"function processAdd(cmd) {
    doc = cmd.solrDoc;  // org.apache.solr.common.SolrInputDocument
    id = doc.getFieldValue(""id"");
    logger.info(""update-script#processAdd: id="" + id);

    var instrument_field_name = ""instrumentation"";
    var collection_field_name = ""collection"";

    collection_value = doc.getFieldValue(collection_field_name);
    if(collection_value != null) {
        // Add sorted collection dictionay field
        var field_name = ""collection_dictionary"";
        var sorted_field_name = ""collection_sorted_dictionary"";
        if(collection_value in collection_sorted_dictionary) {
            doc.addField(field_name, collection_sorted_dictionary[collection_value].substr(5));
            logger.debug(""update-script#Added: "" + field_name + ""="" + collection_sorted_dictionary[collection_value]);
            doc.addField(sorted_field_name, collection_sorted_dictionary[collection_value]);
            logger.debug(""update-script#Added: "" + sorted_field_name + ""="" + collection_sorted_dictionary[collection_value]);
        } else {
            doc.addField(field_name, collection_value);
            logger.debug(""update-script#Added: "" + field_name + ""="" + collection_value + ""(WARN: NOT IN DICTIONARY)"");
            doc.addField(sorted_field_name, collection_value + ""::"" + collection_value);
            logger.debug(""update-script#Added: "" + sorted_field_name + ""="" + collection_value + ""(WARN: NOT IN DICTIONARY)"");
        }
    }

    instrumentation_value = doc.getFieldValues(instrument_field_name);
    if(instrumentation_value != null) {
        var instruments_map = {};
        var is_instrument_required = {};
        var instruments_with_alternatives = [];
        var instrument;
        instrumentation_values = instrumentation_value.toArray();

        // Move instrumentation without alternatives to front
        instrumentation_values = moveNonAlternativesToFront(instrumentation_values);

        // Clear current value of the instrumentation field
        doc.removeField(instrument_field_name);
        for(i=0; i < instrumentation_values.length; i++) {
            var value = instrumentation_values[i];

            var info_count_array, info_count_length;
            var info_count_length_map = {};

            if(value.indexOf(""|"") != -1) {  // e.g. cl|sax e.g. cl(2)|sax

                var expanded_combined = """";
                var alternatives = {};

                // Splitting on ""|"" and adding each of them as separate value.
                // Also, concatenate the expanded version and add it as another field
                while(value.indexOf(""|"") != -1) {
                    curr = value.substring(0, value.indexOf(""|""));
                    value = value.substring(value.indexOf(""|"")+1);

                    // Add label value of the current instrument to the dictionary fields
                    instrument = getInstrumentDictionaryValue(curr);

                    alternatives[instrument.name] = true;

                    // Add last value of the instrumentation field
                    doc.addField(instrument_field_name, curr);
                    logger.debug(""update-script#Added: "" + instrument_field_name + ""="" + curr);

                    // Add to map
                    if(!(instrument.name in info_count_length_map)) {
                        info_count_length_map[instrument.name] = getCurrentInfoCountLength(instrument.name, instruments_map)
                    }
                    info_count_length = info_count_length_map[instrument.name];
                    is_required = (instrument.name in is_instrument_required)? true : false;
                    addInstrumentToMap(instrument, instruments_map, is_required, info_count_length);

                    // Append to combined alternatives
                    expanded_combined += joinedNameInfoCount(instrument) + "" OR "";
                }
                // Add label value of the last instrument to the dictionary fields
                instrument = getInstrumentDictionaryValue(value);

                alternatives[instrument.name] = true;

                // Set is_instrument_required true, only if all alternatives are count variants
                // of same instrument E.g: cl(2)|cl(3)
                if (Object.keys(alternatives).length == 1) {
                    is_instrument_required[instrument.name] = true;
                }

                // Add to map
                if(!(instrument.name in info_count_length_map)) {
                    info_count_length_map[instrument.name] = getCurrentInfoCountLength(instrument.name, instruments_map)
                }
                info_count_length = info_count_length_map[instrument.name];
                is_required = (instrument.name in is_instrument_required)? true : false;
                addInstrumentToMap(instrument, instruments_map, is_required, info_count_length);

                // Append to combined alternatives
                expanded_combined += joinedNameInfoCount(instrument);

                // Add to combined field to instruments_with_alternatives
                instruments_with_alternatives.push(expanded_combined);

                // Add last value of the instrumentation field
                doc.addField(instrument_field_name, value);
                logger.debug(""update-script#Added: "" + instrument_field_name + ""="" + value);

            } else { // e.g.1 sax e.g.2 cl(2)
                //addDynamicInstrumentationField(value, doc);
                // Add label value of the instrument to the dictionary fields
                instrument = getInstrumentDictionaryValue(value);

                // Set is_instrument_required true
                is_instrument_required[instrument.name] = true;

                // Add to map
                addInstrumentToMap(instrument, instruments_map, true);

                // Add to combined field to nstruments_with_alternatives
                instruments_with_alternatives.push(joinedNameInfoCount(instrument));

                // Add value to the instrumentation field
                doc.addField(instrument_field_name, value);
                logger.debug(""update-script#Added: "" + instrument_field_name + ""="" + value);
            }
        }

        // Add values in map to doc
        for (var key in instruments_map) {
            // Add key to instrument_dictionary
            doc.addField(""instrumentation_dictionary"", key);
            logger.debug(""update-script#Added: instrumentation_dictionary="" + key);

            var info_count_array = instruments_map[key];

            // Add joined key value to instrument_dictionary_full
            for (var i=0; i<info_count_array.length; i++) {
                var instrument = {""name"": key, ""info_count"": info_count_array[i]};
                doc.addField(""instrumentation_dictionary_full"", joinedNameInfoCountWithSortPrefix(instrument));
                logger.debug(""update-script#Added: instrumentation_dictionary_full="" + joinedNameInfoCountWithSortPrefix(instrument));
            }
        }

        // Add values in instruments_with_alternatives to doc
        for (var i=0; i<instruments_with_alternatives.length; i++) {
            doc.addField(""instrumentation_dictionary_full_with_alt"", instruments_with_alternatives[i]);
            logger.debug(""update-script#Added: instrumentation_dictionary_full_with_alt="" + instruments_with_alternatives[i]);
        }
    }
}",// This function will be called for every record during index.,"/* Accepts a cmd. 
This function is called on every record during index and to possibly add to the fields in a documentation */",
"static removeRoleFromUser(id, role) {

    // Check to see if the user role is in the allowable set of roles.
    if (USER_ROLES.indexOf(role) === -1) {

      // User role is not supported! Error out here.
      return Promise.reject(new Error(`role ${role} is not supported`));
    }

    return UserModel.update({
      id: id
    }, {
      $pull: {
        roles: role
      }
    });
  }","/**
   * Removes a role from a user.
   * @param  {String}   id   id of a user
   * @param  {String}   role role to remove
   * @param  {Function} done callback after the operation is complete
   */",/* Accepts an ID and a role. Removes a role from a user with passed in id. Returns the updated info about the user*/,
"function isEliminated(game) {
		return this.units.filter(function (unit) {
			return !unit.isDead();
		}).length <= 0;
	}","/** An army is eliminated if all its units are eliminated.
	 */",// Accepts a game. An army is eliminated if all its units are eliminated. Returns true or false indicating if an army is eliminated,
"function migrateFormData(savedData, migrations) {
  // migrations is an array that looks like this:
  // [
  //   (savedData) => {
  //     // Makes modifications to savedData to update it from version 0 -> version 1
  //   },
  //   (savedData) => {
  //     // Makes modifications to update the data from version 1 -> version 2
  //   },
  //   ...
  // ]
  // The functions transform the data from version of their index to the next one up.
  // This works because every time the version is bumped on the form, its because
  //  the saved data needs to be manipulated, so there will be no skipped versions.

  // Break out early in case we dont have any migrations for the form yet
  if (!migrations) {
    return savedData;
  }

  let savedDataCopy = Object.assign({}, savedData);
  let savedVersion = savedData.metadata.version;
  while (typeof migrations[savedVersion] === 'function') {
    savedDataCopy = migrations[savedVersion](savedDataCopy);
    savedVersion++;
  }

  return savedDataCopy;
}","/**
 * Transforms the data from an old version of a form to be used in the latest
 *  version.
 *
 * @param {SaveInProgressData} savedData The old version of the
 *  save-in-progress data.
 * @param {Array} migrations - An array of functions which transform the data
 *  saved to work with the current version.
 * @return {SaveInProgressData} The modified save-in-progress data which should
 *  work with the current version of the form.
 */",// Accepts parameters savedData and  migrations. Allows data from an outdated version of a form to be used in the latest version. ,
"async owofy(text) {
    const data = await this.request(`owofy`, `text=${text}`);
    const json = await data.json();
    return json;
  }","/**
   * owofy endpoint
   * @param text: the text to convert to owofied text
   */",// Accepts text parameter. owofy endpoint that owofies the text passed in. Returns a json object,
"function spawnPair(number){
        if(!Game.creeps['Miner'+number]){
            spawn.run('minerEnergy', 'Miner'+number);
        };
        if(!Game.creeps['Hauler'+number]){
            spawn.run('haulerEnergy', 'Hauler'+number);
        };
            
        if(Game.creeps['Miner'+number] && Game.creeps['Hauler'+number]){
            Memory.pairActive = true;
        }
        if (!Game.creeps['Miner'+number] || !Game.creeps['Hauler'+number]) {
            Memory.pairActive = false;;
        };
    }","// MINER / HAULER - add if last num is 0 than go to source 0, if num == 1 then go to source 1
","// Accepts a num parameter. MINER / HAULER - add if last num is 0 than go to source 0, if num == 1 then go to source 1",
"fromJSON(json)
		{
			if(""n"" in json)
				this.modulus = new Integer({ valueHex: stringToArrayBuffer(fromBase64(json.n, true)).slice(0, 256) });
			else
				throw new Error(""Absent mandatory parameter \""n\"""");

			if(""e"" in json)
				this.publicExponent = new Integer({ valueHex: stringToArrayBuffer(fromBase64(json.e, true)).slice(0, 3) });
			else
				throw new Error(""Absent mandatory parameter \""e\"""");
		}","/**
		 * Convert JSON value into current object
		 * @param {Object} json
		 */",// Accepts a json parameter. Sets current object's modulus and public Exponent values using JSON values. Otherwise throws errors,
"unregisterCommands(appId) {
        if (this.providedCommands.has(appId)) {
            this.providedCommands.get(appId).forEach((r) => {
                this.bridge.unregisterCommand(r.slashCommand.command, appId);
                this.touchedCommandsToApps.delete(r.slashCommand.command);
                const ind = this.appsTouchedCommands.get(appId).indexOf(r.slashCommand.command);
                this.appsTouchedCommands.get(appId).splice(ind, 1);
                r.isRegistered = true;
            });
            this.providedCommands.delete(appId);
        }
        if (this.appsTouchedCommands.has(appId)) {
            // The commands inside the appsTouchedCommands should now
            // only be the ones which the App has enabled, disabled, or modified.
            // We call restore to enable the commands provided by the bridged system
            // or unmodify the commands modified by the App
            this.appsTouchedCommands.get(appId).forEach((cmd) => {
                this.bridge.restoreCommand(cmd, appId);
                this.modifiedCommands.get(cmd).isRegistered = false;
                this.modifiedCommands.delete(cmd);
                this.touchedCommandsToApps.delete(cmd);
            });
            this.appsTouchedCommands.delete(appId);
        }
    }","/**
     * Unregisters the commands from the system and restores the commands
     * which the app modified in the system.
     *
     * @param appId the appId for the commands to purge
     */",//Accepts an appId paraemter. Unregisters the commands from the system if it has the appId and restores the commands which the app modified in the system.,
"function createInputElement(taskId, listContainer, listElement) {
    var inputElement = document.createElement(""input"");
    inputElement.setAttribute(""type"", ""text"");
    inputElement.setAttribute(""id"", ""list-item-input-"" + taskId);
    inputElement.setAttribute(""class"", ""list-item"");
    inputElement.setAttribute(""placeholder"", ""Enter task here."");
   
   
    // Initialize flag 'submitFired' to prevent calling submitTask twice. This happens when the user hits 'enter' to submit a task: it fires the inputElement.onkeyup event AND the inputElement.onblur event. When submitFired is true, the 'enter' key has been hit, so exit the inputElement.onblur callback function.
    var submitFired = false;
    document.body.addEventListener('submit', function() {
      submitFired = true;
    });
   
    // Click away to submit, at task creation
    inputElement.onblur = function() { 
      if(submitFired) {
        return;
      }
      checkToSubmit(taskId);
    };
    
    /*
    Listen for when user presses the 'Enter' key, if input has a non-empty value, submitTask... If it's the last task, add a new task. If the submitted task is empty, add keyframes bounce animation.
    */
    inputElement.onkeyup = function(event) {
      var userInput = inputElement.value;
      if (userInput !== """" && event.keyCode === 13) {
        var submitEvent = new CustomEvent('submit');
        document.body.dispatchEvent(submitEvent);
        submitTask(userInput, taskId);
        /*
        Need to make sure the last task is the one being submitted before adding a new task... The <div> that holds each task's elements has an id of the form ""list-item-container-x"". Does x = taskId? If so, the submitted task is the last task on the list, so add a new task.
        First get x, which is the value of 'row' at the time the <div> was created. x is the number after the last dash on the id name.
        */
        var idString = listContainer.lastChild.id;
        /*
        id name is of form: ""list-item-container-x"", so want 4th element in array (indexes at 0) returned by element.split
        */
        var lastId = idString.split(""-"")[3];
        //taskId is a number, lastId is a string
        if (taskId.toString() === lastId) {
          addTask();
        }
      } else if (userInput === """" && event.keyCode === 13) {
        inputElement.classList.add(""bounce"");
        setTimeout(function() {
          /*
          remove the class so animation can occur as many times as user triggers event, delay must be longer than the animation duration and any delay.
          */
          inputElement.classList.remove(""bounce"");
        }, 1000);
      }
    };

    return inputElement;
  }","/*
  Creates a new, unique input text field, <input type=""text""> with which the user can enter and submit a task, adding a new task (if the submitted task is the last task) or remove an empty task (that isn't the last task) when the user clicks away; taskId is a number.
  */","/* Accepts taskId, listContainer, and listElement parameters Creates a new, unique input text field, <input type=""text""> with which the user can 
enter and submit a task, adding a new task (if the submitted task is the last task) 
or remove an empty task (that isn't the last task) when the user clicks away; taskId is a number. */",
"containsEdge(node1, node2) {
        'use strict';
        return this.adjacencyList[node1].some(x => x.node === node2);

    }","/**
     * Check if there is an edge between the specified nodes
     *
     * @param node1 - the first node
     * @param node2 - the second node
     * @returns True if there is an edge. False if there is no edge.
     */",// Accepts two nodes as parameters. Check if there is an edge between the specified nodes and returns true,
"function collapseAceEditor() {
    toggleAceEditor();
    let vIcon = document.getElementById(CTRLS.SOURCE.id + 'Text');
    vIcon.style.color = (CTRLS.SOURCE.collapsed) ? 'lightgray' : '#007bff';
}",// Use to toggle Editor - use by collaspe source button,// Use to toggle Editor and change the vlcon style color,
"function forkRepo() {
  document.getElementById(""forking_spinner"").active = true;
  var baseRepo = github.getRepo(""polytipe"", ""polytipe-projects"");
  baseRepo.fork(function(err,res) {
    app.toggle_issues_body = {
      ""name"": ""polytipe-projects"",
      ""has_issues"": true
    };
    app.toggle_issues_params = {""access_token"": app.token};
    document.getElementById('toggle_issues_ajax').generateRequest();
    user_repos = [];
    user_repos.push({""name"": res.owner.login, ""icon"": ""folder""});
    app.user_repos = user_repos;
    document.getElementById(""forking_spinner"").active = false;
    document.getElementById(""loading_repos_box"").style.display = ""none"";
    document.getElementById(""empty_state_repo"").style.display = ""none"";
    document.getElementById('create_repo_dialog').close();
    document.getElementById('created_repo_toast').show();
  });
}",//Forks the the polytipe-projects repo if it doesn't have it,// Forks the the polytipe-projects repo if it doesn't have it,
"function loadNameFromSQL(name, year, month) {
    return new Promise(function (resolve, reject) {
        // Load file with table names for checking
        IO.loadTextFile('tables').then(function (measureList) {
            let tableName;

            // Check if inserted measure data really exists
            for (i = 0; i < measureList.length; i++) {
                if (measureList[i][0] === name) {
                    tableName = (measureList[i][measureList[i].length - 1]).slice(0, (measureList[i][measureList[i].length - 1]).length - 1);
                }
            }

            // Slice out the info for building the yearly column
            let indexCut = tableName.indexOf('~');
            tableName = tableName.slice(0, indexCut);

            // If user has entered a year we can add it and query the database
            if (year) {
                tableName = tableName + ""_"" + year.trim();
                SQL.getMeasureFromDB(tableName, month).then(function (result) {
                    resolve([tableName, result]);
                }).catch(function (error) {
                    reject(error);
                });
            }

            // Catch errors while loading and pass them upwards
        }).catch(function (error) {
            reject(error);
        });
    });
}","/**
 * Receives name and year, converts it to corresponding name in sql database and queries
 * the database for this measure, converts it to a string and passes it upwards. All errors
 * are also passed upwards.
 * @param {Name of the measure that we want the data of.} name 
 * @param {Year of the measure that we want the data of.} year 
 */","/* Uses the passed in name, year and month to query the database, 
resolves the result by converting it to a string and passing it upward. */ ",
"function createCylinder(radiusTop, radiusBottom, height, radialSegments, color,
    x, y, z) {
    var geom = new THREE.CylinderGeometry(radiusTop, radiusBottom, height, radialSegments);
    var mat = new THREE.MeshPhongMaterial({ color: color, flatShading: true });
    var cylinder = new THREE.Mesh(geom, mat);
    cylinder.castShadow = true;
    cylinder.receiveShadow = true;
    cylinder.position.set(x, y, z);
    return cylinder;
}","/**
 * Generic cylinder that casts and receives shadows
 */","// Accepts following parameters: radiusTop, radiusBottom, height, radialSegments, color,x, y, z. Creates a generic cylinder that casts and receives shadows using parameters",
"function processIdentifiers(tokenizer, scope) {
  let params = [];

  if (tokenizer.token.match(types.IDENTIFIER)) {
    const keys = [tokenizer.token.value];

    tokenizer.skipToken();

    while (tokenizer.token.match(types.DOT, types.OPEN_SQUARE_BRACKET)) {
      const token = tokenizer.token;

      tokenizer.skipToken();

      const isValidKey = token.match(types.OPEN_SQUARE_BRACKET) ?
        tokenizer.token.match(types.CONSTANT) :
        (
          (tokenizer.token.match(types.CONSTANT) && (typeof tokenizer.token.value === 'number')) ||
          tokenizer.token.match(types.IDENTIFIER)
        );

      if (!isValidKey) {
        throw new SyntaxError('Unexpected object key notation');
      }

      keys.push(tokenizer.token.value);
      tokenizer.skipToken();

      if (token.match(types.OPEN_SQUARE_BRACKET)) {
        if (!tokenizer.token.match(types.CLOSE_SQUARE_BRACKET)) {
          throw new SyntaxError('Closing square bracket expected');
        }

        tokenizer.skipToken();
      }
    }

    const identifier = keys.join('.');

    if (!tokenizer.token.match(types.OPEN_PARENTHESES)) {
      scope.identifiers.push(identifier);

      return createIdentifierNode(identifier);
    }

    params = [];
    tokenizer.skipToken();
    if (!tokenizer.token.match(types.CLOSE_PARENTHESES)) {
      params.push(processLogicalOr(tokenizer, scope));

      while (tokenizer.token.match(types.DELIMITER)) {
        params.push(processLogicalOr(tokenizer.skipToken(), scope));
      }
    }

    if (!tokenizer.token.match(types.CLOSE_PARENTHESES)) {
      throw new SyntaxError('Unexpected end of expression');
    }

    tokenizer.skipToken();
    return createOperatorNode(identifier, params);
  }

  return processConstants(tokenizer, scope);
}","/**
 * Process custom identifiers and functions
 * @return {Function} compiled node
 * @private
 */",// Accepts tokenizer and scope parameters. Process custom identifiers and functions. Returns a compiled node function,
"start() {
    if (this.#running) return false;

    this.startTime = Date.now();
    this.#timerId = setTimeout(this.#callback.bind(this), this.#timerLength);
    this.#running = true;

    return true;
  }","/*
   * Method to start the timeout
   *
   * returns Boolean (True if timeouts was started. Rejects with false if timeout was already running.)
   */",// Tries to start the timeout and returns True if timeout was started.,
"function ScalarLeafs(context) {
  return {
    Field: function Field(node) {
      var type = context.getType();
      if (type) {
        if ((0, _typeDefinition.isLeafType)(type)) {
          if (node.selectionSet) {
            return new _error.GraphQLError(noSubselectionAllowedMessage(node.name.value, type), [node.selectionSet]);
          }
        } else if (!node.selectionSet) {
          return new _error.GraphQLError(requiredSubselectionMessage(node.name.value, type), [node]);
        }
      }
    }
  };
}","/**
 * Scalar leafs
 *
 * A GraphQL document is valid only if all leaf fields (fields without
 * sub selections) are of scalar or enum types.
 */","/* Accepts a context parameter. Checks if GraphQL document is valid. Only valid if all leaf fields are of scalar or enum types. 
Returns error if not valid. */",
"function GetEventPULAs() {

    layerDefinitions[0] = ""1=0""; //pending
    layerDefinitions[1] = """";    //created
    layerDefinitions[2] = ""1=0""; //published
    layerDefinitions[3] = ""1=0""; //effective
    layerDefinitions[4] = ""1=0""; //expired

    var returningEventPULAs;
    var ShapeIDarray = [];
    window.top.GetEventPULAs(eventID, function (result) {
        returningEventPULAs = result;
        if (returningEventPULAs != undefined) {
            //put the shapeIDs into an array
            //returningPULAs[0] = Created
            for (var i = 0; i < returningEventPULAs.length; i++) {
                ShapeIDarray[i] = returningEventPULAs[i].ShapeID;
            }

            //loop through to set definitions
            var i = 0;
            while (i < ShapeIDarray.length) {
                if (i == ShapeIDarray.length - 1) {
                    layerDefinitions[1] += ""PULASHAPEID = "" + ShapeIDarray[i];
                }
                else {
                    layerDefinitions[1] += ""PULASHAPEID = "" + ShapeIDarray[i] + "" or "";
                }
                i++;
            }
            PendingPULALayer.setLayerDefinitions(layerDefinitions);
            CreatedPULALayer.setLayerDefinitions(layerDefinitions);
            PublishedPULALayer.setLayerDefinitions(layerDefinitions);
            EffectivePULALayer.setLayerDefinitions(layerDefinitions);
            ExpirePULALayer.setLayerDefinitions(layerDefinitions);

            CreatedPULALayer.setVisibility(true);
            dojo.style('refreshScreen', 'visibility', 'hidden');
            //            dojo.style('loadingScreen', 'visibility', 'hidden');

            //do a query to get geometry of results (returningPULAs) and set that as extent
            ZoomToExtent(layerDefinitions[1]);

        }
    });

//    var ShapeIDarray = [];

//    setTimeout(function () {
//        if (returningEventPULAs != undefined) {
//            //put the shapeIDs into an array
//            //returningPULAs[0] = Created
//            for (var i = 0; i < returningEventPULAs.length; i++) {
//                ShapeIDarray[i] = returningEventPULAs[i].ShapeID;
//            }

//            //loop through to set definitions
//            var i = 0;
//            while (i < ShapeIDarray.length) {
//                if (i == ShapeIDarray.length - 1) {
//                    layerDefinitions[1] += ""PULASHAPEID = "" + ShapeIDarray[i];
//                }
//                else {
//                    layerDefinitions[1] += ""PULASHAPEID = "" + ShapeIDarray[i] + "" or "";
//                }
//                i++;
//            }
//            PendingPULALayer.setLayerDefinitions(layerDefinitions);
//            CreatedPULALayer.setLayerDefinitions(layerDefinitions);
//            PublishedPULALayer.setLayerDefinitions(layerDefinitions);
//            EffectivePULALayer.setLayerDefinitions(layerDefinitions);
//            ExpirePULALayer.setLayerDefinitions(layerDefinitions);

//            CreatedPULALayer.setVisibility(true);
//            dojo.style('refreshScreen', 'visibility', 'hidden');
//            //            dojo.style('loadingScreen', 'visibility', 'hidden');

//            //do a query to get geometry of results (returningPULAs) and set that as extent
//            ZoomToExtent(layerDefinitions[1]);

//        }
//    }, 3000);
}",//Get contributor polygons based off the EventId,// Get contributor polygons based off the EventId,
"public function store(Request $request)
    {
        $user_id = $this->getAuthUserId();

        $device_token = $request->device_token;

        if( !$device_token )
        {
            return $this->respondValidationFailed('Device token missing.');
        }
        $ios_device = false;

        if($request->device_type === 'ios')
        {
            $ios_device = true;
        }
        Notification::create(array(
            'user_id' => $user_id,
            'ios' => $ios_device,
            'token' => $device_token
        ));

        return $this->respondCreated('Device token accepted.');
    }","/**
     * Store a newly created resource in storage.
     *
     * @param  \Illuminate\Http\Request  $request
     * @return \Illuminate\Http\Response
     */","/** Accepts a Request object parameter. Stores a newly created resource in storage if a device token is attached. 
Also checks to see if resource is related to ios devices. */",
"public function viewport($string = null, $html = true) {
        $str = $string;
        if (empty($string)) $str = $this->config('meta_viewport');
        
        $this->content['text']['meta_viewport'] = $str;
        $this->content['html']['meta_viewport'] = '<meta name=""' . __FUNCTION__ . '"" content=""' . $str . '"" />';
    }","/**
     * Render Meta Tag for Viewport
     *
     * @param string $string
     * @param string $html
     */","// Accepts two string parameters, 'html' and 'string'. Render Meta Tag for Viewport using string",
"function imscp_view($imscp, $course, $cm, $context) {

    // Trigger course_module_viewed event.
    $params = array(
        'context' => $context,
        'objectid' => $imscp->id
    );

    $event = \mod_imscp\event\course_module_viewed::create($params);
    $event->add_record_snapshot('course_modules', $cm);
    $event->add_record_snapshot('course', $course);
    $event->add_record_snapshot('imscp', $imscp);
    $event->trigger();

    // Completion.
    $completion = new completion_info($course);
    $completion->set_module_viewed($cm);
}","/**
 * Mark the activity completed (if required) and trigger the course_module_viewed event.
 *
 * @param  stdClass $imscp   imscp object
 * @param  stdClass $course     course object
 * @param  stdClass $cm         course module object
 * @param  stdClass $context    context object
 * @since Moodle 3.0
 */","// Accepts 4 parameters: $imscp, $course, $cm, $context. Mark the activity completed (if required) and trigger the course_module_viewed event.",
"public function specify(Select $select, $mapper)
    {

        foreach ($this->equalTo as $key => $val) {
            $db_key = $mapper($key);
            $select->where->equalTo($db_key, $val);
        }

        foreach ($this->greaterOrEqual as $key => $val) {
            $db_key = $mapper($key);
            $select->where->greaterThanOrEqualTo($db_key, $val);
        }

        foreach ($this->lessOrEqual as $key => $val) {
            $db_key = $mapper($key);
            $select->where->lessThanOrEqualTo($db_key, $val);
        }

        return $select;

    }","/**
     * @param Select $select
     * @param callable $mapper The mapper knows how to convert from model key to db key
     * @return Select
     */","// Accepts Select object, and a mapper. Queries the tables and based on a key and puts in different categories: equalTo, greaterOrEqual, lessOrEqual. ",
"public function testValidateFailTampering(): void
    {
        $unlocked = '';
        $fields = ['Model.hidden' => 'value', 'Model.id' => '1'];
        $debug = urlencode(json_encode([
            '/articles/index',
            $fields,
            [],
        ]));
        $fields = urlencode(Security::hash(serialize($fields) . $unlocked . Security::getSalt()));
        $fields .= urlencode(':Model.hidden|Model.id');
        $data = [
            'Model' => [
                'hidden' => 'tampered',
                'id' => '1',
            ],
            '_Token' => compact('fields', 'unlocked', 'debug'),
        ];

        $this->validate($data, 'Tampered field `Model.hidden` in POST data (expected value `value` but found `tampered`)');
    }","/**
     * testValidateFailTampering method
     *
     * Test that validate fails with tampered fields and explanation.
     *
     * @return void
     */",// Test that validate fails with tampered fields and explanation.,
"public function run()
    {
//        $this->call(UsersTableSeeder::class);
        $this->call('NoteTableSeeder');
        $this->call('UsersTableSeeder');
        $this->call('NoteUserTableSeeder');
        $this->call('NoteReleasesTableSeeder');
        $this->call('GroupTableSeeder');
        $this->call('GroupNoteTableSeeder');
        $this->call('GroupUserTableSeeder');
    }","/**
     * Run the database seeds.
     *
     * @return void
     */",// Run the database seeds.,
"function query($query)
		{

			//if flag to convert query from MySql syntax to MS-Sql syntax is true
			//convert the query
			if($this->convertMySqlToMSSqlQuery == true)
				$query = $this->ConvertMySqlToMSSql($query);


			// Initialise return
			$return_val = 0;


			// Flush cached values..
			$this->flush();

			// For reg expressions
			$query = trim($query);

			// Log how the function was called
			$this->func_call = ""\$db->query(\""$query\"")"";

			// Keep track of the last query for debug..
			$this->last_query = $query;

			// Count how many queries there have been
			$this->count(true, true);

			// Use core file cache function
			if ( $cache = $this->get_cache($query) )
			{
				return $cache;
			}


			// If there is no existing database connection then try to connect
			if ( ! isset($this->dbh) || ! $this->dbh )
			{
				$this->connect($this->dbuser, $this->dbpassword, $this->dbname, $this->dbhost);
			}

			// Perform the query via std mssql_query function..

			$this->result = @sqlsrv_query($this->dbh, $query);

			// If there is an error then take note of it..
			if ($this->result === false )
			{
				$errors = sqlsrv_errors();
				if (!empty($errors)) {
					foreach ($errors as $error) {
						$sqlError = ""ErrorCode: "".$error['code']."" ### State: "".$error['SQLSTATE']."" ### Error Message: "".$error['message']."" ### Query: "".$query;
						$this->register_error($sqlError);
						$this->show_errors ? trigger_error($sqlError ,E_USER_WARNING) : null;
					}
				}

				return false;
			}

			// Query was an insert, delete, update, replace
			$is_insert = false;
			if ( preg_match(""/^(insert|delete|update|replace)\s+/i"",$query) )
			{
				$is_insert = true;
				$this->rows_affected = @sqlsrv_rows_affected($this->result);

				// Take note of the insert_id
				if ( preg_match(""/^(insert|replace)\s+/i"",$query) )
				{

					$identityresultset = @sqlsrv_query($this->dbh, ""select SCOPE_IDENTITY()"");

					if ($identityresultset != false )
					{
						$identityrow = @sqlsrv_fetch($identityresultset);
						$this->insert_id = $identityrow[0];
					}

				}

				// Return number of rows affected
				$return_val = $this->rows_affected;
			}
			// Query was a select
			else
			{

				// Take note of column info
				$i=0;
				foreach ( @sqlsrv_field_metadata( $this->result) as $field ) {
					foreach ($field as $name => $value) {
						$name = strtolower($name);
						if ($name == ""size"") $name = ""max_length"";
						else if ($name == ""type"") $name = ""typeid"";
						//DEFINED FOR E_STRICT
						$col = new StdClass();
						$col->{$name} = $value;
					}

					$col->type = $this->get_datatype($col);
					$this->col_info[$i++] = $col;
					unset($col);
				}

				// Store Query Results
				$num_rows=0;

				while ( $row = @sqlsrv_fetch_object($this->result) )
				{

					// Store relults as an objects within main array
					$this->last_result[$num_rows] = $row;
					$num_rows++;
				}

				@sqlsrv_free_stmt($this->result);

				// Log number of rows the query returned
				$this->num_rows = $num_rows;

				// Return number of rows selected
				$return_val = $this->num_rows;
			}

			// disk caching of queries
			$this->store_cache($query,$is_insert);

			// If debug ALL queries
			$this->trace || $this->debug_all ? $this->debug() : null ;

			return $return_val;

		}","/**********************************************************************
		*  Perform mssql query and try to detirmin result value
		*/",// Accepts a query parameter. Perform mssql query on required tables and try to determine result value. Return the result value,
"private function assertFiles(array $files, $exists = TRUE, $directory = NULL)
    {
        if (!is_null($directory)) {
            array_walk($files, function(&$o) use ($directory) { $o = $directory . '/' . $o; });
        }
        foreach ($files as $file) {
            if ($exists) {
                $this->assertTrue(file_exists($file));
            } else {
                $this->assertFalse(file_exists($file));
            }

        }
    }","/**
     * Check if files either exist or don't exist.
     *
     * @param array $files
     *   Array of file paths.
     */","// Accepts array of files, exists boolean, and a directory. Check if each file in array either exist or don't exist in the directory.",
"public function update($id, UpdateClasseServicoRequest $request)
    {
        $classeServico = $this->classeServicoRepository->find($id);

        if (empty($classeServico)) {
            Flash::error('Classe Servico not found');

            return redirect(route('classeServicos.index'));
        }

        $classeServico = $this->classeServicoRepository->update($request->all(), $id);

        Flash::success('Classe Servico updated successfully.');

        return redirect(route('classeServicos.index'));
    }","/**
     * Update the specified ClasseServico in storage.
     *
     * @param int $id
     * @param UpdateClasseServicoRequest $request
     *
     * @return Response
     */","// Accepts $id, UpdateClasseServicoRequest parameters. Update the specified ClasseServico in storage. Redirect to classeServicos.index page",
"public function fetch_book_suggest()
    {
        $query = $this->request->getVar('query');

        $result = $this->book_model->like('title', $query)->where('status', 'Available')->get()->getResultArray();

        if (count($result) > 0) {
            foreach ($result as $row) {
            $output[] = $row['title'];
            }
            echo json_encode($output);
        }
    }",// Search Book with suggest typeahead.js,// Search Book with suggest typeahead.js and echos the output as json,
"private function extractDataRefMapRecursively( $node, &$dataRefEndMap)
    {
        if($this->nodeContainsNestedPcTags($node)) {
            foreach ( $node->inner_html as $nestedNode ) {
                $this->extractDataRefMapRecursively($nestedNode, $dataRefEndMap);
            }
        }

        if($node->tagname === 'pc'){
            if(isset($node->attributes['dataRefEnd'])){
                $dataRefEnd = $node->attributes['dataRefEnd'];
            } elseif(isset($node->attributes['dataRefStart'])) {
                $dataRefEnd = $node->attributes['dataRefStart'];
            } else {
                $dataRefEnd = null;
            }

            $dataRefEndMap[] = [
                    'id' => isset($node->attributes['id'] ) ? $node->attributes['id'] : null,
                    'dataRefEnd' => $dataRefEnd,
            ];
        }
    }","/**
     * Extract (recursively) the dataRefEnd map from single nodes
     *
     * @param object $node
     * @param $dataRefEndMap
     */",// Recursively extract the dataRefend map from single nodes,
"private function resolve_format() {
        global $CFG;
        $thisformat = $this->get_export_config('format');
        $allformats = portfolio_supported_formats();
        require_once($CFG->libdir . '/portfolio/formats.php');
        $thisobj = new $allformats[$thisformat];
        foreach ($this->supported_formats() as $f) {
            $class = $allformats[$f];
            if ($thisobj instanceof $class) {
                return $f;
            }
        }
    }","/**
     * internal helper function, that converts between the format constant,
     * which might be too specific (eg 'image') and the class in our *supported* list
     * which might be higher up the format hierarchy tree (eg 'file')
     */",// Converts between a possibly too specific format constant to a possibly more general supported format constant. Return general format constant,
"public function rules()
    {
        return [
            'tasa_financiamiento'   =>  'required',
            'enganche'              =>  'required',
            'plazo_max'             =>  'required',
        ];
    }","/**
     * Get the validation rules that apply to the request.
     *
     * @return array
     */",// Returns the validation rules that apply to the request.,
"public static function FindHosts(){
        echo ""nMapCrawler::FindHosts()\n"";
        if(!nMapCrawler::DoCrawlNetwork()) return null;
        echo ""Find nMap Hosts\n"";
        $ip = LocalIp();
        list($ip_a, $ip_b, $ip_c) = explode(""."",$ip);
        $ip_root = ""$ip_a.$ip_b.$ip_c."";
        $raw_output = shell_exec(""nmap -sP $ip_root*"");
        $lines = explode(""\n"",$raw_output);
        $hosts = array();
        foreach($lines as $line){
            if(strpos($line,$ip_root) !== false){
                $pos = strpos($line,$ip_root);
                $host = substr($line,$pos);
                if(strpos($host,"")"") !== false){
                    $host = substr($host,0,strpos($host,"")""));
                }
                if($host != $ip_root.""1""){
                    nMap::SaveHost(['ip'=>$host]);
                    array_push($hosts,nMap::LoadByIp($host));
                }
            }
        }
        return $hosts;
    }","/**
     * find network hosts
     * @return array list of local hosts
     */",// Find the network hosts and return an array list of local hosts,
"public function store(Request $request)
    {

        // dd($request);
        $this->validate($request, [
            'author' => 'required',
            'title' => 'required',
            // 'image' => 'required',
            'body' => 'required',
        ]);

        if ($request->hasFile('image')) {
            $file = $request->file('image');
            $originalname = microtime() . '_' . $file->getClientOriginalName();
            $path = $file->storeAs('public/images', $originalname);
        } else {
            $originalname = '';
        }


        $blog = Blog::create([
            'author' => $request->author,
            'title' => $request->title,
            'image' => $originalname,
            'body' => $request->body,
        ]);

        // dd('data stored', $blog);
        // $imageLink = url(""/images/{$blog->image}"");
        Mail::to(env('MAIL_USERNAME'))->send(new BlogEmail($blog));
        // Mail::send(new BlogEmail());
        // if () {
            dd('Mail is not working');

            return view('blog.detail', ['blog' => $blog]);
        // } else {
        // }
        // array_push($blog,$imageLink);
    }","/**
     * Store a newly created resource in storage.
     *
     * @param  \Illuminate\Http\Request  $request
     * @return \Illuminate\Http\Response
     */",// Accepts a Request object.Store a newly created blog post in storage using information embedded in request. Returns view of the blog details,
"public function createAction(Request $request)
    {
        $entity = new PensumAnio();
        $form = $this->createCreateForm($entity);
        $form->handleRequest($request);

        if ($form->isValid()) {
            $em = $this->getDoctrine()->getManager();
            $em->persist($entity);
            $em->flush();

            return $this->redirect($this->generateUrl('pensumanio_show', array('id' => $entity->getId())));
        }

        return array(
            'entity' => $entity,
            'form'   => $form->createView(),
        );
    }","/**
     * Creates a new PensumAnio entity.
     *
     * @Route(""/"", name=""pensumanio_create"")
     * @Method(""POST"")
     * @Template(""UmgVotacionBundle:PensumAnio:new.html.twig"")
     */",// Accepts a Request Object. Creates a new PensumAnio entity and returns the entity as well as a form created from the entity,
"public function photoAlbums(){
        $albums = PhotoAlbum::paginate(env('PAGINATE_DEFAULT'));

        $photoAlbumarray = $albums->toArray();


        foreach ($albums as $key => $photoAlbum){
            $photoAlbumarray['data'][$key] += ['quantity' => $photoAlbum->photos->count()];
        }
        
        ApiLogger::logInfo();
        return response()->json($photoAlbumarray);
    }","/**
     * Retrives List of photo albums (paged)
     *
     * @return string
     */",// Retrieves list of photo albums and returns it as a json ,
"protected function validate($config, $name, $path)
    {
        if (!is_array($config)) {
            throw new \InvalidArgumentException(sprintf('The definition of ""%s"" in ""%s"" must be a YAML array.', $name, $path));
        }
        if ($extraKeys = array_diff(array_keys($config), self::$availableKeys)) {
            throw new \InvalidArgumentException(sprintf(
                'The routing file ""%s"" contains unsupported keys for ""%s"": ""%s"". Expected one of: ""%s"".',
                $path, $name, implode('"", ""', $extraKeys), implode('"", ""', self::$availableKeys)
            ));
        }
        if (isset($config['resource']) && isset($config['pattern'])) {
            throw new \InvalidArgumentException(sprintf(
                'The routing file ""%s"" must not specify both the ""resource"" key and the ""pattern"" key for ""%s"". Choose between an import and a route definition.',
                $path, $name
            ));
        }
        if (!isset($config['resource']) && isset($config['type'])) {
            throw new \InvalidArgumentException(sprintf(
                'The ""type"" key for the route definition ""%s"" in ""%s"" is unsupported. It is only available for imports in combination with the ""resource"" key.',
                $name, $path
            ));
        }
        if (!isset($config['resource']) && !isset($config['pattern'])) {
            throw new \InvalidArgumentException(sprintf(
                'You must define a ""pattern"" for the route ""%s"" in file ""%s"".',
                $name, $path
            ));
        }
    }","/**
     * Validates the route configuration.
     *
     * @param array  $config A resource config
     * @param string $name   The config key
     * @param string $path   The loaded file path
     *
     * @throws \InvalidArgumentException If one of the provided config keys is not supported,
     *                                   something is missing or the combination is nonsense
     */","// Accepts config, name, and path parameters. Validates the route configuration.",
"public function update(Request $request, $id)
    {
        $commune = Commune::findOrFail($id);
        $commune->name = $request->name;
        $commune->save();
        return redirect()->route('commune.index')->with('status','Commune modifi avec succs');
    }","/**
     * Update the specified resource in storage.
     *
     * @param  \Illuminate\Http\Request  $request
     * @param  \App\Commune  $commune
     * @return \Illuminate\Http\Response
     */","// Accepts a Request Object, and an id. Update the resource specified in request in storage. Redirects to commune.index page",
"public function create()
    {
        //
        return view('admin.actuForm');
    }","/**
     * Show the form for creating a new resource.
     *
     * @return \Illuminate\Http\Response
     */",// Show the form for creating a new resource.,
"def render_stop(cls, stop) -> str:
        """"""
        Render a stop to HTML.
        """"""
        if hasattr(stop, ""x"") and hasattr(stop, ""y""):
            osm_url = (
                f""https://www.openstreetmap.org/query?""
                f""lat={stop.y}&lon={stop.x}""
                f""#map=18/{stop.y}/{stop.x}""
            )
        else:
            osm_url = None
        return cls._stop_template.render(this=stop, osm_url=osm_url)","
        Render a stop to HTML.
        ","# Accepts cls and stop parameters. Renders a stop to HTML , if it has a x and y value",
"def UpdateClusterParamsFromFlags(self, cluster, job_name):
    """"""Update `cluster` with a training cluster configuration from flags.""""""
    cluster.mode = FLAGS.mode
    cluster.job = job_name
    cluster.task = FLAGS.task

    cluster.controller.name = FLAGS.controller_job
    cluster.controller.gpus_per_replica = FLAGS.controller_gpus

    cluster.worker.name = FLAGS.worker_job
    cluster.worker.replicas = FLAGS.worker_replicas
    cluster.worker.gpus_per_replica = FLAGS.worker_gpus
    cluster.worker.tpus_per_replica = FLAGS.worker_tpus
    cluster.worker.num_tpu_hosts = FLAGS.worker_num_tpu_hosts
    cluster.worker.devices_per_split = FLAGS.worker_split_size

    cluster.ps.name = FLAGS.ps_job
    cluster.ps.replicas = FLAGS.ps_replicas
    cluster.ps.gpus_per_replica = FLAGS.ps_gpus

    cluster.input.name = FLAGS.input_job
    cluster.input.replicas = FLAGS.input_replicas
    cluster.input.targets = FLAGS.input_targets

    cluster.evaler.name = FLAGS.evaler_job
    cluster.evaler.replicas = FLAGS.evaler_replicas
    cluster.evaler.gpus_per_replica = FLAGS.evaler_gpus

    cluster.decoder.name = FLAGS.decoder_job
    cluster.decoder.replicas = FLAGS.decoder_replicas
    cluster.decoder.gpus_per_replica = FLAGS.decoder_gpus",Update `cluster` with a training cluster configuration from flags.,# Accepts cluster and job_name parameters. Update `cluster` with a training cluster configuration from flags.,
"def _setup_widget(self):
        """"""create widget which will have parameters and range for x axis as well""""""
        params = [ipywidgets.BoundedFloatText(
            **param.to_widget_dict()) for param in self._params if param.name != self._ind_var]
        param_widget = self._setup_params_widget(params)
        ind_var = [i for i in self._params if i.name == self._ind_var][0]
        x_min = ipywidgets.FloatText(description='x_min', value=ind_var.min)
        x_max = ipywidgets.FloatText(description='x_max', value=ind_var.max)

        submit_button = ipywidgets.Button(description='Plot')
        clear_button = ipywidgets.Button(description='Clear Plots')
        clear_check = ipywidgets.Checkbox(description='Fresh Plot')
        self._fig, self._ax = plt.subplots()
        _xx = np.linspace(x_min.value, x_max.value)
        self._ax.plot(_xx, self._func(
            _xx, **{param.description: param.value for param in params}))
        self._layout = ipywidgets.VBox(
            [param_widget,
             ipywidgets.HBox([x_min, x_max, clear_check]),
             ipywidgets.HBox([submit_button, clear_button])
             ])

        def _on_button_clicked(b):
            xx = np.linspace(x_min.value, x_max.value)
            # update parameters
            param_dict = {param.description: param.value for param in params}
            IPython.display.clear_output(wait=True)
            if clear_check.value:
                self._ax.cla()
            self._ax.plot(xx, self._func(xx, **param_dict))
            self._fig.canvas.draw()
            IPython.display.display(self._ax.figure)

        def _on_clear_button_clicked(b):
            IPython.display.clear_output(wait=True)
            self._ax.cla()
            self._fig.canvas.draw()
            IPython.display.display(self._ax.figure)

        submit_button.on_click(_on_button_clicked)
        clear_button.on_click(_on_clear_button_clicked)
        IPython.display.display(self._layout)",create widget which will have parameters and range for x axis as well,# Create widget which will have parameters and range for x axis as well,
"def _updated_data(self) -> None:
        """"""Called when data has been updated by coordinator""""""

        self.appliance = self.coordinator.appliance
        self._attr_available = self.appliance.online
        if not self.coordinator.available:
            self.on_online(False)
        elif not self._was_online_registered:
            self.on_online(True)

        if self.appliance.online:
            self.on_update()
        self.async_write_ha_state()",Called when data has been updated by coordinator,# Called when data has been updated by coordinator. Sets self.appliance and self._attr_available variables. Calls self.on_online() or self.on_update() methods,
"def scan_ID_store():
    """"""Set up the ScanIDStore and increment the scan ID a few times so we don't accidentally get 0""""""
    store = ScanIDStore()
    for _ in range(5):
        store.get_next_id()
    return store",Set up the ScanIDStore and increment the scan ID a few times so we don't accidentally get 0,# Set up the ScanIDStore and increment the scan ID a few times so we don't accidentally get 0,
"def begin():
    """"""Marks the start of a block.

    A `begin` command must be followed by a block processor command (eg. `grid` or `repeat`),
    which indicates how the block is processed. Blocks must be ended by a `end` command.

    Blocks can be nested.
    """"""
    return BeginBlock()","Marks the start of a block.

    A `begin` command must be followed by a block processor command (eg. `grid` or `repeat`),
    which indicates how the block is processed. Blocks must be ended by a `end` command.

    Blocks can be nested.
    ",# Marks the start of a block.,
"def createSpaceDistribution():
    """"""
    analyzes groups of files that have grouped reaction and creates a 
    distribution of how much space thse context-sensitive reactions cover
    """"""
    with open('spaceCovered.dump', 'rb') as f:
        space = pickle.load(f)
    infiniteCycles = 0
    infiniteCyclesFiles = set([])
    spaceCovered = []
    groupSize = []
    modelsWithGroups = []
    for element in space:
        for entry in space[element]:
            if entry[0] == -1:
                infiniteCycles += 1
                infiniteCyclesFiles.add(element)
                continue
            if entry[1] > entry[0]:
                continue

            spaceCovered.append(entry[1] * 1.0 / entry[0])
            modelsWithGroups.append((element, spaceCovered[-1]))
            groupSize.append(entry[0])
    constructHistogram(spaceCovered, 'spaceCovered', 'Percentage of space covered by\
reaction groups', np.ones(len(spaceCovered)), False)

    # uncomment if we wish to obtain a list of which model belongs
    # to which group
    # analyzeSpaceDistribution(modelsWithGroups)","
    analyzes groups of files that have grouped reaction and creates a 
    distribution of how much space thse context-sensitive reactions cover
    ",# Accepts a method as a parameter. Analyzes groups of files that have grouped reaction and creates a histogram of how much space these context-sensitive reactions cover,
"def handle_exceptions(method):
    """"""Transforms the exception for the volume but keeps its traceback intact.
    """"""
    def wrapper(self, ctx, volume_id, *args, **kwargs):
        try:
            res = method(self, ctx, volume_id, *args, **kwargs)
        except (keystone_exc.NotFound,
                cinder_exception.NotFound,
                cinder_exception.OverLimit) as e:
            raise exceptions.BackendException(str(e))
        return res
    return wrapper","Transforms the exception for the volume but keeps its traceback intact.
    ",# Accepts a method parameter. Transforms the exception for the volume but keeps its traceback intact.,
"def commuting_measurement_value_upper_bound(self, k: Union[int, str] = 1) -> float:
        """"""
        Compute an upper bound on the commuting measurement value of the nonlocal game.

        This function calculates an upper bound on the commuting measurement value by
        using k-levels of the NPA hierarchy [NPA]_. The NPA hierarchy is a uniform family
        of semidefinite programs that converges to the commuting measurement value of
        any nonlocal game.

        You can determine the level of the hierarchy by a positive integer or a string
        of a form like '1+ab+aab', which indicates that an intermediate level of the hierarchy
        should be used, where this example uses all products of one measurement, all products of
        one Alice and one Bob measurement, and all products of two Alice and one Bob measurements.

        References
        ==========
        .. [NPA] Miguel Navascues, Stefano Pironio, Antonio Acin,
            ""A convergent hierarchy of semidefinite programs characterizing the
            set of quantum correlations.""
            https://arxiv.org/abs/0803.4290

        :param k: The level of the NPA hierarchy to use (default=1).
        :return: The upper bound on the commuting strategy value of a nonlocal game.
        """"""
        alice_out, bob_out, alice_in, bob_in = self.pred_mat.shape

        mat = defaultdict(cvxpy.Variable)
        for x_in in range(alice_in):
            for y_in in range(bob_in):
                mat[x_in, y_in] = cvxpy.Variable(
                    (alice_out, bob_out), name=""M(a, b | {}, {})"".format(x_in, y_in)
                )

        p_win = cvxpy.Constant(0)
        for a_out in range(alice_out):
            for b_out in range(bob_out):
                for x_in in range(alice_in):
                    for y_in in range(bob_in):
                        p_win += (
                            self.prob_mat[x_in, y_in]
                            * self.pred_mat[a_out, b_out, x_in, y_in]
                            * mat[x_in, y_in][a_out, b_out]
                        )

        npa = npa_constraints(mat, k)
        objective = cvxpy.Maximize(p_win)
        problem = cvxpy.Problem(objective, npa)
        cs_val = problem.solve()

        return cs_val","
        Compute an upper bound on the commuting measurement value of the nonlocal game.

        This function calculates an upper bound on the commuting measurement value by
        using k-levels of the NPA hierarchy [NPA]_. The NPA hierarchy is a uniform family
        of semidefinite programs that converges to the commuting measurement value of
        any nonlocal game.

        You can determine the level of the hierarchy by a positive integer or a string
        of a form like '1+ab+aab', which indicates that an intermediate level of the hierarchy
        should be used, where this example uses all products of one measurement, all products of
        one Alice and one Bob measurement, and all products of two Alice and one Bob measurements.

        References
        ==========
        .. [NPA] Miguel Navascues, Stefano Pironio, Antonio Acin,
            ""A convergent hierarchy of semidefinite programs characterizing the
            set of quantum correlations.""
            https://arxiv.org/abs/0803.4290

        :param k: The level of the NPA hierarchy to use (default=1).
        :return: The upper bound on the commuting strategy value of a nonlocal game.
        ",# Compute an upper bound on the commuting measurement value of the nonlocal game.,
"def extract_tasks_dict(self, task_list):
        """"""
        The function serves as a shim that can turn lists of operators
        into the dictionary dependency format required for processing.
        """"""
        # contains a list of lists [[fillmissing, Logop]], Normalize, Categorify]
        task_dicts = []
        for obj in task_list:
            if isinstance(obj, list):

                for idx, op in enumerate(obj):
                    # kwargs for mapping during load later
                    self.ops_args[op._id] = op.export_op()[op._id]
                    if idx > 0:
                        to_add = {op._id: [[obj[idx - 1]._id]]}
                    else:
                        to_add = {op._id: [[]]}
                    task_dicts.append(to_add)
            else:
                self.ops_args[obj._id] = obj.export_op()[obj._id]
                to_add = {obj._id: [[]]}
                task_dicts.append(to_add)
        return task_dicts","
        The function serves as a shim that can turn lists of operators
        into the dictionary dependency format required for processing.
        ",# Accepts a task_list parameter. Converts list of operators into the dictionary dependency format required for processing,
"def dispatch(self, *, sync: bool = True,
                 root: Optional[Callable] = None, missing: Optional[Callable] = None) -> Any:
        """"""
        Dispatcher. Call pointed method with request arguments.
        """"""
        if root is None and callable(getattr(self, 'home', None)):
            root = self.home
        if missing is None and callable(getattr(self, 'missing', None)):
            missing = self.missing
        # TODO: use async too
        if sync:
            return self.router.sync_dispatch(self.req.url, root=root, missing=missing)
        return self.router.dispatch(self.req.url, root=root, missing=missing)","
        Dispatcher. Call pointed method with request arguments.
        ",# Sets the dispatch root and missing values using request arguments,
"def add_voucher_to_checkout(
    checkout: Checkout,
    lines: Iterable[CheckoutLine],
    voucher: Voucher,
    discounts: Optional[Iterable[DiscountInfo]] = None,
):
    """"""Add voucher data to checkout.

    Raise NotApplicable if voucher of given type cannot be applied.
    """"""
    discount = get_voucher_discount_for_checkout(voucher, checkout, lines, discounts)
    checkout.voucher_code = voucher.code
    checkout.discount_name = voucher.name
    checkout.translated_discount_name = (
        voucher.translated.name if voucher.translated.name != voucher.name else """"
    )
    checkout.discount = discount
    checkout.save(
        update_fields=[
            ""voucher_code"",
            ""discount_name"",
            ""translated_discount_name"",
            ""discount_amount"",
        ]
    )","Add voucher data to checkout.

    Raise NotApplicable if voucher of given type cannot be applied.
    ","# Accepts checkout, lines, voucher, discounts parameters. Add voucher data to checkout.",
"def ir_receiver(self) -> ""TasmotaIRReceiverConnector"":
        """"""
        Returns a *subscribable* :class:`bytes`-typed Connector that publishes the data/hash of each IR command received
        by the Tasmota device (via an IR receiver, attatched to a pin configured as `IRrecv`).

        See https://tasmota.github.io/docs/Tasmota-IR/#receiving-ir-commands for more information about receiving IR
        commands.
        """"""
        return self._get_or_create_connector(TasmotaIRReceiverConnector)","
        Returns a *subscribable* :class:`bytes`-typed Connector that publishes the data/hash of each IR command received
        by the Tasmota device (via an IR receiver, attatched to a pin configured as `IRrecv`).

        See https://tasmota.github.io/docs/Tasmota-IR/#receiving-ir-commands for more information about receiving IR
        commands.
        ",# Returns a subscribable Connector that publishes the data/hash of each IR command received by the Tasmota device ,
"def _overview_view(self, layout_name, source_name):
        """"""
        Renders the overview which can hold several graphs
        and is built via config
        """"""
        self.sync_layout_config()

        layouts = self.layouts.get(""layouts"")
        graphs = self.config.get(""graphs"")

        if layout_name:
            _layout = layouts.get(layout_name)
        else:
            if source_name:
                # if source name is specified but layout name is not
                # find the first layout that is a custom layout
                _layout = [l for l in list(layouts.values()) if l[""type""] == ""custom""][
                    0
                ]
            else:
                # if neither source name nor layout name are specified
                # we want to render the index layout
                _layout = layouts.get(""index"")

        layout = copy.deepcopy(_layout)

        source = layout.get(""source"", source_name)

        if source:
            title = layout.get(""title"", source)
        else:
            title = layout.get(""title"", ""overview"")

        sources = [source]

        ids = 1

        if layout.get(""type"") == ""index"":
            # index layout

            if not layout.get(""layout""):
                # auto-generate grid
                grid = [int(x) for x in layout.get(""grid"", ""3x3"").split(""x"")]

                # get all possible sources
                sources = layout.get(""sources"", graphsrv.group.get_paths())

                sources = [
                    {
                        ""source"": s,
                        ""type"": d.get(""default_graph"", ""multitarget""),
                        ""config"": d.get(""default_graph"", ""multitarget""),
                    }
                    for s, d in list(sources.items())
                ]

                sources = sorted(sources, key=lambda a: a.get(""source""))

                # filter sources matching the index
                # sources = [s for s,d in sources.items()
                #           if layout.get(""graph"").get(""config"") == d.get(""default_graph"",""multitarget"")]

                layout[""layout""] = [
                    {
                        ""cols"": [
                            {
                                ""graph"": copy.deepcopy(layout.get(""graph"")),
                                ""width"": int(12 // grid[0]),
                            }
                            for _ in range(0, grid[0])
                        ],
                        ""height"": float(100.00 // float(grid[1])),
                    }
                    for _ in range(0, grid[1])
                ]

        for row in layout.get(""layout""):
            for col in row.get(""cols"", []):
                if ""graph"" in col:

                    if layout.get(""type"") == ""index"":
                        if not col[""graph""].get(""source"") and sources:
                            col[""graph""].update(sources.pop(0))
                        if not col[""graph""].get(""id""):
                            col[""graph""][""id""] = ""auto-%s"" % ids
                            ids += 1

                    else:
                        col[""graph""][""source""] = sources[0]

                    cfg = graphs.get(col[""graph""].get(""config""))

                    # default configs
                    if ""targets"" not in cfg:
                        cfg[""targets""] = [{""target"": ""all""}]
                    if ""inspect"" not in cfg:
                        cfg[""inspect""] = Graph.Configuration.inspect.default
                    if ""inspect_layout"" not in cfg:
                        cfg[
                            ""inspect_layout""
                        ] = Graph.Configuration.inspect_layout.default

                    col[""graph""][""config_dict""] = cfg

        return self.render(
            ""overview.html"",
            self.wsgi_plugin.request_env(layout=layout, source=source, title=title),
        )","
        Renders the overview which can hold several graphs
        and is built via config
        ",# Accepts layout_name and source_name parameters. Renders the overview which can hold several graphs and is built via config,
"def xrtDeviceOpen(deviceIndex):
    """"""
    xrtDeviceOpen(): Open a device and obtain its xrt device handle

    :param deviceIndex: (unsigned int) Slot number of device 0 for first device, 1 for the second device...
    """"""
    libcoreutil.xrtDeviceOpen.restype = ctypes.POINTER(xrtDeviceHandle)
    libcoreutil.xrtDeviceOpen.argTypes = [ctypes.c_uint]
    return _valueOrError(libcoreutil.xrtDeviceOpen(deviceIndex))","
    xrtDeviceOpen(): Open a device and obtain its xrt device handle

    :param deviceIndex: (unsigned int) Slot number of device 0 for first device, 1 for the second device...
    ",# Opens a device and obtain its xrt device handle,
"def filename(self):
        """"""
        The name of a file to use for the csv data        
        """"""
        if self._filename is None:
            self._filename = self.configuration.get(self.section,
                                                    PollerEnum.filename,
                                                    optional=True,
                                                    default=PollerEnum.default_filename)
        return self._filename","
        The name of a file to use for the csv data        
        ","# Sets the name of file used for csv data if it is none, Returns the name of the file",
"def language(self, language):
        """"""Sets the language of this DiagnosticsStackFrame.

        programming language of the frame  # noqa: E501

        :param language: The language of this DiagnosticsStackFrame.  # noqa: E501
        :type: string
        """"""
        allowed_values = [undefined, undefined, undefined, undefined, undefined, undefined, undefined, undefined, undefined, ]  # noqa: E501

        self._language = language","Sets the language of this DiagnosticsStackFrame.

        programming language of the frame  # noqa: E501

        :param language: The language of this DiagnosticsStackFrame.  # noqa: E501
        :type: string
        ",# Accepts language parameter. Sets the language of this DiagnosticsStackFrame.,
"def python_tokenizer(
        self,
        token,
        match,
        style,
    ):
        """"""
        Callback for python specific highlighting.
        """"""

        value = xmlescape(match.group(), quote=False)
        if token == 'MULTILINESTRING':
            self.change_style(token, style)
            self.output.append(value)
            self.strMultilineString = match.group(1)
            return 'PYTHONMultilineString'
        elif token == 'ENDMULTILINESTRING':
            if match.group(1) == self.strMultilineString:
                self.output.append(value)
                self.strMultilineString = ''
                return 'PYTHON'
        if style and style[:5] == 'link:':
            self.change_style(None, None)
            (url, style) = style[5:].split(';', 1)
            if url == 'None' or url == '':
                self.output.append('<span style=""%s"">%s</span>'
                                   % (style, value))
            else:
                self.output.append('<a href=""%s%s"" style=""%s"">%s</a>'
                                   % (url, value, style, value))
        else:
            self.change_style(token, style)
            self.output.append(value)
        if token == 'GOTOHTML':
            return 'HTML'
        return None","
        Callback for python specific highlighting.
        ","# Accepts token, match, and style parameters. Acts as Callback for python specific highlighting. Perform different actions depending token and style values",
"def tests_random_uniform(row, col, interval, threshold=10**-10, diagnostics=False):
    """"""
    This function randomly (uniform distribution) generates a matrix of shape (row, col) 
    within given interval. See run_all_tests and tests_random_cond_rank documentation for 
    information on ""threshold"" and ""diagnostics"" along with the return values.

    """"""

    A = np.random.uniform(interval[0], interval[1], size=(row, col))
    success = True

    t0 = perf_counter()
    A_rank_svd = matrix_rank(A)
    svd_time = perf_counter() - t0

    t1 = perf_counter()
    A_rank_lup = RANK_COMPUTER(A, threshold)
    lup_time = perf_counter() - t1

    if A_rank_svd != A_rank_lup:
        success = False
        if diagnostics:
            print(""*****"")
            print(""Failed! From tests_random_uniform with row:{0},column:{1},threshold:{2},interval:{3}"".format(
                row, col, threshold, interval))
            print(""LUP_rank:{0}, SVD_rank:{1}"".format(A_rank_lup, A_rank_svd))

    return success, lup_time, svd_time","
    This function randomly (uniform distribution) generates a matrix of shape (row, col) 
    within given interval. See run_all_tests and tests_random_cond_rank documentation for 
    information on ""threshold"" and ""diagnostics"" along with the return values.

    ","# Accepts row, col, interval, threshold, diagnostics parameters. Function checks if it can randomly generate a matric of shape(row, col) within a given interval",
"def parse_now_current(raw_data: str) -> NowCurrentMeasurement:
    """"""
    Parse Current
    Signed short x2, 4 bytes in total, so 16 bits each

    From Appendix_Release_N_E.pdf:
      ""This property indicates the measured effective instantaneous R and T phase currents in 0.1A unit.""

    :param raw_data: raw_data in hex string
    :return: a tuple of the r_phase and t_phase values
    """"""
    r_phase_raw = raw_data[0:4]
    t_phase_raw = raw_data[4:8]

    r_phase = twos_complement(r_phase_raw, 16) / 10
    t_phase = twos_complement(t_phase_raw, 16) / 10
    if r_phase >= 3276.7 or t_phase >= 3276.7:
        raise ValueError(""NOW_CURRENT - Overflow"")
    elif r_phase <= -3276.8 or t_phase <= -3276.8:
        raise ValueError(""NOW_CURRENT - Underflow"")

    if r_phase == 3276.6:
        r_phase = None
    if t_phase == 3276.6:
        t_phase = None
    return NowCurrentMeasurement(r_phase=r_phase, t_phase=t_phase)","
    Parse Current
    Signed short x2, 4 bytes in total, so 16 bits each

    From Appendix_Release_N_E.pdf:
      ""This property indicates the measured effective instantaneous R and T phase currents in 0.1A unit.""

    :param raw_data: raw_data in hex string
    :return: a tuple of the r_phase and t_phase values
    "," Accepts raw_data parameter. Gets the twos complement of the raw r_phase and t_phase  current values in a 0.1A unit 
to check for underflows, overflow, no flows. Returns the tuple of the twos complement 
R_phase and t_phase values if they pass the checks ",
