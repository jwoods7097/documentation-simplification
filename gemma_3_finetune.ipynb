{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7063800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eebc4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "#os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35485c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e66e9e84bcd49118433045cb04a43fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#dataset = load_dataset('code-search-net/code_search_net')\n",
    "dataset = load_dataset(\"Fsoft-AIC/the-vault-function\", split_set=[\"test\"], trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcca6194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hexsha', 'repo', 'path', 'license', 'language', 'identifier',\n",
       "       'return_type', 'original_string', 'original_docstring', 'docstring',\n",
       "       'docstring_tokens', 'code', 'code_tokens', 'short_docstring',\n",
       "       'short_docstring_tokens', 'comment', 'parameters', 'docstring_params'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = pd.DataFrame(dataset['test'])\n",
    "sample_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "759c19be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hexsha', 'repo', 'path', 'license', 'language', 'identifier',\n",
       "       'return_type', 'original_string', 'original_docstring', 'docstring',\n",
       "       'docstring_tokens', 'code', 'code_tokens', 'short_docstring',\n",
       "       'short_docstring_tokens', 'comment', 'parameters', 'docstring_params'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample = sample_df.groupby('language', group_keys=False).sample(n=20)\n",
    "dataset_sample.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75727e7",
   "metadata": {},
   "source": [
    "# Testing the Baseline model\n",
    "\n",
    "Here we define the system prompt for the Llama 2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa72c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prompt(language, documentation, code):\n",
    "    return \\\n",
    "    f'''You are a helpful agent designed to simplify code documentation for beginner programmers.\n",
    "    You will be provided with a block of {language} code and the existing doucmentation that accompanies it.\n",
    "    Simplify the given documentation, using the provided code as context, so that it is understandable\n",
    "    to beginner programmers. Output absolutely nothing else besides the simplified documentation.\n",
    "    Make sure to keep any documentation formatting codes present in the simplified documentation.\n",
    "    If you feel that the existing documentation is simple enough and meaning would be lost by simplifying\n",
    "    it further, feel free to keep the documentation as is. Here is the original documentation and code:\\n\n",
    "    Documentation:\\n{documentation}\\n\\nCode:\\n{code}'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd58925",
   "metadata": {},
   "source": [
    "Creating the pipeline for the Gemma 3 model using the HuggingFace transformers library. Modified from the example here: https://huggingface.co/docs/transformers/v4.51.3/en/model_doc/gemma#gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5bbc1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688b32ef1e7344be8e72b1b812bf5476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The model 'Gemma3ForConditionalGeneration' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "\"\"\"pipe_gemma = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/gemma-3-4b-it\",\n",
    "    torch_dtype=torch.float32,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\"\"\"\n",
    "pipe_gemma = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/gemma-3-4b-it\",\n",
    "    torch_dtype=torch.float32,\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee38e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65530953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Documentation:\n",
      "Build the library mappings tables.\n",
      "\n",
      "Code:\n",
      "def build_mapping_tables(app):\n",
      "    \"\"\"Build the library mappings tables.\"\"\"\n",
      "    env = Environment(loader=FileSystemLoader(f\"{DIR_PATH}\"))\n",
      "    template_file = env.get_template(\"table_template.j2\")\n",
      "\n",
      "    LIST_OF_MAP_DICTS = []\n",
      "    for attr in dir(lib_mapper):\n",
      "        if (attr.endswith(\"MAPPER_REVERSE\") or attr.endswith(\"_MAPPER\")) and not (\n",
      "            attr.startswith(\"_\") or attr.startswith(\"NETMIKO\") or attr.startswith(\"MAIN\")\n",
      "        ):\n",
      "            LIST_OF_MAP_DICTS.append(attr)\n",
      "\n",
      "    for dict_name in LIST_OF_MAP_DICTS:\n",
      "        lib_name = dict_name.split(\"_\")[0]\n",
      "        filename = f\"{lib_name}_reverse\" if \"REVERSE\" in dict_name else lib_name\n",
      "        headers = [\"NORMALIZED\", lib_name] if \"REVERSE\" in dict_name else [lib_name, \"NORMALIZED\"]\n",
      "        rendered_template = template_file.render(lib_names=headers, mappings=getattr(lib_mapper, dict_name))\n",
      "        with open(f\"{DIR_PATH}/netutils/lib_mapping/{filename}_table.rst\", \"w\") as table_file:\n",
      "            table_file.write(rendered_template)\n",
      "\n",
      "***********************Result************************************\n",
      "You are a helpful agent designed to simplify code documentation for beginner programmers.\n",
      "    You will be provided with a block of Python code and the existing doucmentation that accompanies it.\n",
      "    Simplify the given documentation, using the provided code as context, so that it is understandable\n",
      "    to beginner programmers. Output absolutely nothing else besides the simplified documentation.\n",
      "    Make sure to keep any documentation formatting codes present in the simplified documentation.\n",
      "    If you feel that the existing documentation is simple enough and meaning would be lost by simplifying\n",
      "    it further, feel free to keep the documentation as is. Here is the original documentation and code:\n",
      "\n",
      "    Documentation:\n",
      "Build the library mappings tables.\n",
      "\n",
      "Code:\n",
      "def build_mapping_tables(app):\n",
      "    \"\"\"Build the library mappings tables.\"\"\"\n",
      "    env = Environment(loader=FileSystemLoader(f\"{DIR_PATH}\"))\n",
      "    template_file = env.get_template(\"table_template.j2\")\n",
      "\n",
      "    LIST_OF_MAP_DICTS = []\n",
      "    for attr in dir(lib_mapper):\n",
      "        if (attr.endswith(\"MAPPER_REVERSE\") or attr.endswith(\"_MAPPER\")) and not (\n",
      "            attr.startswith(\"_\") or attr.startswith(\"NETMIKO\") or attr.startswith(\"MAIN\")\n",
      "        ):\n",
      "            LIST_OF_MAP_DICTS.append(attr)\n",
      "\n",
      "    for dict_name in LIST_OF_MAP_DICTS:\n",
      "        lib_name = dict_name.split(\"_\")[0]\n",
      "        filename = f\"{lib_name}_reverse\" if \"REVERSE\" in dict_name else lib_name\n",
      "        headers = [\"NORMALIZED\", lib_name] if \"REVERSE\" in dict_name else [lib_name, \"NORMALIZED\"]\n",
      "        rendered_template = template_file.render(lib_names=headers, mappings=getattr(lib_mapper, dict_name))\n",
      "        with open(f\"{DIR_PATH}/netutils/lib_mapping/{filename}_table.rst\", \"w\") as table_file:\n",
      "            table_file.write(rendered_template)\n",
      "\n",
      "\"\"\"\n",
      "    \"\"\"\n",
      "    Simplified Documentation:\n",
      "    Create tables to map libraries.\n",
      "\n",
      "    This function goes through the `lib_mapper`\n"
     ]
    }
   ],
   "source": [
    "\"\"\"message = [\n",
    "    {\"role\": \"system\", \"content\": test_prompt(dataset['test'][0]['language'])},\n",
    "    {\"role\": \"user\", \"content\": f\"Documentation:\\n{dataset['test'][0]['original_docstring']}\\n\\nCode:\\n{dataset['test'][0]['original_string']}\"}\n",
    "]\"\"\"\n",
    "lan = dataset['test'][0]['language']\n",
    "code = dataset['test'][0]['original_string']\n",
    "doc = dataset['test'][0]['original_docstring']\n",
    "print(f\"Original Documentation:\\n{dataset['test'][0]['original_docstring']}\\n\")\n",
    "print(f\"Code:\\n{dataset['test'][0]['original_string']}\\n\")\n",
    "print(\"***********************Result************************************\")\n",
    "print(pipe_gemma(test_prompt(lan, doc, code), pad_token_id=pipe_gemma.tokenizer.eos_token_id, max_new_tokens=30)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c386b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/adeniji/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/adeniji/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/adeniji/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352b6b0822f347299088d8a397014f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_semantic_similarities_untrained = []\n",
    "gemma_metrics_untrained = evaluate.combine(['rouge', 'meteor'])\n",
    "\n",
    "for instance in tqdm(dataset_sample.itertuples()):\n",
    "    lan = instance.language\n",
    "    code = instance.original_docstring\n",
    "    doc = instance.original_string\n",
    "\n",
    "    message = test_prompt(lan, doc, code)\n",
    "\n",
    "    result = pipe_gemma(message, pad_token_id=pipe_gemma.tokenizer.eos_token_id, max_new_tokens=30)[0]['generated_text']\n",
    "\n",
    "    #result = output.replace(message, \"\")\n",
    "\n",
    "    embedding_original = eval_model.encode(instance.original_docstring, convert_to_tensor=True)\n",
    "    embedding_predicted = eval_model.encode(result, convert_to_tensor=True)\n",
    "\n",
    "    gemma_semantic_similarities_untrained.append(util.pytorch_cos_sim(embedding_original, embedding_predicted).item())\n",
    "    gemma_metrics_untrained.add(predictions=result, references=instance.original_docstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e1a5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained Gemma 3: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.502455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.130216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.128034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.418758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.515865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.604241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.771373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  200.000000\n",
       "mean     0.502455\n",
       "std      0.130216\n",
       "min      0.128034\n",
       "25%      0.418758\n",
       "50%      0.515865\n",
       "75%      0.604241\n",
       "max      0.771373"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Untrained Gemma 3: \\n\")\n",
    "sims_untrianed = pd.DataFrame(gemma_semantic_similarities_untrained)\n",
    "sims_untrianed.to_excel('results/Semantic_Similarities_Gemma_untrained.xlsx')\n",
    "sims_untrianed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e6f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results/rouge_meteor_gemma_untrained.json', 'w') as file:\n",
    "    mr = gemma_metrics_untrained.compute()\n",
    "    json.dump(mr, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66620ff",
   "metadata": {},
   "source": [
    "# Testing the FineTuned model\n",
    "\n",
    "Here we Import the training examples from an excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89fe1a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['language', 'original_string', 'original_docstring',\n",
       "       'modified_short_docstring'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_excel(\"self_training_annotated.xlsx\", sheet_name=\"Sheet1\", usecols=[5, 8, 9, 16])\n",
    "training_sample = train_df.groupby('language', group_keys=False).sample(n=5)\n",
    "training_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1847f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_prompt(language, documentation, code, modified):\n",
    "    return \\\n",
    "    f'''You are a helpful agent designed to simplify code documentation for beginner programmers.\n",
    "    You will be provided with a block of {language} code and the existing doucmentation that accompanies it.\n",
    "    Simplify the given documentation, using the provided code as context, so that it is understandable\n",
    "    to beginner programmers. Output absolutely nothing else besides the simplified documentation.\n",
    "    Make sure to keep any documentation formatting codes present in the simplified documentation.\n",
    "    If you feel that the existing documentation is simple enough and meaning would be lost by simplifying\n",
    "    it further, feel free to keep the documentation as is. Here is the original documentation and code:\\n\n",
    "    Code:\\n{code}\\n\\nDocumentation:\\n{documentation}\\n\\nModified documentation:\\n{modified}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de4fac96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d6081c9ab947d59f868e4539f42f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for instance in tqdm(training_sample.itertuples()):\n",
    "    lan = instance.language\n",
    "    code = instance.original_docstring\n",
    "    doc = instance.original_string\n",
    "    mod = instance.modified_short_docstring\n",
    "\n",
    "    message = training_prompt(lan, doc, code, mod)\n",
    "\n",
    "    pipe_gemma(message, pad_token_id=pipe_gemma.tokenizer.eos_token_id, max_new_tokens=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "289441bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_gemma.device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36182a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/adeniji/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/adeniji/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/adeniji/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6727a65d35804323983aa751e3be0290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m doc \u001b[38;5;241m=\u001b[39m instance\u001b[38;5;241m.\u001b[39moriginal_string\n\u001b[1;32m     10\u001b[0m message \u001b[38;5;241m=\u001b[39m test_prompt(lan, doc, code)\n\u001b[0;32m---> 12\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpipe_gemma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipe_gemma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m embedding_original \u001b[38;5;241m=\u001b[39m eval_model\u001b[38;5;241m.\u001b[39mencode(instance\u001b[38;5;241m.\u001b[39moriginal_docstring, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m embedding_predicted \u001b[38;5;241m=\u001b[39m eval_model\u001b[38;5;241m.\u001b[39mencode(result, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/documentation-simplification/cis732/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:173\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[1;32m    178\u001b[0m     ):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/Documents/documentation-simplification/cis732/lib/python3.10/site-packages/transformers/pipelines/base.py:1340\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m postprocess_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postprocess_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params}\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1341\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1342\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1343\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1344\u001b[0m     )\n\u001b[1;32m   1346\u001b[0m is_dataset \u001b[38;5;241m=\u001b[39m Dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, Dataset)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "gemma_semantic_similarities_trained = []\n",
    "gemma_metrics_trained = evaluate.combine(['rouge', 'meteor'])\n",
    "\n",
    "\n",
    "for instance in tqdm(dataset_sample.itertuples()):\n",
    "    lan = instance.language\n",
    "    code = instance.original_docstring\n",
    "    doc = instance.original_string\n",
    "\n",
    "    message = test_prompt(lan, doc, code)\n",
    "\n",
    "    result = pipe_gemma(message, pad_token_id=pipe_gemma.tokenizer.eos_token_id, max_new_tokens=30)[0]['generated_text']\n",
    "\n",
    "    embedding_original = eval_model.encode(instance.original_docstring, convert_to_tensor=True)\n",
    "    embedding_predicted = eval_model.encode(result, convert_to_tensor=True)\n",
    "\n",
    "    gemma_semantic_similarities_trained.append(util.pytorch_cos_sim(embedding_original, embedding_predicted).item())\n",
    "    gemma_metrics_trained.add(predictions=result, references=instance.original_docstring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80efc66",
   "metadata": {},
   "source": [
    "# Summary statistics results\n",
    "Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ad74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trained Gemma 3: \\n\")\n",
    "sims_trained = pd.DataFrame(gemma_semantic_similarities_trained)\n",
    "sims_trained.to_excel('results/Semantic_Similarities_Gemma_trained.xlsx')\n",
    "sims_trained.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0132c1db",
   "metadata": {},
   "source": [
    "ROUGE AND METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae45fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/rouge_meteor_gemma_trained.json', 'w') as file:\n",
    "    mr = gemma_metrics_trained.compute()\n",
    "    json.dump(mr, file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cis732",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
